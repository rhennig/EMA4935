{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04cc81f2",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde1e698",
   "metadata": {},
   "source": [
    "# Math - Algebra\n",
    "\n",
    "(Based on https://online.stat.psu.edu/stat462/node/132/ and https://www.geeksforgeeks.org/ml-normal-equation-in-linear-regression)\n",
    "\n",
    "Linear algebra is the branch of mathematics concerning linear equations,\n",
    "$$\n",
    "a_{1}x_{1}+\\cdots +a_{n}x_{n}=b,\n",
    "$$\n",
    "linear maps,\n",
    "$$\n",
    "(x_{1},\\ldots ,x_{n})\\mapsto a_{1}x_{1}+\\cdots +a_{n}x_{n},\n",
    "$$\n",
    "and their representations in vector spaces and through matrices. Linear algebra is a key foundation to the field of machine learning, from the notations used to describe the equations and operation of algorithms to the efficient implementation of algorithms in code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d7e49a",
   "metadata": {},
   "source": [
    "## 1. Motivational Example of Linear Regression\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b0/Linear_least_squares_example2.svg/782px-Linear_least_squares_example2.svg.png\" alt=\"Linear Regression\" align=\"right\" style=\"width: 25%;float: right;\"/>\n",
    "\n",
    "### Goal:\n",
    "Linear regression seeks to find the best-fit line that models the relationship between independent variables $x$ and the dependent variable $y$:\n",
    "$$\n",
    "\\hat{y} = \\beta_0 + \\beta_1 x\n",
    "$$\n",
    "Where:\n",
    "- $ \\hat{y} $: Predicted value.\n",
    "- $ x $: Independent variable (input).\n",
    "- $ \\beta_0 $: Intercept of the line.\n",
    "- $ \\beta_1 $: Slope of the line.\n",
    "\n",
    "### Matrix formulation\n",
    "We first derive the linear regression model in matrix form. In linear regression, we fit a linear function to a dataset of $n$ data points $(x_i, y_i)$. The linear model is given by\n",
    "$$\n",
    "y(x) = \\beta_0 + \\beta_1 x.\n",
    "$$\n",
    "\n",
    "Linear regression desscribes the data by minimizing the least squares deviation between the data and the linear model:\n",
    "$$\n",
    "y_i = \\beta_0 + \\beta_1 x_i + \\epsilon _i, \\, \\text{for }i = 1, \\dots , n.\n",
    "$$\n",
    "Here the $\\epsilon_i$ describes the deviation between the model and data and are assumed to be Gaussian distributed.\n",
    "\n",
    "Writing out the set of equations for $i = 1, \\dots, n$, we obtain $n$ equations:\n",
    "$$\n",
    "y_1 = \\beta_0 + \\beta_1 x_1 + \\epsilon _1 \\\\\n",
    "y_2 = \\beta_0 + \\beta_1 x_2 + \\epsilon _2 \\\\\n",
    "\\vdots \\\\\n",
    "y_n = \\beta_0 + \\beta_1 x_n + \\epsilon _n \\\\\n",
    "$$\n",
    "\n",
    "We can formulate the above simple linear regression function in matrix notation:\n",
    "$$\n",
    "\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "   1 & x_1 \\\\\n",
    "   1 & x_2 \\\\\n",
    "   \\vdots \\\\\n",
    "   1 & x_n\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "   \\beta_0 \\\\\n",
    "   \\beta_1\n",
    "\\end{bmatrix} +\n",
    "\\begin{bmatrix}\n",
    "   \\epsilon_1 \\\\\n",
    "   \\epsilon_2 \\\\\n",
    "   \\vdots \\\\\n",
    "   \\epsilon_n\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "We can write this matrix equation in a more compact form\n",
    "$$\n",
    "\\mathbf{Y} = \\mathbf{X}\\, \\mathbf{\\beta} + \\mathbf{\\epsilon},\n",
    "$$\n",
    "where\n",
    "- $\\mathbf{X}$ is an $n \\times 2$ matrix.\n",
    "- $\\mathbf{Y}$ is an $n \\times 1$ column vector\n",
    "- $\\mathbf{\\beta}$ is a $2 \\times 1$ column vector\n",
    "- $\\mathbf{\\epsilon}$ is an $n \\times 1$ column vector.\n",
    "\n",
    "The matrix $\\mathbf{X}$ and vector $\\mathbf{\\beta}$ are multiplied together using the techniques of matrix multiplication.\n",
    "And, the vector $\\mathbf{X} \\mathbf{\\beta}$ is added to the vector $\\mathbf{\\epsilon}$ using the techniques of matrix addition.\n",
    "\n",
    "Let's quickly review matrix algebra, the subject of mathematics that deals with operations of matrices, vectors, and tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a088010c",
   "metadata": {},
   "source": [
    "## 2. Least Squares Estimates of Linear Regression Coefficients\n",
    "\n",
    "As we will discuss later, minimizing the mean squared error, $|| \\epsilon ||^2$, over the model parameters, ${\\bf \\beta}$, using the given data leads to the following equation for the coefficient vector ${\\bf \\beta}$:\n",
    "$$\n",
    "\\mathbf{\\beta} = \\begin{bmatrix} \\beta_0 \\\\ \\vdots \\\\ \\beta_k \\end{bmatrix}\n",
    "= ( \\mathbf{X}^\\mathrm{T} \\mathbf{X} )^{-1}\\, \\mathbf{X}^\\mathrm{T}\\, \\mathbf{Y},\n",
    "$$\n",
    "where\n",
    "- $\\mathbf{X}^\\mathrm{T}$ is the transpose of the $\\mathbf{X}$ matrix, and\n",
    "- $( \\mathbf{X}^\\mathrm{T} \\mathbf{X} )^{-1}$ is the inverse of the $\\mathbf{X}^\\mathrm{T} \\mathbf{X}$ matrix.\n",
    "\n",
    "Let's remind ourselves of the transpose and inverse of a matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddb8b72",
   "metadata": {},
   "source": [
    "## 3. Transpose of a Matrix\n",
    "\n",
    "The transpose of a matrix $\\mathbf{A}$, denoted as $\\mathbf{A}^\\mathrm{T}$ or $\\mathbf{A}^{\\prime}$, is a matrix whose rows are the columns of $\\mathbf{A}$ and whose columns are the rows of $\\mathbf{A}$. All in the same order.\n",
    "\n",
    "For example, the transpose of the $3 \\times 2$ matrix $\\mathbf{A}$:\n",
    "$$\n",
    "\\mathbf{A} = \\begin{bmatrix} a_{0,0} & a_{0,1} \\\\ a_{1,0} & a_{1,1} \\\\ a_{2,0} & a_{2,1} \\end{bmatrix}\n",
    "$$\n",
    "is the $2 \\times 3$ matrix $\\mathbf{A}^\\mathrm{T}$:\n",
    "$$\n",
    "\\mathbf{A}^\\mathrm{T} = \\begin{bmatrix} a_{0,0} & a_{1,0} & a_{2,0} \\\\ a_{0,1} & a_{1,1} & a_{2,1} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The $\\mathbf{X}$ matrix in the simple linear regression setting is:\n",
    "$$\n",
    "\\mathbf{X} = \\begin{bmatrix}\n",
    "   1 & x_1 \\\\\n",
    "   1 & x_2 \\\\\n",
    "   \\vdots \\\\\n",
    "   1 & x_n\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Hence, the $\\mathbf{X}^\\mathrm{T} \\mathbf{X}$ matrix in the linear regression is:\n",
    "$$\n",
    "\\mathbf{X}^\\mathrm{T} \\mathbf{X} = \\begin{bmatrix}\n",
    "   1 & 1 & \\dots & 1\\\\\n",
    "   x_1 & x_2 & & x_n\n",
    "\\end{bmatrix}\n",
    "\n",
    "\\begin{bmatrix}\n",
    "   1 & x_1 \\\\\n",
    "   1 & x_2 \\\\\n",
    "   \\vdots \\\\\n",
    "   1 & x_n\n",
    "\\end{bmatrix}\n",
    "\n",
    "= \\begin{bmatrix}\n",
    "n & \\sum_{i=1}^n x_i \\\\ \\sum_{i=1}^n x_i & \\sum_{i=1}^n x_i^2\n",
    "\\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092dcb8a",
   "metadata": {},
   "source": [
    "## 4. The Inverse of a Matrix\n",
    "\n",
    "The inverse $\\mathbf{A}^{-1}$ of a **square matrix** $\\mathbf{A}$ is the matrix that fulfills the equation:\n",
    "$$\n",
    "\\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I} = \\mathbf{A} \\mathbf{A}^{-1}.\n",
    "$$\n",
    "\n",
    "That is, the inverse of $\\mathbf{A}$ is the matrix $\\mathbf{A}^{-1}$ that you multiply $\\mathbf{A}$ by to obtain the identity matrix $\\mathbf{I}$. Note that the inverse only exists for square matrices.\n",
    "\n",
    "Now, finding inverses, particularly for large matrices, is a complicated task. We will use numpy to calculate inverse matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779eef88",
   "metadata": {},
   "source": [
    "## 5. Solution for Linear Regresssion\n",
    "\n",
    "We will use a data set from the Python library sklearn for linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b016984c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy             as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets     import make_regression\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83f55fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data set\n",
    "x, y = make_regression(n_samples=100, n_features=1, n_informative=1, noise=10, random_state=10)\n",
    " \n",
    "# Plot the data set\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.rcParams['font.size'] = '16'\n",
    "\n",
    "plt.scatter(x, y, s = 30, marker = 'o')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.title('Scatter Data', fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73695cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the vector of y variables into an (nx1) numpy array\n",
    "Y = np.expand_dims(y, axis=-1) \n",
    "\n",
    "# Create matrix X by adding x0 = 1 to each instance of x and taking the transpose\n",
    "X = np.stack( ( np.ones(x.size), np.ravel(x) ), axis=1 )\n",
    "\n",
    "# Determining the coefficients of linear regression\n",
    "\n",
    "# Calculate X^T X\n",
    "XT_times_X = np.matmul(X.T, X)\n",
    "\n",
    "# Calculate (X^T X)^-1\n",
    "XT_times_X_inverse = np.linalg.inv(XT_times_X)\n",
    "\n",
    "# Calculate (X^T Y)\n",
    "XT_times_Y = np.matmul(X.T, Y)\n",
    "\n",
    "# Calculate (X^T X)^-1 (X^T Y)\\\n",
    "Beta = np.matmul(XT_times_X_inverse, XT_times_Y).reshape(2)\n",
    "\n",
    "# Display best values obtained\n",
    "print(f\"Matrix X =\\n\"\n",
    "      f\"{X[1:5, :]}\\n\\n\")\n",
    "\n",
    "print(f\"Matrix X'X =\\n\"\n",
    "      f\"{XT_times_X}\\n\\n\")\n",
    "\n",
    "print(f\"Inverse of (X'X) =\\n\"\n",
    "      f\"{XT_times_X_inverse}\\n\\n\")\n",
    "\n",
    "print(\"Regression coefficients\\n\"\n",
    "      f\"β0 = {Beta[0]:6.4f}\\n\"\n",
    "      f\"β1 = {Beta[1]:6.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e544e8",
   "metadata": {},
   "source": [
    "### 5.1 Predict y-values using the regression coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1673bcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the values for given data instance.\n",
    "x_sample     = np.array( [[-2.5],[3]] )\n",
    "\n",
    "# Generate matrix X\n",
    "X_sample     = np.stack( ( np.ones( x_sample.size), np.ravel(x_sample) ), axis=1 )\n",
    "\n",
    "# Multiply matrix X by the regression coefficients\n",
    "y_predicted  = np.matmul(X_sample, Beta)\n",
    "\n",
    "print(f\"Predicted values = {', '.join([f'[ {i[0]:.2f}, {i[1]:.2f} ]' for i in zip(np.ravel(x_sample), y_predicted)])}\")\n",
    "\n",
    "# Plot the generated data set\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.rcParams['font.size'] = '16'\n",
    "\n",
    "plt.scatter(x, y, s = 30, marker = 'o')\n",
    "plt.plot(x_sample, y_predicted, color='black', lw=2)\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.title('Scatter Data', fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1375ae7",
   "metadata": {},
   "source": [
    "### 5.2 Now using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db71eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression function from scikit-learn\n",
    "linear_regression = LinearRegression()\n",
    "\n",
    "# Fit the model\n",
    "linear_regression.fit(x, y)\n",
    " \n",
    "# Print obtained theta values\n",
    "print(f\"β0 = {linear_regression.intercept_:6.4f}\\n\"\n",
    "      f\"β1 = {linear_regression.coef_[0]:6.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06194735",
   "metadata": {},
   "source": [
    "> ## Assignment\n",
    ">\n",
    "> The projection matrix, $H$, converts values from the observed variable $y_i$ into the estimated values $\\hat{y}$  obtained with the least squares method.\n",
    "> $$\n",
    "> \\hat{Y} = H Y\n",
    "> $$\n",
    "> The projection matrix, $\\mathbf{H}$, is given by\n",
    "> $$\n",
    "> \\mathbf{H} =  \\mathbf{X}\\, (\\mathbf{X}^\\mathrm{T} \\mathbf{X})^{-1}\\, \\mathbf{X}^\\mathrm{T}\n",
    "> $$\n",
    ">\n",
    "> Calculate the projection matrix, $\\mathbf{H}$, and show that you obtain the predicted $y$-values by creating a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcd3466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate X^T X\n",
    "\n",
    "\n",
    "# Calculate (X^T X)^-1\n",
    "\n",
    "\n",
    "# Calculate the projection matrix H\n",
    "\n",
    "\n",
    "# Apply the projection matrix to the y-values to generate the y predictions\n",
    "\n",
    "\n",
    "# Plot the predicted and original y values vs. the x values\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.rcParams['font.size'] = '16'\n",
    "\n",
    "plt.scatter(x, y, s = 30, marker = 'o', label='Original y values')\n",
    "plt.scatter(x, Y_hat, s = 30, marker = 'x', label='Predicted y values')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Original and Predicted y values', fontsize=20)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606ccd9a",
   "metadata": {},
   "source": [
    "## R² – The Coefficient of Determination\n",
    "\n",
    "**Definition:**\n",
    "R², also known as the coefficient of determination, measures the proportion of the variance in the residuals for the dependent variable (target) that is predictable from the independent variable(s) (features) in a regression model. It indicates how well the regression model fits the data.\n",
    "\n",
    "**Range:**\n",
    "- $R^2 \\in [0, 1] $ for typical cases:\n",
    "  - $R^2 = 1$: Perfect fit (the model explains all the variance in the data).\n",
    "  - $R^2 = 0$: The model explains none of the variance in the data, equivalent to using the mean of the target as the prediction.\n",
    "\n",
    "- $R^2$ can be negative if the model performs worse than a constant prediction using the mean of the target.\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "R^2 = 1 - \\frac{\\text{SS}_{\\text{res}}}{\\text{SS}_{\\text{tot}}}\n",
    "$$\n",
    "Where:\n",
    "- $ \\text{SS}_{\\text{res}} $ (Residual Sum of Squares): $\\sum_{i=1}^n (y_i - \\hat{y}_i)^2$, the error between actual and predicted values.\n",
    "- $ \\text{SS}_{\\text{tot}} $ (Total Sum of Squares): $\\sum_{i=1}^n (y_i - \\bar{y})^2$, the variance of the actual values from the mean of the target.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efda1a91",
   "metadata": {},
   "source": [
    "### R² in Terms of Mean Squared Error and Variance\n",
    "\n",
    "The coefficient of determination (\\(R^2\\)) can also be expressed in terms of the **Mean Squared Error (MSE)** and the **variance** of the dependent variable. This formulation emphasizes how \\(R^2\\) quantifies the reduction in prediction error compared to using the mean of the dependent variable as the prediction.\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "R^2 = 1 - \\frac{\\text{MSE}_{\\text{model}}}{\\text{Var}(y)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\text{MSE}_{\\text{model}} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2$: Mean Squared Error of the regression model.\n",
    "- $\\text{Var}(y) = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\bar{y})^2$: Variance of the dependent variable $y$.\n",
    "\n",
    "### Explanation\n",
    "1. $ \\text{MSE}_{\\text{model}} $:\n",
    "   - This is the average of the squared residuals, representing the error of the regression model in predicting the dependent variable.\n",
    "\n",
    "2. $ \\text{Var}(y) $:\n",
    "   - This is the total variance of the dependent variable, quantifying how much \\(y\\) varies around its mean (\\( \\bar{y} \\)).\n",
    "\n",
    "3. $R^2$:\n",
    "   - The ratio $\\text{MSE}_{\\text{model}}/\\text{Var}(y) $ indicates the proportion of variance not explained by the model.\n",
    "   - Subtracting this from 1 gives the proportion of variance explained by the model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9be8563",
   "metadata": {},
   "source": [
    "\n",
    "### Alternative Interpretation:\n",
    "Using the Mean Squared Error of a baseline model:\n",
    "- Baseline model simply predicts the mean of $y$ by $\\hat{y}_i = \\bar{y}$ for all $i$.\n",
    "\n",
    "The formula becomes:\n",
    "$$\n",
    "R^2 = 1 - \\frac{\\text{MSE}_{\\text{model}}}{\\text{MSE}_{\\text{baseline}}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "-  $\\text{MSE}_{\\text{baseline}} = \\text{Var}(y) $ is the error of the baseline model.\n",
    "\n",
    "### Intuition:\n",
    "- If the regression model perfectly predicts all values ($\\text{MSE}_{\\text{model}} = 0 $), then $ R^2 = 1 $.\n",
    "- If the regression model performs no better than predicting the mean ($ \\text{MSE}_{\\text{model}} = \\text{Var}(y) $), then $ R^2 = 0 $.\n",
    "- If the model performs worse than the baseline, $ R^2 < 0 $, indicating that the model adds more error than simply using the mean of $y$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bb01f9",
   "metadata": {},
   "source": [
    "> From the projection matrix, $\\mathbf{H}$, we can also express the $R^2$ value for the linear regression using a matrix equation:\n",
    "> $$\n",
    "> R^2 = 1 - \\frac{\\mathbf{y}^\\mathrm{T}\\, (\\mathbf{I} - \\mathbf{H})\\, \\mathbf{y}} {\\mathbf{y}^\\mathrm{T}\\, (\\mathbf{I} - \\mathbf{M})\\, \\mathbf{y}}\n",
    "> $$\n",
    "> where $\\mathbf{I}$ is the identity matrix,\n",
    "> $$\n",
    "> \\mathbf{M} = \\mathbf{1}\\, (\\mathbf{1}^\\mathrm{T} \\mathbf{1})^{-1}\\, \\mathbf{1}^\\mathrm{T},\n",
    "> $$\n",
    "> and $\\mathbf{1}$ is a column vector of ones.\n",
    "> \n",
    "> Calculate the $R^2$ value using the above matrix form of the equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fd15a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column vector of ones\n",
    "\n",
    "\n",
    "# Calculate the matrix M\n",
    "\n",
    "\n",
    "# Calculate R2\n",
    "\n",
    "\n",
    "print(f\"R2 = {R2[0][0]:6.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e477d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also use sci-kit learn to calculate the R2 value\n",
    "R2 = linear_regression.score(x, y)\n",
    "print(f\"R2 = {R2:6.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c248c22",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- **Linear regression** finds the best-fit line by minimizing the residual errors (SSE).\n",
    "- The **coefficients** ($\\beta_0, \\beta_1$) are derived using least squares optimization.\n",
    "- **R²** quantifies how much of the variance in the target variable is explained by the regression model.\n",
    "- Using a **baseline model** the simply predicts the mean, i.e., $\\hat{y}_i = \\bar{y}$ for all $i$, we can interprete $R^2$ as the improvement in that mean squared error relative to the baseline model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
