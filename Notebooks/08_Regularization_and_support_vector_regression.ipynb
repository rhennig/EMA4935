{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization and Vector Support Regression\n",
    "\n",
    "Based on https://www.nbshare.io/notebook/819279082/Regularization-Techniques-in-Linear-Regression-With-Python and https://github.com/tomsharp/SVR/blob/master/SVR.ipynb\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "In machine learning we use data to approximate a model that is flexible enough to fit such data. Larger flexibility results in a model closer on average to the trainin set, with low bias but large variance. In those situations we need to control and prevent overfitting to the data set. We do this through a concept known as regularization. We will analize common regularization methods and their appication in vector support regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librarires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy  as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib              import cm\n",
    "\n",
    "from sklearn                 import svm\n",
    "from sklearn                 import datasets\n",
    "from sklearn.svm             import LinearSVR\n",
    "from sklearn.linear_model    import LinearRegression\n",
    "from sklearn.preprocessing   import PolynomialFeatures\n",
    "from sklearn.linear_model    import ElasticNet, Lasso, Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics         import mean_squared_error\n",
    "from sklearn.metrics         import mean_absolute_error\n",
    "\n",
    "# Ignore warnings about convergence\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.rc('xtick', labelsize=18)\n",
    "plt.rc('ytick', labelsize=18)\n",
    "\n",
    "blue   = '#0021A5'\n",
    "orange = '#FA4616'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Regularization\n",
    "\n",
    "Regularization is used to constrain (or regularize) the estimated coefficients of a model. This protects the model from overfitting on the training data. In the previous notebook on cross-validation we showed that overfitting of a polynomial regression can lead to poor prediction accuracy. Cross-validation is one approach to reduce overfitting. In that example, we used cross-validation to determine the optimal order of the polynomial for fitting the data.\n",
    "\n",
    "Regularization lets us include more features into a model while avoiding to overfit through constraints of the optimized model coefficients. In other words, regularization is used to prevent overfitting but too much regularization can result in underfitting.\n",
    "\n",
    "A common approach of regularization is to shrink the model coefficients towards zero. We apply such a regularization constraint, we by simply adding a term to our loss function that measures the deviation of the model coefficients from zero.\n",
    "\n",
    "1. LASSO regularization (absolute values of coefficients)\n",
    "$$\n",
    "C_\\mathrm{LASSO} = C({\\bf X}, {\\bf y}, \\beta) + \\lambda \\sum_{j=1}^k | \\beta_j |.\n",
    "$$\n",
    "\n",
    "2. Ridge regularization (squared coefficients)\n",
    "$$\n",
    "C_\\mathrm{Ridge} = C({\\bf X}, {\\bf y}, \\beta) + \\lambda \\sum_{j=1}^k \\beta_j^2.\n",
    "$$\n",
    "\n",
    "\n",
    "3. ElasticNet regularization (linear combination of absolute values and squared coefficients)\n",
    "$$\n",
    "C_\\mathrm{ElasticNet} = C({\\bf X}, {\\bf y}, \\beta) + \\lambda \\sum_{j=1}^k \\left ( | \\beta_j | + \\beta_j^2 \\right ).\n",
    "$$\n",
    "\n",
    "The variable $\\lambda$ is a hyperparameter to control the strength of our regularization. If $\\lambda = 0$, we end up with linear regression with the usual loss function. If  $\\lambda = \\infty$, the regularization term would would dwarf the original loss function and drive all the coefficients to zero in the parameter optimization. Hence, a large Î» results in underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to plot the data\n",
    "def plot_3d(subplot, title):\n",
    "\n",
    "    ax = fig.add_subplot(1, 3, subplot, projection='3d')\n",
    "    fig.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "    ax.plot_surface(X1, X2, Y, cmap=cm.jet,\n",
    "                    antialiased=False)\n",
    "    \n",
    "    ax.set_title(title, fontsize=20)\n",
    "    \n",
    "    ax.view_init(elev=10, azim=30)\n",
    "\n",
    "    ax.xaxis.set_ticklabels([])\n",
    "    ax.yaxis.set_ticklabels([])\n",
    "    ax.zaxis.set_ticklabels([])\n",
    "\n",
    "# Create a regular 2d mesh\n",
    "points = np.linspace(-5.0, 5.0, num=200)\n",
    "\n",
    "X1, X2 = np.meshgrid(points, points)\n",
    "\n",
    "fig    = plt.figure(figsize=(12,8))\n",
    "\n",
    "# Plot original data\n",
    "Y = 0.1*(X2*X2)\n",
    "plot_3d(1, 'Original data')\n",
    "\n",
    "# Plot L1 LASSO regularization\n",
    "Y = 0.1*(X2*X2) + 0.2*( np.abs(X1) + np.abs(X2) )\n",
    "plot_3d(2, 'L1 regularization')\n",
    "\n",
    "# Plot L2 Ridge regularization \n",
    "Y = 0.1*(X2*X2) + 0.03*(X1*X1 + X2*X2)\n",
    "plot_3d(3, 'L2 regularization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The original data is constructured such that is has a valley along which the cost function does not change. This is also called soft degree of freedom, which can occur when the data does not constrain all model parameters equally well. These poorly constrained parameters can lead to a large variance in the prediction error on the validation set.\n",
    "\n",
    "- The $L_1$ and $L_2$ regularization turn this valley in to minimum. For $L_1$ regularization, the minimum becomes a sharp point due to the use of the absolute value.\n",
    "\n",
    "- The $L_2$ regularization leads to a quadratic mimimum, that can be found analytically in linear regression. For the likelihood, which is the negative of the cost function, the original data shows a ridge. The $L_2$ regularization turns that ridge into a nice peak in likelihood space, equivalent to a quadratic depression in the cost function.\n",
    "\n",
    "### 1.2 Feature Scaling\n",
    "\n",
    "The various features in our data set may not be on the same scale. In that case, the model  coefficients are not going to be on the same scale either, resulting in different regularization on the various coefficients. Therefore, we need to normalize all the data to be on the same scale. This step is called feature scaling or data normalization.\n",
    "\n",
    "#### 1.2.1 Example for Linear Regression\n",
    "\n",
    "Let's generate a small data set to analize the behavior of these regularization methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_errors(lambdas, train_errors, test_errors, title, inset_kwargs=None):\n",
    "\n",
    "    fig, ax = plt.subplots( figsize=(12, 8) )\n",
    "\n",
    "    ax.set_title(title, fontsize=20)\n",
    "\n",
    "    ax.set_xlabel(r'$\\lambda$', fontsize=18)\n",
    "    ax.set_ylabel('MSE', fontsize=18)\n",
    "\n",
    "    ax.plot(lambdas, train_errors, lw=4, color=blue,   label='Training')\n",
    "    ax.plot(lambdas, test_errors,  lw=4, color=orange, label='Testing')\n",
    "    \n",
    "    ax.legend(fontsize=18)\n",
    "\n",
    "    if inset_kwargs:\n",
    "        inset = ax.inset_axes( [inset_kwargs['x0'],\n",
    "                                inset_kwargs['y0'],\n",
    "                                inset_kwargs['width'],\n",
    "                                inset_kwargs['height']] )\n",
    "\n",
    "        inset.plot(inset_kwargs['lambdas'],\n",
    "                   inset_kwargs['train_errors'],\n",
    "                   lw=4, color=blue,   label='Training')\n",
    "        \n",
    "        inset.plot(inset_kwargs['lambdas'],\n",
    "                   inset_kwargs['test_errors'], \n",
    "                   lw=4, color=orange, label='Testing')\n",
    "\n",
    "        ax.indicate_inset_zoom(inset, edgecolor='black')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: In sklearn, they refer to lambda as alpha, the name is different in different literature\n",
    "\n",
    "def evaluate_model(Model, lambdas, x_train=None, y_train=None, x_test=None, y_test=None):\n",
    "\n",
    "    training_errors = []\n",
    "    testing_errors  = []\n",
    "\n",
    "    for lambda_value in lambdas:\n",
    "        \n",
    "        model = Model(alpha=lambda_value, max_iter=1000) # Lasso, Ridge or ElasticNet\n",
    "        model.fit(x_train, y_train)\n",
    "\n",
    "        training_predictions = model.predict(x_train)\n",
    "\n",
    "        training_mse         = mean_squared_error(y_train, training_predictions)\n",
    "        training_errors.append(training_mse)\n",
    "\n",
    "        testing_predictions = model.predict(x_test)\n",
    "\n",
    "        testing_mse         = mean_squared_error(y_test, testing_predictions)\n",
    "        testing_errors.append(testing_mse)\n",
    "\n",
    "    return training_errors, testing_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a data set for machine learning\n",
    "np.random.seed(seed=5)\n",
    "\n",
    "x = np.linspace(0.0, 2.0, num=300)\n",
    "x = x + np.random.normal(0.0, 0.3, x.shape)\n",
    "\n",
    "y = np.cos(x) + 2.0*np.sin(x) + 3.0*np.cos(2.0*x) + np.random.normal(0.0, 1.0, x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into 80% for training and 20% for testing\n",
    "x = x.reshape( (-1,1) )\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8, shuffle=True)\n",
    "\n",
    "# Plot the training and testing dataset\n",
    "fig,ax=plt.subplots( figsize=(8,8) )\n",
    "\n",
    "ax.scatter(x_train, y_train, color=blue,   label='Training')\n",
    "ax.scatter(x_test, y_test,   color=orange, label='Testing')\n",
    "\n",
    "ax.set_title('Training and testing data', fontsize=20)\n",
    "\n",
    "ax.set_xlabel('X values', fontsize=18)\n",
    "ax.set_ylabel(r'$\\cos(x)+2\\sin(x)+3\\cos(2x)$', fontsize=18)\n",
    "\n",
    "plt.legend(fontsize=18)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Create the polynomial features for a large degree for the polynomials\n",
    "degree  = 20\n",
    "\n",
    "X_train = PolynomialFeatures(degree).fit_transform(x_train)\n",
    "X_test  = PolynomialFeatures(degree).fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Lasso $L^1$ Regularization\n",
    "$$\n",
    "C_\\mathrm{LASSO} = C({\\bf X}, {\\bf y}, \\beta) + \\lambda \\sum_{j=1}^k | \\beta_j |.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate range of values for a smaller range of lambda\n",
    "lambdas = np.arange(0.0, 0.4, step=0.001)\n",
    "\n",
    "train, test = evaluate_model(Lasso, lambdas,\n",
    "                             x_train=x_train, x_test=x_test, y_train=y_train, y_test=y_test)\n",
    "\n",
    "inset_kwargs = {'lambdas'     : lambdas,\n",
    "                'train_errors': train,\n",
    "                'test_errors' : test,\n",
    "                'x0'          : 0.25,\n",
    "                'y0'          : 0.15,\n",
    "                'width'       : 0.4,\n",
    "                'height'      : 0.6}\n",
    "\n",
    "# Generate range of values for lambda from 0 (no-regularization) and (0.5 too much regularization)\n",
    "lambdas = np.arange(0.0, 10.0, step=0.1)\n",
    "\n",
    "train, test = evaluate_model(Lasso, lambdas,\n",
    "                             x_train=x_train, x_test=x_test, y_train=y_train, y_test=y_test)\n",
    "\n",
    "plot_errors(lambdas, train, test, 'Lasso', inset_kwargs=inset_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inset shows an inflection between training and testing, indicating that for the $C_\\mathrm{LASSO}$ regularization, the optimal value of $\\lambda$ is no larger than 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3 Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate range of values for a smaller range of lambda\n",
    "lambdas = np.arange(0.0, 40.0, step=1.0)\n",
    "\n",
    "train, test = evaluate_model(Ridge, lambdas,\n",
    "                             x_train=x_train, x_test=x_test, y_train=y_train, y_test=y_test)\n",
    "\n",
    "inset_kwargs = {'lambdas'     : lambdas,\n",
    "                'train_errors': train,\n",
    "                'test_errors' : test,\n",
    "                'x0'          : 0.50,\n",
    "                'y0'          : 0.10,\n",
    "                'width'       : 0.45,\n",
    "                'height'      : 0.45}\n",
    "\n",
    "# let's generate different values for lambda from 0 (no-regularization) and (10 too much regularization)\n",
    "lambdas = np.arange(0.0, 500.0, step=1.0)\n",
    "\n",
    "train, test = evaluate_model(Ridge, lambdas,\n",
    "                             x_train=x_train, x_test=x_test, y_train=y_train, y_test=y_test)\n",
    "\n",
    "plot_errors(lambdas, train, test, 'Ridge', inset_kwargs=inset_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see an analogous effect for the $C_\\mathrm{Ridge}$ regularization. But notice that for this method, the inflection occurs for $\\lambda \\approx 30$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.4 Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate range of values for a smaller range of lambda\n",
    "lambdas = np.arange(0, 0.3, step=0.01)\n",
    "\n",
    "train, test = evaluate_model(ElasticNet, lambdas,\n",
    "                             x_train=x_train, x_test=x_test, y_train=y_train, y_test=y_test)\n",
    "\n",
    "inset_kwargs = {'lambdas'     : lambdas,\n",
    "                'train_errors': train,\n",
    "                'test_errors' : test,\n",
    "                'x0'          : 0.55,\n",
    "                'y0'          : 0.10,\n",
    "                'width'       : 0.40,\n",
    "                'height'      : 0.40}\n",
    "\n",
    "# Generate a range of values for lambda from 0 (no-regularization) to a larger value with probably too much regularization\n",
    "lambdas = np.arange(0, 1.5, step=0.01)\n",
    "\n",
    "train, test = evaluate_model(ElasticNet, lambdas,\n",
    "                             x_train=x_train, x_test=x_test, y_train=y_train, y_test=y_test)\n",
    "\n",
    "plot_errors(lambdas, train, test, 'Elastic Net', inset_kwargs=inset_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, for the $C_\\mathrm{Elastic Net}$ regularization this point is located at $\\lambda \\approx 0.2$.\n",
    "\n",
    "### 1.3 Regularization Techniques Comparison\n",
    "- Lasso: will eliminate many features, and reduce overfitting in your linear model.\n",
    "- Ridge: will reduce the impact of features that are not important in predicting your y values.\n",
    "- Elastic Net: combines feature elimination from Lasso and feature coefficient reduction from the Ridge model to improve your modelâs predictions.\n",
    "\n",
    "### 1.4 Cross Validation and Regularization \n",
    "\n",
    "The relationship between the training error / test error versus the model complexity has a U-shaped form. When learning a model, we have two goals:\n",
    "\n",
    "1. Find the optimum on the model complexity axis where the U curve starts to go up again. This happens for the test error curve, even though the training error curve continues to go down overfitting the training data in high-complexity models.\n",
    "\n",
    "2. Reduce the gap between the training error and test error curve.\n",
    "\n",
    "Goal (1) is achieved by using cross validation, finding the fine balance between bias and variance.\n",
    "\n",
    "Goal (2) is achieved by using regularization, raising the training error curve to be closer to the test error.\n",
    "\n",
    "## 2. Support Vector Regression\n",
    "\n",
    "- SVR minimizes the $L_2$ norm of the coefficient vector, not the squared error.\n",
    "\n",
    "- The error term is instead handled in the constraints, where we set the absolute error less than or equal to a specified maximum error, $\\epsilon$.\n",
    "\n",
    "- For any data point that falls outside of $\\epsilon$, we can denote its deviation from the margin by a slack variable $\\xi$:\n",
    "$$\n",
    "\\vert y_i - {\\bf x}_i\\, \\beta \\vert \\le \\epsilon + \\vert \\xi_i \\vert\n",
    "$$\n",
    "\n",
    "- We minimize the $L_2$ norm of the coefficient vector and these deviations ising an additional hyperparameter $C$:\n",
    "$$\n",
    "\\min \\frac{1}{2} \\| \\beta \\|^2 + C \\sum_{i=1}^n \\vert \\xi_i \\vert^2\n",
    "$$\n",
    "\n",
    "In order to illustrate the support vector regression, let's create a noisy linear dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize random seed\n",
    "np.random.seed(19)\n",
    "\n",
    "# Create a linear dataset\n",
    "points = 100\n",
    "\n",
    "x = np.linspace(-5.0, 5.0, num=points, endpoint=True)\n",
    "y = x + 1.0 + np.random.normal(0.0, 2.0, points)\n",
    "\n",
    "# Create list with 2 points in interval x:[-5,5]\n",
    "x_pred = np.linspace(-5.0, 5.0, num=2, endpoint=True)\n",
    "\n",
    "X = x.reshape(-1,1)\n",
    "y = y.reshape(-1,1)\n",
    "\n",
    "X_pred = x_pred.reshape(-1,1)\n",
    "\n",
    "# Set axes and labels\n",
    "fig = plt.figure( figsize = (8,6) )\n",
    "\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "ax.set_xlabel('x', fontsize=18)\n",
    "ax.set_ylabel('y', fontsize=18)\n",
    "\n",
    "plt.scatter(x, y, color=blue)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Linear Regression\n",
    "\n",
    "First we start with our well-know linear regression.\n",
    "\n",
    "> Split the dataset into 80% for training and 20% for testing. Create the plot and report\n",
    "> - Mean Absolute Deviation between regression and reference data,\n",
    "> - Intercept coefficient for the linear regression, and\n",
    "> - Slope for the linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Support Vector Regression\n",
    "\n",
    "We can define a function to help us plot results more efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svr_results(y_test, X_test, model, epsilon=0.0):\n",
    "    \n",
    "    test_mae = mean_absolute_error(y_test, model.predict(X_test))\n",
    "\n",
    "    within_epsilon            = np.sum( np.abs(y_test - model.predict(X_test)) <= epsilon )\n",
    "    percentage_within_epsilon = 100.0*within_epsilon/y_test.size\n",
    "    \n",
    "    # Plot outputs\n",
    "    plt.figure(figsize=(12,8))\n",
    "\n",
    "    plt.scatter(x, y, color=blue)\n",
    "\n",
    "    plt.plot(X_pred, model.predict(X_pred),           color=orange, lw=4)\n",
    "    plt.plot(X_pred, model.predict(X_pred) + epsilon, color='g', ls='dashed', lw=4)\n",
    "    plt.plot(X_pred, model.predict(X_pred) - epsilon, color='g', ls='dashed', lw=4)\n",
    "\n",
    "    plt.title('SVR Prediction', fontsize=20)\n",
    "\n",
    "    plt.xlabel('x', fontsize=18)\n",
    "    plt.ylabel('y', fontsize=18)\n",
    "    \n",
    "    plt.annotate(f'Slope = {model.coef_[0]:.4f}',\n",
    "                 xy=(0.05,0.9), xycoords='axes fraction', fontsize=18)\n",
    "\n",
    "    plt.annotate(f'Intercept = {model.intercept_[0]:.4f}',\n",
    "                 xy=(0.05,0.85), xycoords='axes fraction', fontsize=18)\n",
    "\n",
    "    plt.annotate(f'MAE = {test_mae:.4f}',\n",
    "                 xy=(0.05,0.8), xycoords='axes fraction', fontsize=18)\n",
    "\n",
    "    plt.annotate(r'Percentage within $\\epsilon$ = {:.1f}'.format(percentage_within_epsilon),\n",
    "                 xy=(0.05,0.75), xycoords='axes fraction', fontsize=18)\n",
    "\n",
    "    plt.annotate(r'$\\epsilon$ = {:.4f}'.format(epsilon),\n",
    "                 xy=(0.65,0.2), xycoords='axes fraction', fontsize=18)\n",
    "\n",
    "    plt.annotate(r'$C$ = {:.4f}'.format(model.C),\n",
    "                 xy=(0.65,0.15), xycoords='axes fraction', fontsize=18)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's use a small hyperparameter $C$ for the constraint on the slack variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 2.0\n",
    "\n",
    "support_vector_regression = LinearSVR(epsilon=epsilon, C=0.01, fit_intercept=True)\n",
    "support_vector_regression.fit(X_train, y_train)\n",
    "\n",
    "svr_results(y_test, X_test, model=support_vector_regression, epsilon=epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we increase $C$, the number of points outside the error margins decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 2.0\n",
    "\n",
    "support_vector_regression = LinearSVR(epsilon=epsilon, C=1.0, fit_intercept=True)\n",
    "support_vector_regression.fit(X_train, y_train)\n",
    "\n",
    "svr_results(y_test, X_test, model=support_vector_regression, epsilon=epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We vary the hyperparameter $C$ to obtain an optimal value minimizing the deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mae_list = []\n",
    "percentage_within_epsilon_list = []\n",
    "\n",
    "epsilon = 2.0\n",
    "c_values = np.linspace(0.01, 1, num=100)\n",
    "\n",
    "for c in c_values:\n",
    "    varied_svr = LinearSVR(epsilon=epsilon, C=c, fit_intercept=True, max_iter=10000)\n",
    "    \n",
    "    varied_svr.fit(X_train, y_train)\n",
    "\n",
    "    test_mae = mean_absolute_error( y_test, varied_svr.predict(X_test) )\n",
    "    test_mae_list.append(test_mae)\n",
    "    \n",
    "    within_epsilon            = np.sum( abs( y_test - varied_svr.predict(X_test) ) <= epsilon )\n",
    "\n",
    "    percentage_within_epsilon = 100*within_epsilon/y_test.size\n",
    "    percentage_within_epsilon_list.append(percentage_within_epsilon)\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(8,8))\n",
    "\n",
    "ax1.set_xlabel('C', fontsize=18)\n",
    "ax1.set_ylabel('% within Epsilon', color=orange, fontsize=18)\n",
    "\n",
    "ax1.scatter(c_values, percentage_within_epsilon_list, color=orange)\n",
    "\n",
    "ax1.tick_params(axis='y', labelcolor=orange)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax2.set_ylabel('Test MAE', color=blue, fontsize=18)\n",
    "\n",
    "ax2.scatter(c_values, test_mae_list, color=blue)\n",
    "\n",
    "ax2.tick_params(axis='y', labelcolor=blue)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the resulting model closely matches linear regression. Support Vector Regression becomes very powerful for large datasets and when using a kernel instead of linear regression.\n",
    "\n",
    "**Compared to Kernel Ridge Regression, Support Vector Regression learns a sparse model and becomes faster for large datasets.**\n",
    "\n",
    "Now let's see a more challenging test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.load_iris()\n",
    "\n",
    "x = data.data[:,:2]\n",
    "y = data.target\n",
    "\n",
    "# Plot data\n",
    "\n",
    "fig, ax = plt.subplots( figsize=(8,8) )\n",
    "\n",
    "scatter = plt.scatter(x[:,0], x[:,1], c=y, label=y, cmap=plt.cm.viridis, edgecolors='k')\n",
    "\n",
    "plt.title('Clustered data', fontsize=20)\n",
    "\n",
    "plt.xlabel(r'$x_0$', fontsize=18)\n",
    "plt.ylabel(r'$x_1$', fontsize=18)\n",
    "\n",
    "handles, _ = scatter.legend_elements()\n",
    "\n",
    "plt.legend(handles=handles, labels=['Insulator', 'Conductor', 'Semiconductor'], loc='best', fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the classifier\n",
    "def support_vector_classifier(x, y, kernel='linear', regularization=1.0, gamma=1e-2, points=300):\n",
    "    \n",
    "    # Create the classifier\n",
    "    support_vector_machine = svm.SVC(kernel=kernel, C=regularization, gamma=gamma)\n",
    "\n",
    "    support_vector_machine.fit(x, y)\n",
    "\n",
    "    # Create a mesh\n",
    "    x_mesh, y_mesh = np.meshgrid( np.linspace(x[:,0].min()-1.0, x[:,0].max()+1.0, num=points),\n",
    "                                  np.linspace(x[:,1].min()-1.0, x[:,1].max()+1.0, num=points) )\n",
    "    \n",
    "    # stack X and y column-wise\n",
    "    xy = np.column_stack( [np.ravel(x_mesh), np.ravel(y_mesh)] )\n",
    "\n",
    "    z_mesh = support_vector_machine.predict(xy)\n",
    "\n",
    "    z_mesh = z_mesh.reshape(x_mesh.shape)\n",
    "\n",
    "    return x_mesh, y_mesh, z_mesh, support_vector_machine\n",
    "\n",
    "# Plot classification regions\n",
    "def plot_classifier(x, y, x_mesh, y_mesh, z_mesh, text=None):\n",
    "\n",
    "    fig, ax = plt.subplots( figsize=(8,8) )\n",
    "\n",
    "    ax.contourf(x_mesh, y_mesh, z_mesh, cmap=plt.cm.viridis, alpha=0.3)\n",
    "\n",
    "    scatter = plt.scatter(x[:,0], x[:,1], c=y, label=y, cmap=plt.cm.viridis, edgecolors='k')\n",
    "\n",
    "    plt.title('Clustered data', fontsize=20)\n",
    "\n",
    "    plt.xlabel(r'$x_0$', fontsize=18)\n",
    "    plt.ylabel(r'$x_1$', fontsize=18)\n",
    "\n",
    "    handles, _ = scatter.legend_elements()\n",
    "\n",
    "    plt.legend(handles=handles, labels=['Insulator', 'Conductor', 'Semiconductor'], loc='lower right', fontsize=18)\n",
    "\n",
    "    if text:\n",
    "        plt.text(0.05, 0.90, text, transform=ax.transAxes, fontsize=18)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularization parameter\n",
    "C = 1.0\n",
    "\n",
    "# Divide data into 80% for training and 20% for testing\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8, random_state=12)\n",
    "\n",
    "# Train the classifier\n",
    "x_mesh, y_mesh, z_mesh, svm_classifier = support_vector_classifier(x_train, y_train, regularization=C)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = svm_classifier.score(x_test, y_test)\n",
    "\n",
    "# Plot data\n",
    "plot_classifier(x, y, x_mesh, y_mesh, z_mesh, text=f'Accuracy = {100.0*accuracy:.2f} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Assignment\n",
    ">\n",
    "> - Optimize the regularization parameter for the SVM classifier. \n",
    "> - Plot the classification regions for that optimum value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we only discused the linear kernel, but `sklearn` also provides other kernels. Let's see how the Radial Basis Function (RBF) kernel performs. The main characteristic of this kernel is that we can define a region where a point influences another point in the set. It's value often is seen as the inverse of this influence radius."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularization parameter\n",
    "C = 1.0\n",
    "\n",
    "x_mesh, y_mesh, z_mesh, svm_classifier = support_vector_classifier(x_train, y_train, kernel='rbf', regularization=C, gamma=1.0)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = svm_classifier.score(x_test, y_test)\n",
    "\n",
    "# Plot data\n",
    "plot_classifier(x, y, x_mesh, y_mesh, z_mesh, text=f'Accuracy = {100.0*accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the effects of both $C$ and $\\gamma$ on the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_values     = [0.01, 1.0, 10.0]\n",
    "gamma_values = [0.01, 10.0, 100.0]\n",
    "\n",
    "rows, cols   = 3, 3\n",
    "\n",
    "fig, ax      = plt.subplots(nrows=rows, ncols=cols, figsize=(8,8), layout='tight')\n",
    "\n",
    "for row in range(rows):\n",
    "    for col in range(cols):\n",
    "        \n",
    "        x_mesh, y_mesh, z_mesh, _ = support_vector_classifier(x_train, y_train, kernel='rbf',\n",
    "                                                              regularization=C_values[row], gamma=gamma_values[col])\n",
    "\n",
    "        ax[row,col].contourf(x_mesh, y_mesh, z_mesh, cmap=plt.cm.viridis, alpha=0.3)\n",
    "\n",
    "        ax[row,col].scatter(x[:,0], x[:,1], c=y, label=y, cmap=plt.cm.viridis, edgecolors='k')\n",
    "\n",
    "        ax[row,col].set_title(r'$C$ = {}, $\\gamma$ = {}'.format(C_values[row], gamma_values[col]), fontsize=18)\n",
    "\n",
    "        ax[row,col].set_xlabel(None)\n",
    "        ax[row,col].set_ylabel(None)\n",
    "\n",
    "        ax[row,col].set_xticks([])\n",
    "        ax[row,col].set_yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see clearly that the choice of the regularization parameter $C$ and the kernel parameter $\\gamma$ are very important for the performance of the classifier.\n",
    "    \n",
    "> ### Assignment\n",
    ">\n",
    "> use a grid search to find the best parameters for the classifier. And plot the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
