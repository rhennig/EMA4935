{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Math - Probability and Statistics\n",
    "\n",
    "Probability and Statistics are the foundation of Data Science. In fact, the underlying principles of machine learning and artificial intelligence are nothing but statistics, linear algebra, and differential calculus.\n",
    "\n",
    "**Probability** is the branch of mathematics concerning numerical descriptions of how likely an event is to occur, or how likely it is that a proposition is true. **Statistics** is the discipline that concerns the collection, organization, analysis, interpretation, and presentation of data.\n",
    "\n",
    "In this tutorial, we will\n",
    "- Learn about probability rules and terms like random variables, probability distributions functions\n",
    "- Learn about different probability distributions functions along with some of their properties.\n",
    "- Learn to create and plot these distributions in python.\n",
    "\n",
    "**Sources:** This notebook uses materials from the hackerearth statistics tutorial https://www.hackerearth.com/practice/machine-learning/, the datacamp tutorial on probability https://www.datacamp.com/community/tutorials/probability-distributions-python., and the lecture notes from Probability Theory for Machine Learning\n",
    "by Chris Cremer, https://www.cs.toronto.edu/~urtasun/courses/CSC411_Fall16/tutorial1.pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries\n",
    "\n",
    "Make sure we have installed the required plotting and image packages for this notebook. If not, install them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install vennfig\n",
    "# %pip install seaborn\n",
    "# %pip install scikit-learn\n",
    "\n",
    "import numpy   as np\n",
    "import pandas  as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import vennfig as venn\n",
    "import sklearn as sk\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Motivation\n",
    "Uncertainty arises from:\n",
    "- Noisy measurements\n",
    "- Finite size of data sets\n",
    "- Ambiguity: The word bank can mean (1) a financial institution, (2) the side of a river, or (3) tilting an airplane. Which meaning was intended, based on the words that appear nearby?\n",
    "- Limited Model Complexity\n",
    "\n",
    "Probability theory provides a consistent framework for the quantification\n",
    "and manipulation of uncertainty. It enables optimal predictions given all the information available, even though that information may be incomplete or ambiguous.\n",
    "\n",
    "## 2. Terminology for probability, events, and sample space\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/15/Coin_Toss_%283635981474%29.jpg/440px-Coin_Toss_%283635981474%29.jpg\" alt=\"Coin Flip\" align=\"right\" style=\"width: 200px;float: right;\"/>\n",
    "\n",
    "The **probability** gives the information about how likely an event can occur. The probability of an event is a number between 0 and 1, where, roughly speaking, 0 indicates impossibility of the event and 1 indicates certainty. The higher the probability of an event, the more likely it is that the event will occur.\n",
    "\n",
    "The **sample space** Ω is the set of possible outcomes of an experiment. Points ω in Ω are called **sample outcomes**, realizations, or elements. Subsets of Ω are called **events**.\n",
    "\n",
    "Example: If we toss a coin twice then the sample space is Ω = {HH, HT, TH, TT}. The event that the first toss is heads is A = {HH, HT}.\n",
    "\n",
    "We say that events $A_1$ and $A_2$ are **disjoint** (mutually exclusive) if the events $A_1$ and $A_2$ have no sample outcomes in common. Mathematically, the intersection of the events $A_1$ and $A_2$ is the empty set, i.e., $A_i ∩ A_j = \\{\\}$.\n",
    "\n",
    "Example: First flip being heads and first flip being tails are disjoint events.\n",
    "\n",
    "## 3. Basic probability calculation\n",
    "\n",
    "As per the definition, if $A$ is an event of an experiment and it contains $n$ outcomes and Ω is the sample space then the probability of $A$ is\n",
    "$$\n",
    "P(A) = \\sum_{i=1}^n P(E_i),\n",
    "$$\n",
    "where $E_1 \\dots E_n$ are the outcomes in $A$. If all the outcomes of the experiment are equally likely then\n",
    "\n",
    "$$\n",
    "P(A) = \\frac{\\text {No. of outcomes in A}}{\\text{No. of outcomes in Ω}}.\n",
    "$$\n",
    "\n",
    "Hence the value of probability is between 0 and 1. As the sample space is the whole possible set of outcomes, $P(S) = 1.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Complement of A:** Complement of an event $A$ means not($A$). Probability of complement event of $A$ means the probability of all the outcomes in sample space other than the ones in $A$. Denoted by $A^c$ and\n",
    "$$\n",
    "P(A^c) = 1 - P(A).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venn.complement(title_a='Event A', title_b='Complement of Event A', set_a='A', set_b=\"A'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Union and Intersection:** The union of two events $A$ and $B$ is the combination of all outcomes contained in the events. The intersection of two events $A$ and $B$ is the set of outcomes of $A$ that also belong to $B$.\n",
    "\n",
    "The probability of intersection of two events $A$ and $B$ is $P(A\\cap B)$. When event $A$ occurs in union with event $B$ then the probability together is defined as\n",
    "$$\n",
    "P(A \\cup B) = P(A) + P(B)- P(A \\cap B),\n",
    "$$\n",
    "which is also known as the **addition rule of probability**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venn.true_false(subs=2, size=6, bg_color='white', title_a='Union of A and B')\n",
    "\n",
    "venn.and_nand(subs=2, size=6, fill_color='#f55faa', bg_color='white', \n",
    "              title_a='Intersection of events A and B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mutually exclusive:** Any two events are mutually exclusive, meaning they cannot both happen at the same time, when they have non-overlapping outcomes. If $A$ and $B$ are two mutually exclusive events then,\n",
    "$P(A \\cap B) = 0$. From the addition rule of probability\n",
    "$P(A \\cup B) = P(A) + P(B)$ as $A$ and $B$ are disjoint or mutually exclusive events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venn.mut_exclusive(size=6, title='A ∩ B = 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Joint, Marginal, and Conditional Probabilities\n",
    "\n",
    "https://machinelearningmastery.com/joint-marginal-and-conditional-probability-for-machine-learning/\n",
    "\n",
    "In machine learning, we often have many random variables that interact in often complex and unknown ways.\n",
    "\n",
    "There are specific techniques that can be used to quantify the probability for multiple random variables, such as the marginal, joint, and conditional probability. These techniques provide the basis for a probabilistic understanding of fitting a predictive model to data.\n",
    "\n",
    "- **Marginal probability**, $P(A)$, is the probability of an event, $A$, occuring irrespective of the outcome of other events.\n",
    "- **Joint probability**, $P(AB)$, is the probability of two events, $A$ and $B$, occurring simultaneously.\n",
    "- **Conditional probability**, $P(A|B)$, is the probability of one event, $A$, given a second event, $B$.\n",
    "- **Independent events**: Any two events are independent of each other if one has zero effect on the other, *i.e.*, the occurrence of one event does not affect the occurrence of the other.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Origin of the term \"Marginal probability\"\n",
    "\n",
    "“Marginal” comes from the Latin margo, meaning edge or boundary. The name arises from how probabilities were historically computed and displayed:\n",
    " - Joint probabilities were tabulated in tables.\n",
    " - Joint Probability Table $P(X,Y)$\n",
    " \n",
    " |            | **$Y = 0$** | **$Y = 1$** | **Marginal $P(X)$** |\n",
    " |------------|-----------|-----------|------------------------|\n",
    " | **$X = 0$**  | 0.10      | 0.20      | **0.30**               |\n",
    " | **$X = 1$**  | 0.25      | 0.15      | **0.40**               |\n",
    " | **Marginal $P(Y)$** | **0.35** | **0.35** | **1.00** |\n",
    "\n",
    " - To obtain the probability of a single variable, one summed over the other variables.\n",
    " - The resulting totals were written in the margins of the table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Probabilities Rules\n",
    "- Assuming that $P(B) > 0$, the conditional probability of $A$ given $B$:\n",
    "$$\n",
    "P(A|B)=P(AB)/P(B)\n",
    "$$\n",
    "- Product rule for joint probability:\n",
    "$$\n",
    "P(AB) = P(A|B)P(B) = P(B|A)P(A)\n",
    "$$\n",
    "- Two events $A$ and $B$ are **independent** if the joint probability is the product of the marginals\n",
    "$$\n",
    "P(AB) = P(A)P(B)\n",
    "$$\n",
    "<center>\n",
    "or </center>\n",
    "\n",
    "$$\n",
    "P(A|B) = P(A)\n",
    "$$\n",
    "\n",
    "### Example 1:\n",
    "- 80% of students pass the final and 60% pass both the final and the midterm.\n",
    "- What percent of students who passed the final also passed the midterm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reworded: What percent of students passed the midterm, given they passed the final?\n",
    "$$\n",
    "P(M|F) = P(MF) / P(F) = 0.60 / 0.80 = 0.75 = 75\\%\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum Rule and Law of Total Probability\n",
    "- Sum Rule (Marginalization)\n",
    "$$\n",
    "P(A) = \\sum_B P(AB)\n",
    "$$\n",
    "\n",
    "- Law of Total Probability\n",
    "$$\n",
    "P(A) = \\sum_B P(A|B) \\, P(B)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/d/d4/Thomas_Bayes.gif\" alt=\"Thomas Bayes\" align=\"right\" style=\"width: 200px;float: right;\"/>\n",
    "\n",
    "## 5. Bayes' Theorem\n",
    "Bayes' Theorem describes the probability of an event, based on prior knowledge of conditions that might be related to the event. It is named after Thomas Bayes (shown on the right), an English Statistician, Philosopher, Presbyterian Minister living from 1701 to 1761. Bayes' work was published after his dead by Richard Price, who edited and corrected Bayes' manuscript. The modern formulation of Bayes theorem was devised by Pierre-Simon Laplace in 1774, who was unaware of Bayes' work.\n",
    "\n",
    "Bayes' theorem states\n",
    "$$\n",
    "P(H|E) = \\frac{P(H)\\, P(E|H)}{P(E)}.\n",
    "$$\n",
    "\n",
    "(Bayes' Theorem is derived from the product rule for joint probability.)\n",
    "\n",
    "We are interested in calculating the probability $P(H|E)$, which is the conditional probability that the hypthesis, H, is true given the evidence, E. It is called the **posterior probability**.\n",
    "\n",
    "The probability $P(E|H)$ is the probability of seeing the evidence, if the hypothesis is true. It is called the **likelihood**.\n",
    "\n",
    "The probabilities $P(H)$ is the probability that the hypothesis is true before any evidence is present. This marginal probability of call the **prior** probability.\n",
    "\n",
    "The probability $P(E)$ is the probability of observing the **evidence** $E$. For the use of Bayes' Theorem, we often express the probability $P(E)$ by the conditional probabilities of $P(E|H)$ and $P(E|H^c)$:\n",
    "$$\n",
    "P(E) = P(E|H)\\cdot P(H) + P(E|H^c) \\cdot [1-P(H)].\n",
    "$$\n",
    "\n",
    "This leads to the extended form of Bayes Theorem:\n",
    "$$\n",
    "P(H|E) = \\frac{P(H)\\, P(E|H)}{P(E|H)\\cdot P(H) + P(E|H^c) \\cdot [1-P(H)]}.\n",
    "$$\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/18/Bayes%27_Theorem_MMB_01.jpg/640px-Bayes%27_Theorem_MMB_01.jpg\" alt=\"Neon sign of Bayes theorem\" style=\"width:300px; float:right\"/>\n",
    "  \n",
    "This image shows Bayes' theorem spelled out in blue neon at the offices of Autonomy in Cambridge, a company specializing in finger print recognition.\n",
    "\n",
    "For a nice read about Bayes Theorem and testing for rare diseases, I suggest reading the following article in the Guardian: https://www.theguardian.com/world/2021/apr/18/obscure-maths-bayes-theorem-reliability-covid-lateral-flow-tests-probability.\n",
    "\n",
    "### Example of Bayes' Theorem: Sensitivity and Specificity\n",
    "\n",
    "Bayes' theorem elegantly demonstrates the effect of false positives and false negatives in medical tests.\n",
    "\n",
    "**Sensitivity** is the true positive rate. It measures the fraction of correctly identified positives. In a Covid-19 test, it is be the percentage of people with a positive test who have Covid. A sensitive test rarely misses a positive.\n",
    "\n",
    "**Specificity** is the true negative rate. It measures the fraction of correctly identified negatives. In a Covid-19 test, it is be the percentage of people with a negative test who do not have Covid. A specific test rarely registers a false positive.\n",
    "\n",
    "A perfect test would be 100 percent sensitive and specific. In reality, tests have a minimum error called the Bayes error rate.\n",
    "\n",
    "For example, consider a Covid-19 test that is 90% sensitive and 80% specific. If 5% of people have Covid-19, what is the probability a random person with a positive test actually has Covid?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relative population that has Covid\n",
    "relative_population_with_covid          = 0.05\n",
    "\n",
    "# True positive rate (sensitivity). The probability of having Covid when the test was positive.\n",
    "positive_population_with_positive_covid = 0.90\n",
    "\n",
    "# True negative rate (specificity). The probability of not having Covid when the test was negative.\n",
    "negative_population_with_negative_covid = 0.80\n",
    "\n",
    "# False positive rate. Probability of having Covid when the test was negative.\n",
    "positive_population_with_negative_covid = 1.0 - negative_population_with_negative_covid\n",
    "\n",
    "# Use the Law of Total Probability to calculate the denominator of Bayes' Theorem P(E),\n",
    "# the probability of having a postive test\n",
    "\n",
    "propability_of_positive_test = relative_population_with_covid * positive_population_with_positive_covid + \\\n",
    "                               positive_population_with_negative_covid * (1.0 - relative_population_with_covid)\n",
    "\n",
    "# Bayes theorem\n",
    "probability_positive_test_with_covid = relative_population_with_covid * positive_population_with_positive_covid / propability_of_positive_test\n",
    "\n",
    "print(f\"The probability of a random person with a \"\n",
    "      f\"positive test having Covid is {probability_positive_test_with_covid*100.0:.1f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Assignment\n",
    ">\n",
    "> 1. How does this probability change when Covid is more prevalent, like 20 % of the population?\n",
    "> 2. What happens for very rare diseases that only occur for 1 in 10,000 people for the same test accuracy?\n",
    "> 3. How does this probability change for tests with higher specificity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Confusion Matrix\n",
    "\n",
    "<img src=\"https://github.com/rhennig/EMA6938/blob/main/Notebooks/Figures/ConfusionMatrix.png?raw=1\" alt=\"Confusion Matrix\" align=\"right\" style=\"width:400px; float:right\"/>\n",
    "\n",
    "A confusion matrix is a table summarizing the number or percentage of correct and incorrect predictions. In machine learning, it is used to describe the performance of a classification model.\n",
    "\n",
    "For a boolean classifier (Positive/Negative), the confusion matrix shown on the right contains four basic entries:\n",
    "\n",
    "- **True Positives (TP)**: when the actual value is Positive and the prediction is also Positive.\n",
    "- **True Negatives (TN):** when the actual value is Negative and the prediction is also Negative.\n",
    "- **False Positives (FP):** When the actual value is Negative but the prediction is Positive. This is denoted as Type 1 error.\n",
    "- **False Negatives (FN):** When the actual value is Positive but the prediction is Negative. This is denoted as Type 2 error.\n",
    "\n",
    "A good model has high TP and TN fractions and low FP and FN fractions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Performance Measures\n",
    "There are various metrics to measure the performance of a classification model, accuracy, precision, recall and others.\n",
    "\n",
    "#### Accuracy\n",
    "\n",
    "- Accuracy measures how often the classifier makes the correct prediction. It’s the ratio between the number of correct predictions and the total number of predictions:\n",
    "<img src=\"https://github.com/rhennig/EMA6938/blob/main/Notebooks/Figures/ConfusionMatrix-Accuracy.png?raw=1\" alt=\"Confusion Matrix Accuracy\" align=\"right\" style=\"width:300px; float:right\"/>\n",
    "\n",
    "$$\n",
    "\\mathrm{ACC = \\frac{TP + TN}{P + N}}.\n",
    "$$\n",
    "\n",
    "- Accuracy measures correctness that is achieved in true prediction. It describes how many predictions are actually correct out of all the total positive and negative predictions. It is a useful metric for evaluation a classification problem, which is well balanced and not skewed and has no imbalance in the number of entries for each class. For imbalanced data, the accuracy can be high if the model predicts that each point belongs to the majority class. However, in that case the model is not accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/rhennig/EMA6938/blob/main/Notebooks/Figures/ConfusionMatrix-Precision.png?raw=1\" alt=\"Confusion Matrix Precision\" align=\"right\" style=\"width:300px; float:right\"/>\n",
    "\n",
    "#### Precision\n",
    "\n",
    "- Precision measures the correctness that is achieved for true predictions. It describes how many predictions are actually positive among all positive predictions.\n",
    "\n",
    "$$\n",
    "\\mathrm{PREC = \\frac{TP}{TP+FP}}.\n",
    "$$\n",
    "\n",
    "- Precision is a useful measure when False Positive is a higher concern than False Negatives.\n",
    "\n",
    "- Examples where classifiers need to focus on minimizing False Positives are spam detection and recommendation systems. In each of these cases, the goal is to reduce False Positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/rhennig/EMA6938/blob/main/Notebooks/Figures/ConfusionMatrix-Recall.png?raw=1\" alt=\"Confusion Matrix Recall\" align=\"right\" style=\"width:300px; float:right\"/>\n",
    "\n",
    "#### Recall\n",
    "\n",
    "- Recall (or true positive rate) measures actual observations that are predicted correctly, i.e. how many observations of positive class are actually predicted as positive. It is also known as Sensitivity.\n",
    "\n",
    "$$\n",
    "\\mathrm{REC = \\frac{TP}{TP+FN}}.\n",
    "$$\n",
    "\n",
    "- Recall is a useful evaluation metric when we want to capture as many positives as possible.\n",
    "\n",
    "- Recall is a useful metric in cases where False Negative is more important than False Positive.\n",
    "\n",
    "- An example are medical cases where it often matters more that actual positive cases do not go undetected than raising a false alarm. Recall is the better metric because we want to treat sick people and avoid infected people to mix with the healthy population thereby spreading a contagious virus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependence of Performance Measures on Prevalence\n",
    "\n",
    "- Prevalence is the fraction of positive cases in the test set and a measure of class imbalance.\n",
    "  $$\n",
    "  \\text{Prevalence} = \\frac{P}{P+N}\n",
    "  $$\n",
    "- Importantly, some of the above performance measures depend on the prevalence.\n",
    "\n",
    "\n",
    "### Definition of TPR and TNR\n",
    "- **True Positive Rate (TPR) / Sensitivity / Recall**:  \n",
    "  $$\n",
    "  \\text{TPR} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n",
    "  $$\n",
    "  Measures the proportion of actual positives that are correctly identified.\n",
    "\n",
    "- **True Negative Rate (TNR) / Specificity**:  \n",
    "  $$\n",
    "  \\text{TNR} = \\frac{\\text{TN}}{\\text{TN} + \\text{FP}}\n",
    "  $$\n",
    "  Measures the proportion of actual negatives that are correctly identified.\n",
    "\n",
    "### Recall and Specificity as Intrinsic Performance Measures to the Classifier\n",
    "- TPR and TNR depend only on the classifier's performance on each class separately. They do not incorporate how often each class appears in the dataset.\n",
    "- These metrics are calculated *within each class*, meaning they are independent of the relative sizes of the positive and negative groups in the dataset.\n",
    "\n",
    "### Accuracy and Precision are Prevalence-Dependent Measures\n",
    "- **Accuracy**: Affected by class imbalance since it is computed as:\n",
    "  $$\n",
    "  \\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}}\n",
    "  $$\n",
    "  If the dataset is highly imbalanced, accuracy can be misleading.\n",
    "\n",
    "- **Positive Predictive Value (PPV, or Precision)**:\n",
    "  $$\n",
    "  \\text{PPV} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}\n",
    "  $$\n",
    "  PPV depends on prevalence because the denominator includes both true and false positives, meaning that a classifier’s precision can change if the proportion of positive samples changes.\n",
    "\n",
    "### Implications in Evaluation\n",
    "- Since TPR and TNR are independent of prevalence, they are more reliable for comparing classifier performance across datasets with different class distributions.\n",
    "- In contrast, prevalence-dependent metrics (e.g., precision, accuracy) can vary depending on dataset composition.\n",
    "\n",
    "Thus, TPR and TNR are **intrinsic properties of a classifier** because they reflect its ability to correctly classify individual instances within each class without being influenced by class distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Case\n",
    "\n",
    "Let's simulate the case above for 1,000,000 people and estimate the number of positive and negative tests results and the number of people that have Covid or are healthy. We will use some more realistic values for the sensitivity and specificity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import uniform distribution from scipy\n",
    "from scipy.stats import bernoulli\n",
    "\n",
    "total_population    = 1000000\n",
    "sick_probability    = 0.05\n",
    "testing_sensitivity = 0.9\n",
    "testing_specificity = 0.8\n",
    "\n",
    "# Generate bernoulli for a total population of 1,000,000 \n",
    "population_distribution = bernoulli.rvs(size=total_population, p=sick_probability)\n",
    "\n",
    "sick_population         = population_distribution[population_distribution==1]\n",
    "healthy_population      = population_distribution[population_distribution==0]\n",
    "\n",
    "print(f\"People with Covid-19 = {sick_population.size}\")\n",
    "print(f\"Healthy people       = {healthy_population.size}\")\n",
    "\n",
    "# Next, apply the test with a sensitivity of 90% and a specificity of 80%\n",
    "positive_sick_population     = bernoulli.rvs(size=total_population, p=testing_sensitivity)\n",
    "negative_healthy_population  = bernoulli.rvs(size=total_population, p=testing_specificity)\n",
    "\n",
    "test_result = positive_sick_population*population_distribution + \\\n",
    "              (1.0 - negative_healthy_population) * (1.0 - population_distribution)\n",
    "\n",
    "print(f\"Positive tests       = {test_result[test_result==1].size}\")\n",
    "print(f\"Negative tests       = {test_result[test_result==0].size}\")\n",
    "\n",
    "# Process data by replacing numbers with strings\n",
    "population_labeled  = np.where(population_distribution==1, 'Sick', 'Healthy')\n",
    "test_result_labeled = np.where(test_result==1, 'Positive', 'Negative')\n",
    "\n",
    "population_data = {\"Population\": population_labeled,\n",
    "                   \"Test\"      : test_result_labeled}\n",
    "\n",
    "# Here we us the tools of the Pandas Dataframe Library to calculate the confusion matrix\n",
    "df = pd.DataFrame(population_data, columns=['Population', 'Test'])\n",
    "\n",
    "confusion_matrix_data = pd.crosstab(df['Population'], df['Test'],\n",
    "                                    rownames=['Population'], colnames=['Test'], \n",
    "                                    margins = True)\n",
    "\n",
    "print(f\"\\nConfusion matrix:\\n{confusion_matrix_data}\")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "\n",
    "sns.heatmap(confusion_matrix_data, annot=True, fmt='g', cmap='Blues', cbar=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also use scikitlearn to calculate the confusion matrix\n",
    "# Here, we will display the labels, numbers, and percentages\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix_data    = confusion_matrix(population_distribution, test_result)\n",
    "\n",
    "group_names       = ['True Neg','False Pos','False Neg','True Pos']\n",
    "\n",
    "group_counts      = [\"{0:0.0f}\".format(value) for value in confusion_matrix_data.flatten()]\n",
    "\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in confusion_matrix_data.flatten()/np.sum(confusion_matrix_data)]\n",
    "\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\n",
    "\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "\n",
    "sns.heatmap(confusion_matrix_data, annot=labels, fmt='', \n",
    "            xticklabels=['Negative Test', 'Positive Test'],\n",
    "            yticklabels=['Healthy', 'Sick'],\n",
    "            cmap='Blues', cbar=False)\n",
    "\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Actual Values')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Assignment\n",
    ">\n",
    "> 1. What is the number of true positives and false negatives?\n",
    "> 2. What is the probability of a sick person to have a negative test result?\n",
    "> 3. What is the probability of a healthy person to have a positive test result?\n",
    "> 4. What is the probability of a random person with a positive test result to have Covid?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Random Variables\n",
    "\n",
    "### Discrete and Continuous Random Variables\n",
    "\n",
    "A random variable is a variable whose possible values are numerical outcomes of a random phenomenon. \n",
    "\n",
    "Example: Flip a coin ten times. Let X(ω) be the number of heads in the sequence ω. If ω = HHTHHTHHTT, then X(ω) = 6.\n",
    "\n",
    "There are two types of random variables, discrete and continuous.\n",
    "\n",
    "A **discrete random variable** takes on only a countable number of distinct values and thus can be quantized.\n",
    "\n",
    "Example:\n",
    "- The number that comes up when you roll a fair dice. X can take values : [1, 2, 3, 4, 5, 6].\n",
    "- The number of heads coming up when rolling a dice a fixed number of times.\n",
    "\n",
    "A **continuous random variable** can take on infinitely many values and is described by a real number.\n",
    "\n",
    "Example:\n",
    "- Time taken to accomplish task.\n",
    "- Height of graduate students in MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability Distribution\n",
    "\n",
    "A probability distribution is the mathematical function that gives the probabilities of occurrence of different possible outcomes for an experiment.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/85/Discrete_probability_distrib.svg/640px-Discrete_probability_distrib.svg.png\" alt=\"Probability distribution\" align=\"right\" style=\"width: 200px;float: right;\"/>\n",
    "\n",
    "The **probability distribution of a discrete random variable** is a list of probabilities associated with each of its possible values. It is also sometimes called the probability function or the probability mass function. An example is shown on the right for a random variable describing three possible events.\n",
    "\n",
    "Suppose a random variable $X$ takes $k$ different values, with the probability that $X = x_i$ given by $P(x_i) = p_i$. The probabilities $p_i$ must satisfy\n",
    "\n",
    "1) $0 < p_i < 1$ for each i\n",
    "2) $\\sum_{i=1}^k p_i = 1$.\n",
    "\n",
    "Examples of discrete probability distributions are the Bernoulli, Binomial, and Poisson distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Standard_deviation_diagram.svg/640px-Standard_deviation_diagram.svg.png\" alt=\"Probability distribution function\" align=\"right\" style=\"width: 200px;float: right;\"/>\n",
    "\n",
    "A continuous random variable is defined over an interval of values. The **probability distribution of a continuous random variable**, known as probability distribution functions, are the functions that take on continuous values. The example of a normal or Gaussian probability distribution function is shown on the right.\n",
    "\n",
    "The probability of observing any single value is equal to 0 since the number of values which may be assumed by the random variable is infinite. \n",
    "\n",
    "A random variable $X$ may take all values over an interval of real numbers. Then the probability $P(A)$ that $X$ is in the set of outcomes $A$ is defined to be the area in the curve over the set $A$.\n",
    "\n",
    "The curve, which represents a function $p(x)$, must satisfy the following:\n",
    "1) The curve has no negative values (p(x)>0 for all x\n",
    "2) The total area under the curve is equal to 1, $\\int p(x) dx = 1$.\n",
    "\n",
    "Some examples of continuous probability distributions are normal the distribution, exponential, and beta distribution.\n",
    "\n",
    "There’s another type of distribution that is relevant for random variables, the cumulative distribution function. All random variables (discrete and continuous) have a cumulative distribution function. It is a function giving the probability that the random variable $X$ is less or equal than $x$, for every value $x$. For a discrete random variable, the cumulative distribution function is found by summing up the probabilities. For continuous random variables, it is given by the integral from $-\\infty$ to $x$.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/5/54/Combined_Cumulative_Distribution_Graphs.png/800px-Combined_Cumulative_Distribution_Graphs.png\" alt=\"Probability distribution function\" align=\"center\" style=\"width: 600px;float: center;\"/>\n",
    "\n",
    "The left figure shows the probability distribution function (pdf) $f(x)$ and the right one shows the cummulative distribution function (cdf) $F(x) = \\int_{-\\infty^x} p(x') dx'$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, we will explore some important distributions and try to work them out in python. First we will setup our plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for seaborn plotting style\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "# Settings for seaborn plot sizes\n",
    "sns.set(rc={'figure.figsize':(6,4)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Uniform Distribution\n",
    "\n",
    "We can import uniform distribution from scipy.stats and use it to generate uniform random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import uniform distribution\n",
    "from scipy.stats import uniform\n",
    "\n",
    "# Generate random numbers from uniform distribution\n",
    "n     = 10000\n",
    "start = 0\n",
    "width = 10\n",
    "\n",
    "data_uniform = uniform.rvs(size=n, loc = start, scale=width)\n",
    "\n",
    "# Plot the binned data in blue. The orange line indicates the kernel density estimation (KDE) of the probability function\n",
    "# For more information, see https://en.wikipedia.org/wiki/Kernel_density_estimation\n",
    "\n",
    "sns.histplot(data_uniform, stat='density', bins=40, color='skyblue')\n",
    "\n",
    "sns.kdeplot(data_uniform, color=\"orange\")\n",
    "\n",
    "plt.xlabel('Uniform Distribution')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel density estimation\n",
    "\n",
    "If you like to learn more about how scikit learn performs the kernel density estimation, here is a good summary of the method: https://scikit-learn.org/stable/modules/density.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Normal or Gaussian Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import normal distribution\n",
    "from scipy.stats import norm\n",
    "\n",
    "# generate random numbersfrom N(0,1)\n",
    "data_normal = norm.rvs(size=10000, loc=0, scale=1)\n",
    "\n",
    "sns.histplot(data_normal, stat='density', bins=50, color='skyblue')\n",
    "\n",
    "sns.kdeplot(data_normal, color=\"orange\")\n",
    "\n",
    "plt.xlabel('Normal Distribution')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Binomial Distribution\n",
    "The binomial distribution with parameters $n$ and $p$ is the discrete probability distribution of the number of successes in a sequence of $n$ independent experiments, each asking a yes/no question. Each question has the probability of $p$ for yes and $1-p$ for no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import binom\n",
    "from scipy.stats import binom\n",
    "\n",
    "# generate binom\n",
    "data_binom = binom.rvs(n=20, p=0.8,size=10000)\n",
    "\n",
    "sns.histplot(data_binom, stat='density', color='skyblue')\n",
    "\n",
    "plt.xlabel('Binomial Distribution')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the probability of success was 0.8 `p=0.8`, which is greater than 0.5 the distribution is skewed towards the right side.\n",
    "\n",
    "Also, the Poisson distribution is a limiting case of a binomial distribution under the following conditions:\n",
    "- The number of trials is infinitely large or $n \\to \\infty$.\n",
    "- The probability of success for each trial is same and infinitely small or $p \\to 0$.\n",
    "- The product $np=\\lambda$ remains finite.\n",
    "\n",
    "The Normal distribution is another limiting form of binomial distribution under the following conditions:\n",
    "- The number of trials is infinitely large, $n \\to \\infty$.\n",
    "- Neither $p$ nor $q$ are infinitely small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Bernoulli Distribution\n",
    "\n",
    "The Bernoulli distribution is named after the Swiss mathematician Jacob Bernoulli. It is the discrete probability distribution of a random variable which takes the value 1 with probability $p$ and the value 0 with probability $q=1-p$. It can be thought of as a model for the set of possible outcomes of any single experiment that asks a yes/no question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import bernoulli\n",
    "from scipy.stats import bernoulli\n",
    "\n",
    "# generate bernoulli\n",
    "data_bern = bernoulli.rvs(size=10000, p=0.3)\n",
    "\n",
    "\n",
    "sns.histplot(data_bern, stat='density', color='skyblue')\n",
    "\n",
    "plt.xlabel('Bernoulli Distribution')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poisson Distribution\n",
    "\n",
    "The Poisson distribution is named after the French mathematician Siméon Denis Poisson. It is a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time or space if these events occur with a known constant mean rate and independently of the time since the last event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Poisson distribution\n",
    "from scipy.stats import poisson\n",
    "\n",
    "data_poisson = poisson.rvs(mu=3, size=10000)\n",
    "\n",
    "sns.histplot(data_poisson, stat='density', color='skyblue')\n",
    "\n",
    "plt.xlabel('Poisson Distribution')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Assignment\n",
    ">\n",
    "> Create plots for the Binomial distribution that illustrates how the probability distribution function changes to one that approaches the Poisson distribution and the normal distribution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
