{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c69d96b3",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d343efb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy             as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets     import make_regression\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061a8fcc-19f2-4882-949c-6fcd8447039e",
   "metadata": {},
   "source": [
    "# Math - Algebra\n",
    "\n",
    "(Based on https://online.stat.psu.edu/stat462/node/132/ and https://www.geeksforgeeks.org/ml-normal-equation-in-linear-regression)\n",
    "\n",
    "Linear algebra is the branch of mathematics concerning linear equations,\n",
    "$$\n",
    "a_{1}x_{1}+\\cdots +a_{n}x_{n}=b,\n",
    "$$\n",
    "linear maps,\n",
    "$$\n",
    "(x_{1},\\ldots ,x_{n})\\mapsto a_{1}x_{1}+\\cdots +a_{n}x_{n},\n",
    "$$\n",
    "and their representations in vector spaces and through matrices. Linear algebra is a key foundation to the field of machine learning, from the notations used to describe the equations and operation of algorithms to the efficient implementation of algorithms in code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e805e9a-b0af-408f-bc62-4d194ce69ce8",
   "metadata": {},
   "source": [
    "## 1. Motivational Example of Linear Regression\n",
    "\n",
    "We first derive the linear regression model in matrix form. In linear regression, we fit a linear function to a dataset of $n$ data points $(x_i, y_i)$. The linear model is given by\n",
    "$$\n",
    "y(x) = \\beta_0 + \\beta_1 x.\n",
    "$$\n",
    "\n",
    "Linear regression desscribes the data by minimizing the least squares deviation between the data and the linear model:\n",
    "$$\n",
    "y_i = \\beta_0 + \\beta_1 x_i + \\epsilon _i, \\, \\text{for }i = 1, \\dots , n.\n",
    "$$\n",
    "Here the $\\epsilon_i$ describes the deviation between the model and data and are assumed to be Gaussian distributed.\n",
    "\n",
    "Writing out the set of equations for $i = 1, \\dots, n$, we obtain $n$ equations:\n",
    "$$\n",
    "y_1 = \\beta_0 + \\beta_1 x_1 + \\epsilon _1 \\\\\n",
    "y_2 = \\beta_0 + \\beta_1 x_2 + \\epsilon _2 \\\\\n",
    "\\vdots \\\\\n",
    "y_n = \\beta_0 + \\beta_1 x_n + \\epsilon _n \\\\\n",
    "$$\n",
    "\n",
    "We can formulate the above simple linear regression function in matrix notation:\n",
    "$$\n",
    "\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "   1 & x_1 \\\\\n",
    "   1 & x_2 \\\\\n",
    "   \\vdots \\\\\n",
    "   1 & x_n\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "   \\beta_0 \\\\\n",
    "   \\beta_1\n",
    "\\end{bmatrix} +\n",
    "\\begin{bmatrix}\n",
    "   \\epsilon_1 \\\\\n",
    "   \\epsilon_2 \\\\\n",
    "   \\vdots \\\\\n",
    "   \\epsilon_n\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "We can write this matrix equation in a more compact form\n",
    "$$\n",
    "\\mathbf{Y} = \\mathbf{X}\\, \\mathbf{\\beta} + \\mathbf{\\epsilon},\n",
    "$$\n",
    "where\n",
    "- $\\mathbf{X}$ is an $n \\times 2$ matrix.\n",
    "- $\\mathbf{Y}$ is an $n \\times 1$ column vector\n",
    "- $\\mathbf{\\beta}$ is a $2 \\times 1$ column vector\n",
    "- $\\mathbf{\\epsilon}$ is an $n \\times 1$ column vector.\n",
    "\n",
    "The matrix $\\mathbf{X}$ and vector $\\mathbf{\\beta}$ are multiplied together using the techniques of matrix multiplication.\n",
    "And, the vector $\\mathbf{X} \\mathbf{\\beta}$ is added to the vector $\\mathbf{\\epsilon}$ using the techniques of matrix addition.\n",
    "\n",
    "Let's quickly review matrix algebra, the subject of mathematics that deals with operations of matrices, vectors, and tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7a1f44-fc0c-400d-a3ba-4bec10303292",
   "metadata": {},
   "source": [
    "## 2. Least Squares Estimates of Linear Regression Coefficients\n",
    "\n",
    "As we will discuss later, minimizing the mean squared error of model prediction and data leads to the following equation for the coefficient vector ${\\bf \\beta}$:\n",
    "$$\n",
    "\\mathbf{\\beta} = \\begin{bmatrix} \\beta_0 \\\\ \\vdots \\\\ \\beta_k \\end{bmatrix}\n",
    "= ( \\mathbf{X}^\\mathrm{T} \\mathbf{X} )^{-1}\\, \\mathbf{X}^\\mathrm{T}\\, \\mathbf{Y},\n",
    "$$\n",
    "where\n",
    "- $( \\mathbf{X}^\\mathrm{T} \\mathbf{X} )^{-1}$ is the inverse of the $\\mathbf{X}^\\mathrm{T} \\mathbf{X}$ matrix, and\n",
    "- $\\mathbf{X}^\\mathrm{T}$ is the transpose of the $\\mathbf{X}$ matrix.\n",
    "\n",
    "Let's remind ourselves of the transpose and inverse of a matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9110df44-5a0e-4def-b35d-c463df32f4fc",
   "metadata": {},
   "source": [
    "## 3. Transpose of a Matrix\n",
    "\n",
    "The transpose of a matrix $\\mathbf{A}$, denoted as $\\mathbf{A}^\\mathrm{T}$ or $\\mathbf{A}^{\\prime}$, is a matrix whose rows are the columns of $\\mathbf{A}$ and whose columns are the rows of $\\mathbf{A}$. All in the same order.\n",
    "\n",
    "For example, the transpose of the $3 \\times 2$ matrix $\\mathbf{A}$:\n",
    "$$\n",
    "\\mathbf{A} = \\begin{bmatrix} a_{0,0} & a_{0,1} \\\\ a_{1,0} & a_{1,1} \\\\ a_{2,0} & a_{2,1} \\end{bmatrix}\n",
    "$$\n",
    "is the $2 \\times 3$ matrix $\\mathbf{A}^\\mathrm{T}$:\n",
    "$$\n",
    "\\mathbf{A}^\\mathrm{T} = \\begin{bmatrix} a_{0,0} & a_{1,0} & a_{2,0} \\\\ a_{0,1} & a_{1,1} & a_{2,1} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The $\\mathbf{X}$ matrix in the simple linear regression setting is:\n",
    "$$\n",
    "\\mathbf{X} = \\begin{bmatrix}\n",
    "   1 & x_1 \\\\\n",
    "   1 & x_2 \\\\\n",
    "   \\vdots \\\\\n",
    "   1 & x_n\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Hence, the $\\mathbf{X}^\\mathrm{T} \\mathbf{X}$ matrix in the linear regression is:\n",
    "$$\n",
    "\\mathbf{X}^\\mathrm{T} \\mathbf{X} = \\begin{bmatrix}\n",
    "   1 & 1 & \\dots & 1\\\\\n",
    "   x_1 & x_2 & & x_n\n",
    "\\end{bmatrix}\n",
    "\n",
    "\\begin{bmatrix}\n",
    "   1 & x_1 \\\\\n",
    "   1 & x_2 \\\\\n",
    "   \\vdots \\\\\n",
    "   1 & x_n\n",
    "\\end{bmatrix}\n",
    "\n",
    "= \\begin{bmatrix}\n",
    "n & \\sum_{i=1}^n x_i \\\\ \\sum_{i=1}^n x_i & \\sum_{i=1}^n x_i^2\n",
    "\\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327675b3-e09d-4579-8b50-d7f373ffa786",
   "metadata": {},
   "source": [
    "## 4. The Inverse of a Matrix\n",
    "\n",
    "The inverse $\\mathbf{A}^{-1}$ of a **square matrix** $\\mathbf{A}$ is the unique matrix such that:\n",
    "$$\n",
    "\\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I} = \\mathbf{A} \\mathbf{A}^{-1}.\n",
    "$$\n",
    "\n",
    "That is, the inverse of $\\mathbf{A}$ is the matrix $\\mathbf{A}^{-1}$ that you multiply $\\mathbf{A}$ by to obtain the identity matrix $\\mathbf{I}$. Note that the inverse only exists for square matrices.\n",
    "\n",
    "Now, finding inverses, particularly for large matrices, is a complicated task. We will use numpy to calculate the inverses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116fd492-0dca-40f0-b28d-b63d1351d026",
   "metadata": {},
   "source": [
    "## 5. Solution for Linear Regresssion\n",
    "\n",
    "We will use a data set from the Python library sklearn for linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5e0c12-fd33-405c-a92c-973c5ae7ae81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data set\n",
    "x, y = make_regression(n_samples=100, n_features=1, n_informative=1, noise=10, random_state=10)\n",
    " \n",
    "# Plot the data set\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.rcParams['font.size'] = '16'\n",
    "\n",
    "plt.scatter(x, y, s = 30, marker = 'o')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.title('Scatter Data', fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2d9e05-4cf4-44a5-8206-e259a603d035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the vector of y variables into a column vector\n",
    "Y = np.expand_dims(y, axis=-1) \n",
    "\n",
    "# Create matrix X by adding x0 = 1 to each instance of x and taking the transpose\n",
    "X = np.stack( ( np.ones(x.size), np.ravel(x) ), axis=1 )\n",
    "\n",
    "# Determining the coefficients of linear regression\n",
    "\n",
    "# Calculate X^T X\n",
    "XT_times_X = np.matmul(X.T, X)\n",
    "\n",
    "# Calculate (X^T X)^-1\n",
    "XT_times_X_inverse = np.linalg.inv(XT_times_X)\n",
    "\n",
    "# Calculate (X^T Y)\n",
    "XT_times_Y = np.matmul(X.T, Y)\n",
    "\n",
    "# Calculate (X^T X)^-1 (X^T Y)\n",
    "Beta = np.matmul(XT_times_X_inverse, XT_times_Y).reshape(2)\n",
    "\n",
    "# Display best values obtained\n",
    "print(f\"Matrix X =\\n\"\n",
    "      f\"{X[1:5, :]}\\n\\n\")\n",
    "\n",
    "print(f\"Matrix X'X =\\n\"\n",
    "      f\"{XT_times_X}\\n\\n\")\n",
    "\n",
    "print(f\"Inverse of (X'X) =\\n\"\n",
    "      f\"{XT_times_X_inverse}\\n\\n\")\n",
    "\n",
    "print(\"Regression coefficients\\n\"\n",
    "      f\"β0 = {Beta[0]:6.4f}\\n\"\n",
    "      f\"β1 = {Beta[1]:6.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9deca3b",
   "metadata": {},
   "source": [
    "### 5.1 Predict values using the regression coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf98109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the values for given data instance.\n",
    "x_sample     = np.array( [[-2.5],[3]] )\n",
    "\n",
    "# Generatw matrix X\n",
    "X_sample     = np.stack( ( np.ones( x_sample.size), np.ravel(x_sample) ), axis=1 )\n",
    "\n",
    "# Multiply matrix X by the regression coefficients\n",
    "y_predicted  = np.matmul(X_sample, Beta)\n",
    "\n",
    "# Plot the generated data set\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.rcParams['font.size'] = '16'\n",
    "\n",
    "plt.scatter(x, y, s = 30, marker = 'o')\n",
    "plt.plot(x_sample, y_predicted, color='black', lw=2)\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.title('Scatter Data', fontsize=20)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f\"predicted values = {', '.join([f'[ {i[0]:.2f}, {i[1]:.2f} ]' for i in zip(np.ravel(x_sample), y_predicted)])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec8a87b",
   "metadata": {},
   "source": [
    "### 5.2 Now using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fc965d-a2d8-4eb5-84b0-d4dc7a0c7c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression function from scikit-learn\n",
    "linear_regression = LinearRegression()\n",
    "\n",
    "# Fit the model\n",
    "linear_regression.fit(x, y)\n",
    " \n",
    "# Print obtained theta values\n",
    "print(f\"β0 = {linear_regression.intercept_:6.4f}\\n\"\n",
    "      f\"β1 = {linear_regression.coef_[0]:6.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9da25a-6dd7-4ec2-9f29-a0a572efe9e2",
   "metadata": {},
   "source": [
    "> ## Assignment\n",
    ">\n",
    "> The projection matrix converts values from the observed variable $y_i$ into the estimated values $\\hat{y}$  obtained with the least squares method. The projection matrix, $\\mathbf{H}$, is given by\n",
    "> $$\n",
    "> \\mathbf{H} =  \\mathbf{X}\\, (\\mathbf{X}^\\mathrm{T} \\mathbf{X})^{-1}\\, \\mathbf{X}^\\mathrm{T}\n",
    "> $$\n",
    ">\n",
    "> Calculate the projection matrix, $\\mathbf{H}$, and show that you obtain the predicted $y$-values by creating a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a4083e-0b74-4de4-b2a6-0a0c7d065826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the projection matrix\n",
    "\n",
    "\n",
    "\n",
    "# Apply the projection matrix to the y-values to generate the y predictions\n",
    "\n",
    "\n",
    "\n",
    "# Plot the predicted and original y values vs. the x values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f4386e-e28b-4bd8-b6b1-bbeb4d13b67d",
   "metadata": {},
   "source": [
    "> Knowning the projection matrix, $\\mathbf{H}$, we can also express the $R^2$ value for the linear regression using a matrix equation:\n",
    "> $$\n",
    "> R^2 = 1 - \\frac{\\mathbf{y}^\\mathrm{T}\\, (\\mathbf{I} - \\mathbf{H})\\, \\mathbf{y}} {\\mathbf{y}^\\mathrm{T}\\, (\\mathbf{I} - \\mathbf{M})\\, \\mathbf{y}}\n",
    "> $$\n",
    "> where $\\mathbf{I}$ is the identity matrix,\n",
    "> $$\n",
    "> \\mathbf{M} = \\mathbf{1}\\, (\\mathbf{1}^\\mathrm{T} \\mathbf{1})^{-1}\\, \\mathbf{1}^\\mathrm{T},\n",
    "> $$\n",
    "> and $\\mathbf{1}$ is a column vector of ones.\n",
    "> \n",
    "> Calculate the $R^2$ value using the above matrix form of the equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccd90e6-cba3-4901-9d37-841eb66fac2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column vector of ones\n",
    "One = np.expand_dims(np.ones(y.size), axis=-1)\n",
    "\n",
    "# Calculate the matrix M\n",
    "\n",
    "\n",
    "\n",
    "# Calculate R2\n",
    "\n",
    "I = np.identity(H.shape[0])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
