{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel regressions\n",
    "\n",
    "Kernel regression is a technique for non-parametric estimation of regression models, this means that the method does not make assumptions about the distribution of the data. Unlike linear regression, which estimates a single constant coefficient for each predictor variable, kernel regression estimates a smooth function of the predictor variables. It does it through the kernel, which is a function that determines how similar two points are and estimates new values based on the values from nearby points. This makes it well suited for data sets where the underlying relationship between the dependent and independent variables is non-linear. This method can be applied to curve fitting, prediction, classification, and density estimation. It is also well-suited for data with discontinuities or nonlinear structure. In general, any situation where you need to estimate an unknown function from data can benefit from using kernel regression. There exist several methods, some of them are listed below.\n",
    "\n",
    "The **Watson estimator** is used for data with high-dimensional features or with highly correlated variables. The kernel $K_h$ for a bandwidth $h$ is used in the expression\n",
    "$$\n",
    "\\hat{m}_h(x) = \\frac{\\sum_{i=1}^n K_h (x - x_i)\\, y_i}{\\sum_{i=1}^n K_h (x - x_i)}.\n",
    "$$\n",
    "\n",
    "The **Priestley-Chao** kernel, on the other hand, is known for being translation invariant. This means that the estimates produced by the regression will be unchanged if the data points are shifted by any constant amount. This is especially important in applications where data may be subject to measurement error or other sources of imprecision, as it ensures that these errors will not affect the results of the analysis. It is calculated with\n",
    "$$\n",
    "\\hat{m}_\\mathrm{PC}(x) = h^{-1} \\sum_{1=2}^n (x_i - x_{i-1}) K \\left( \\frac{x - x_i}{h} \\right)\\, y_i,\n",
    "$$\n",
    "where $h$ is the bandwidth or smoothing parameter.\n",
    "\n",
    "Finally, the **Gasser-MÃ¼ller** kernel is considered to be more efficient than other types of kernel functions. It has been shown to be more accurate when predicting outcomes for data sets with a high degree of variance. Additionally, the Gasser kernel can handle non-linear relationships between input data points better than other kernel functions. This makes it a popular choice for use in machine learning applications.\n",
    "$$\n",
    "\\hat{m}_\\mathrm{GM}(x) = h^{-1} \\sum_{1=1}^n \\left[ \\int_{s_{i-1}}^{s_i} K \\left( \\frac{x - u}{h} \\right)\\, du \\right]\\, y_i,\n",
    "$$\n",
    "where $s_i = (x_{i-1} + x_i)/2$.\n",
    "\n",
    "## Overview\n",
    "\n",
    "We will explore representative examples of the gaussian kernel for regression applications using the 1D and its multi-dimensional generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy   as np\n",
    "import pandas  as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats             import norm\n",
    "from scipy.stats             import multivariate_normal\n",
    "\n",
    "from sklearn.kernel_ridge    import KernelRidge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics         import mean_squared_error\n",
    "\n",
    "blue   = '#0021A5'\n",
    "orange = '#FA4616'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Kernels in linear regressions\n",
    "\n",
    "Before we get started, let's do a brief reminder of what we know so far about a linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to help us plot\n",
    "def show_plot(x, y, x2=None, y2=None):\n",
    "\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "\n",
    "    plt.scatter(x, y, color=blue)\n",
    "\n",
    "    if (x2 is not None) and (y2 is not None):\n",
    "        plt.plot(x2, y2, c=orange, lw=4)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Generate a random quasi-linear dataset\n",
    "x = np.linspace(2.0, 30.0, num=100)\n",
    "\n",
    "y = 2.0*(x + 15.0) + 3.0*np.random.randn(x.size) + 2.0\n",
    "\n",
    "show_plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already know that a solution can be obtained for $\\mathbf{X} \\in \\mathbb{R}^{n \\times 2}$, and $\\mathbf{y} \\in \\mathbb{R}^n$ by means of the normal equation, in the form\n",
    "$$\n",
    "\\mathbf{B} = (\\mathbf{X}^\\mathrm{T}\\, \\mathbf{X})^{-1}\\, \\mathbf{X}^\\mathrm{T}\\, \\mathbf{Y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also know how to solve this problem analytically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve data using the primal expression\n",
    "def primal_expression(x, y, plot=True, return_B=False):\n",
    "\n",
    "    X = np.stack( [np.ones(x.size), x], axis=1 ).reshape(-1, 2)\n",
    "    Y = y[:,np.newaxis]\n",
    "\n",
    "    X_T_times_X_inv             = np.linalg.inv( np.matmul(X.T, X) )\n",
    "    X_T_times_X_inv_product_X_T = np.matmul(X_T_times_X_inv, X.T)\n",
    "\n",
    "    B = np.matmul(X_T_times_X_inv_product_X_T, Y)\n",
    "    H = np.matmul(X, X_T_times_X_inv_product_X_T)\n",
    "\n",
    "    y_predicted = np.matmul(H, Y).reshape(-1)\n",
    "\n",
    "    if plot:\n",
    "        show_plot(x, y, x2=x, y2=y_predicted)\n",
    "\n",
    "    if return_B:\n",
    "        return B\n",
    "\n",
    "primal_expression(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, however, that we assumed the existence of $(\\mathbf{X}^\\mathrm{T}\\, \\mathbf{X})^{-1}$, although sometimes we can run into numerical trouble computing the inverse matrix. We can avoid it by adding a small value, $\\lambda\\, \\mathbf{I},$ to the matrix product before calculating the inverse. This is known as the **Ridge regression**,\n",
    "$$\n",
    "\\mathbf{B} = (\\mathbf{X}^\\mathrm{T}\\, \\mathbf{X} + \\lambda\\, \\mathbf{I})^{-1}\\, \\mathbf{X}^\\mathrm{T}\\, \\mathbf{Y},\n",
    "$$\n",
    "where $\\mathbf{I}$ is the identity matrix. If we look close at the dimensions for $\\mathbf{X}^\\mathrm{T}$ we get\n",
    "$$\n",
    "\\mathbf{B} = (\\underbrace{\\mathbf{X}^\\mathrm{T}\\, \\mathbf{X}}_{\\mathbb{R}^{2 \\times n}\\, \\mathbb{R}^{n \\times 2} = \\mathbb{R}^{2 \\times 2}} + \\lambda\\, \\mathbf{I})^{-1}\\, \\mathbf{X}^\\mathrm{T}\\, \\mathbf{Y}\n",
    "$$\n",
    "This expression is known as the *primal* form. If instead of $\\mathbf{X}^\\mathrm{T}\\, \\mathbf{X}$ we use $\\mathbf{X}\\, \\mathbf{X}^\\mathrm{T}$ and rearrange the expression we have\n",
    "$$\n",
    "\\mathbf{B} = \\mathbf{X}^\\mathrm{T}\\, (\\underbrace{\\mathbf{X}\\, \\mathbf{X}^\\mathrm{T}}_{\\mathbb{R}^{n \\times 2}\\, \\mathbb{R}^{2 \\times n} = \\mathbb{R}^{n \\times n}} + \\lambda\\, \\mathbf{I})^{-1}\\, \\mathbf{Y}\n",
    "$$\n",
    "We arrive at the *dual* form of the equation. Notice that now the product $\\mathbf{X}\\, \\mathbf{X}^\\mathrm{T}$ results in a square matrix with dimension $n \\times n$. Computing this matrix is more expensive compared to the product $\\mathbf{X}^\\mathrm{T}\\, \\mathbf{X}$ with dimension $2 \\times 2$, but the *dual* form is more useful for classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dual_expression(x,y, plot=True, regularization=1e-10, return_B=False):\n",
    "\n",
    "    X = np.stack( [np.ones(x.size), x], axis=1 ).reshape(-1, 2)\n",
    "    Y = y[:,np.newaxis]\n",
    "    \n",
    "    X_times_X_T_inv             = np.linalg.inv( np.matmul(X, X.T) + regularization*np.identity(x.size))\n",
    "    X_T_product_X_times_X_T_inv = np.matmul(X.T, X_times_X_T_inv)\n",
    "\n",
    "    B = np.matmul(X_T_product_X_times_X_T_inv, Y)\n",
    "    H = np.matmul(X, X_T_product_X_times_X_T_inv)\n",
    "\n",
    "    y_predicted = np.matmul(H, Y).reshape(-1)\n",
    "\n",
    "    if plot:\n",
    "        show_plot(x, y, x2=x, y2=y_predicted)\n",
    "        \n",
    "    if return_B:\n",
    "        return B\n",
    "\n",
    "dual_expression(x, y, regularization=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the regularization parameter is used to avoid singular matrices. In this case, we are using a very small value, but in practice, it is recommended to use a value between 1e-3 and 1e-6. Also notice that the dual expression depends on the magnitude of the regularization parameter $\\lambda$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exponents       = np.arange(start=-5.0, stop=0.0, step=1.0)\n",
    "regularizations = np.power(10.0, exponents)\n",
    "\n",
    "betas = np.array([dual_expression(x, y, regularization=i,\n",
    "                                  plot=False, return_B=True) for i in regularizations])\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.set_xlabel('Regularization', fontsize=18)\n",
    "ax1.set_ylabel(r'$\\beta_0$', fontsize=18)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel(r'$\\beta_1$', fontsize=18)\n",
    "\n",
    "lns1 = ax1.plot(regularizations, betas[:,0], c=orange, lw=4, label=r'$\\beta_0$')\n",
    "lns2 = ax2.plot(regularizations, betas[:,1], c=blue,   lw=4, label=r'$\\beta_1$')\n",
    "\n",
    "lines = lns1+lns2\n",
    "label = [line.get_label() for line in lines]\n",
    "\n",
    "plt.legend(lines, label, loc='upper center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the time it takes to get the regression using both expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -n 1000 primal_expression(x,y, plot=False)\n",
    "\n",
    "%timeit -n 1000 dual_expression(x,y, plot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data, however, is not often linear. If we add at least another dimension to our data, we might be able to fit a linear model to it. This is the idea behind the kernel trick. We can add a new dimension to our data, and then project it back to the original space using a kernel function. This is a very powerful idea, and it allows us to fit non-linear data using linear models. For that we need to define a transformation by means of a kernel function with the general form\n",
    "$$\n",
    "k(x_i, x_j) = \\phi(x_i)\\, \\phi(x_j).\n",
    "$$\n",
    "Now we replace the matrix product from our dual form for linear regression with the kernel functions,\n",
    "$$\n",
    "\\hat{k}(z)\\, \\mathbf{K}\\, y = y(z),\n",
    "$$\n",
    "where\n",
    "$$\n",
    "K_{i,j} = k(x_i,x_j),\n",
    "$$\n",
    "$$\n",
    "\\hat{k}(z) =\n",
    "\\begin{bmatrix}\n",
    "  k(z, x_0) \\\\\n",
    "  k(z, x_1) \\\\\n",
    "  \\vdots    \\\\\n",
    "  k(z,x_n)  \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Notice that are embedding a vector $z$ for the evaluating the regression. Let's start the implementation with the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel(xi, xj, choice='linear', width=0.5):\n",
    "        \n",
    "    if choice in ('g', 'gauss'):\n",
    "        return np.exp( -width*np.linalg.norm(xi - xj) )\n",
    "    \n",
    "    else:\n",
    "        return np.dot(xj, xi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we generate a random dataset with a non-linear relationship between x and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# let's set up some non linear data\n",
    "y = x + 4.0*np.sin(x)+ 4.0*np.random.rand(x.size)\n",
    "\n",
    "show_plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for the kernel-based regression\n",
    "def kernel_regression(x, y, z, choice='linear', regularization=0.1, width=0.1):\n",
    "    \n",
    "    X = np.stack( [np.ones(x.size), x], axis=1 ).reshape(-1, 2)\n",
    "    Z = np.stack([np.ones(z.size), z], axis=1)\n",
    "\n",
    "    K = np.array([ [kernel(X[i], X[j], choice='g', width=width)\n",
    "                    for j in range(x.size) ]\n",
    "                    for i in range(x.size) ])\n",
    "\n",
    "    K_inv = np.linalg.inv(K + regularization*np.identity(x.size))\n",
    "\n",
    "    K_inv_times_y = np.matmul(K_inv, y)\n",
    "\n",
    "    y_pred = np.zeros(z.size)\n",
    "\n",
    "    for idx, z0 in enumerate(Z):\n",
    "\n",
    "        k = [ kernel(z0, xi, choice=choice, width=width) for xi in X ]\n",
    "\n",
    "        y_pred[idx] = np.matmul(k, K_inv_times_y)\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "# Generate a set of points for prediction\n",
    "z = np.arange(0, 35, 0.2)\n",
    "\n",
    "y_pred = kernel_regression(x, y, z, choice='g', regularization=1e-2, width=1.0)\n",
    "\n",
    "show_plot(x, y, x2=z.reshape(-1), y2=y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there are two **hyperparameters** in the kernel-based regression, namely the regularization parameter, $\\lambda$ and the width for the kernel. Considering the RMSE as a metric to evaluate the performance of the model, now we need to select the best choice for $\\lambda$ and the width for the kernel. For that, we can use a grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_kernel_regression(x, y, split=0.8, widths=None, regularizations=None):\n",
    "    results = []\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=split, random_state=0)\n",
    "    \n",
    "    for width in widths:\n",
    "        for regularization in regularizations:\n",
    "            \n",
    "            y_pred = kernel_regression(x_train, y_train, x_test, choice='g', regularization=regularization, width=width)\n",
    "            \n",
    "            rmse   = np.sqrt( mean_squared_error(y_test, y_pred) )\n",
    "            \n",
    "            results.append({'Width': width, 'Regularization': regularization, 'RMSE': rmse})\n",
    "    \n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_pivot = df_results.pivot(index='Width', columns='Regularization', values='RMSE')\n",
    "\n",
    "    return df_pivot\n",
    "    \n",
    "# Call the function with x and y\n",
    "widths          = np.linspace(0.01, 1.0, num=20)\n",
    "regularizations = np.linspace(1e-6, 1.0, num=20)\n",
    "\n",
    "dataframe = grid_search_kernel_regression(x, y, widths=widths, regularizations=regularizations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a heatmap with the results\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "ax.set_title('Grid Search Results', size=20)\n",
    "\n",
    "ax.set_xlabel('Regularization', size=18)\n",
    "ax.set_ylabel('Gaussian width', size=18)\n",
    "\n",
    "sns.heatmap(dataframe, ax=ax, annot=True, fmt='.2f',\n",
    "            cmap='jet', cbar=True,\n",
    "            cbar_kws={'pad':0.01, 'shrink':0.8},\n",
    "\n",
    "            xticklabels=[f'{i:.2f}' for i in regularizations],\n",
    "            yticklabels=[f'{i:.2f}' for i in widths])\n",
    "\n",
    "cbar = ax.collections[0].colorbar\n",
    "\n",
    "cbar.ax.set_title(f\"RMSE\", x=0.6, y=1.02, size=18, rotation=90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After inspecting a heatmap with size $20 \\times 20$, we can see that the best results are obtained with a width of 0.48 and a regularization of 0.32. With this information, we can now plot the final results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.arange(0, 35, 0.2)\n",
    "\n",
    "y_pred = kernel_regression(x, y, z, choice='g', regularization=0.32, width=0.48)\n",
    "\n",
    "show_plot(x, y, x2=z.reshape(-1), y2=y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how kernels can help us do machine learning on non-linear data, let's have a closer look at the kernel itself. In the previous example, we used a Gaussian kernel, so we will start by looking at it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gaussian kernel regression\n",
    "\n",
    "In Gaussian kernel regression the shape of the kernel is the Gaussian curve:\n",
    "$$\n",
    "\\frac{1}{\\sqrt{2\\,\\pi}} \\exp \\left[ - \\frac{z^2}{2} \\right].\n",
    "$$\n",
    "\n",
    "Each constructed kernel describes a normal distribution with mean value ${\\bf x}_i$ and standard deviation $b$, where $b$ is a hyperparameter that controls the width of the Gaussian:\n",
    "$$\n",
    "k(x, x_i) = \\frac{1}{\\sqrt{2\\,\\pi}} \\exp \\left[ - \\frac{(x-x_i)^2}{2\\,b^2} \\right].\n",
    "$$\n",
    "\n",
    "Note that the normalization of the Gaussian does not matter as the weights are being normalized themselves.\n",
    "\n",
    "The weights for a given new input $\\tilde x$ are calculated from the normalized kernel values:\n",
    "$$\n",
    "w_i = \\frac{k(\\tilde x, x_i)}{\\sum_{l=1}^N k(x_l, x_i)}.\n",
    "$$\n",
    "\n",
    "The prediction $\\tilde y$ is obtained by multiplying the weight vector ${\\bf w} = [w_1, w_2, \\dots, w_N]$ with the label vector ${\\bf y} = [y_1, y_2, \\dots, y_N]$:\n",
    "$$\n",
    "\\tilde y = \\sum_{i=1}^N w_i\\, y_i.\n",
    "$$\n",
    "\n",
    "### 2.1 One-dimensional regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 1D dataset\n",
    "X = np.array([10,20,30,40,50,60,70,80,90,100,110,120])\n",
    "Y = np.array([2337,2750,2301,2500,1700,2100,1100,1750,1000,1642,2000,1932])\n",
    "\n",
    "# Plot the dataset\n",
    "fig,ax=plt.subplots( figsize=(8,8) )\n",
    "\n",
    "plt.rc('xtick', labelsize=18) \n",
    "plt.rc('ytick', labelsize=18)\n",
    "\n",
    "ax.set_xlabel('x', fontsize=18)\n",
    "ax.set_ylabel('y', fontsize=18)\n",
    "\n",
    "ax.set_title('Data',fontsize=20)\n",
    "\n",
    "ax.scatter(X, Y, color=blue, label='Training')\n",
    "\n",
    "plt.legend(fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of simplicity, we will create an object for the implementation of the linear regression model. The object will have the following methods:\n",
    "    \n",
    "- **kernel**          : This method will compute the Gaussian kernel for a given set of points.\n",
    "- **predict**         : This method will compute the prediction for a given set of points.\n",
    "- **plot_kerneles**   : This method will plot the Gaussian kernels.\n",
    "- **plot_predictions**: This method will plot the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a class for Gaussian Kernel Regression\n",
    "class GaussianKernelRegression1D:\n",
    "    #\n",
    "    # Initialization\n",
    "    #\n",
    "    def __init__(self, x, y, b):\n",
    "\n",
    "        self.x = np.asarray(x)\n",
    "        self.y = np.asarray(y)\n",
    "        self.b = b\n",
    "    #\n",
    "    # Implement the Gaussian Kernel\n",
    "    #\n",
    "    def kernel(self, z):\n",
    "\n",
    "        return ( 1.0/np.sqrt(2.0*np.pi) )*np.exp(-0.5*z*z)\n",
    "    #\n",
    "    # Calculate weights and return prediction\n",
    "    #\n",
    "    def predict(self, X):\n",
    "\n",
    "        kernels = [self.kernel( (xi-X)/self.b ) for xi in self.x]\n",
    "        weights = [kernel/np.sum(kernels) for kernel in kernels]\n",
    "\n",
    "        return np.dot(weights, self.y)\n",
    "    #\n",
    "    # Visualize the kernels\n",
    "    #\n",
    "    def plot_kernels(self, points=100):\n",
    "\n",
    "        plt.figure( figsize = (12,6) )\n",
    "\n",
    "        plt.title('Kernel', fontsize=20)\n",
    "\n",
    "        plt.ylabel(r'Kernel Weights $w_i$', fontsize=18)\n",
    "        plt.xlabel('x', fontsize=18)\n",
    "\n",
    "        for xi in self.x:\n",
    "            x_normal = np.linspace(xi - 3.0*self.b, xi + 3.0*self.b, num=points)\n",
    "            y_normal = norm.pdf(x_normal, xi, self.b)\n",
    "\n",
    "            plt.plot(x_normal, y_normal, label=r'Kernel at $x_i$={}'.format(xi))\n",
    "            \n",
    "        plt.legend(fontsize=14)\n",
    "    #\n",
    "    # Visualize the predictions\n",
    "    #\n",
    "    def plot_predictions(self, X, points=100):\n",
    "\n",
    "        max_y = 0\n",
    "\n",
    "        plt.figure( figsize = (12,6) )\n",
    "\n",
    "        plt.title('Prediction', fontsize=20)\n",
    "\n",
    "        plt.ylabel(r'Kernel Weights $w_i$', fontsize=18)\n",
    "        plt.xlabel('x', fontsize=18)\n",
    "        \n",
    "        for xi in self.x:\n",
    "            x_normal = np.linspace(xi - 3.0*self.b, xi + 3.0*self.b, num=points)\n",
    "            y_normal = norm.pdf(x_normal, xi, self.b)\n",
    "\n",
    "            max_y = max(max(y_normal), max_y)\n",
    "\n",
    "            plt.plot(x_normal, y_normal, label=r'Kernel at $x_i$={}'.format(xi))\n",
    "            \n",
    "        plt.plot([X,X], [0, max_y], 'k-', lw=2, dashes=[2, 2])\n",
    "        \n",
    "        plt.legend(fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the width of the Gaussian kernel\n",
    "b = 20\n",
    "\n",
    "# Create an instance of the GaussianKernelRegression class\n",
    "gaussian_kernel_regression = GaussianKernelRegression1D(X, Y, b)\n",
    "gaussian_kernel_regression.plot_kernels(points=100)\n",
    "\n",
    "# Prediction for test x\n",
    "x = 26.0\n",
    "gaussian_kernel_regression.plot_predictions(x, points=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the 1-dimensional prediction\n",
    "xlist = np.linspace(0, 120, 240)\n",
    "ylist = np.array([gaussian_kernel_regression.predict(x) for x in xlist])\n",
    "\n",
    "fig,ax = plt.subplots( figsize=(8,8) )\n",
    "\n",
    "ax.set_title('1D Gaussian Kernel',fontsize=20)\n",
    "\n",
    "ax.set_xlabel('x',fontsize=18)\n",
    "ax.set_ylabel('y',fontsize=18)\n",
    "\n",
    "ax.scatter(X, Y,      color=blue, label='Training')\n",
    "ax.plot(xlist, ylist, color=orange, lw=4, label='Prediction')\n",
    "\n",
    "plt.legend(fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 N-dimensional regression\n",
    "\n",
    "For $N$-dimenisonal inputs, we need to calculate the kernels with the Euclidean metric instead.\n",
    "$$\n",
    "k(x, x_i) = \\frac{1}{\\sqrt{2\\,\\pi}} \\exp \\left ( - \\frac{\\|{\\bf x}-{\\bf x}_i\\|^2}{2\\,b^2} \\right ).\n",
    "$$\n",
    "\n",
    "Once again, we will create an object for the implementation of the linear regression model. The object will have the following methods:\n",
    "    \n",
    "- **kernel**          : This method will compute the Gaussian kernel for a given set of points.\n",
    "- **predict**         : This method will compute the prediction for a given set of points.\n",
    "- **plot_kerneles**   : This method will plot the Gaussian kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianKernelRegressionND:\n",
    "    #\n",
    "    # Initialization\n",
    "    #\n",
    "    def __init__(self, x, y, b):\n",
    "\n",
    "        self.x = np.asarray(x)\n",
    "        self.y = np.asarray(y)\n",
    "        self.b = b\n",
    "    #\n",
    "    # Implement the Gaussian Kernel\n",
    "    #\n",
    "    def gaussian_kernel(self, z):\n",
    "\n",
    "        return (1.0/np.sqrt(2.0*np.pi))*np.exp(-0.5*z*z)\n",
    "    #\n",
    "    # Calculate weights and return prediction\n",
    "    #\n",
    "    def predict(self, X):\n",
    "\n",
    "        kernels = [self.gaussian_kernel( ( np.linalg.norm(xi-X) )/self.b ) for xi in self.x]\n",
    "        weights = [(kernel/np.sum(kernels)) for kernel in kernels]\n",
    "\n",
    "        weights = np.asarray(weights)\n",
    "\n",
    "        return np.dot(weights.T, self.y)\n",
    "    #\n",
    "    # Visualize the kernels\n",
    "    #\n",
    "    def plot_kernels(self, points=100):\n",
    "        zsum = np.zeros( (points,points) )\n",
    "\n",
    "        plt.figure(figsize = (14,8))\n",
    "\n",
    "        ax = plt.axes(projection = '3d')\n",
    "\n",
    "        plt.rc('xtick', labelsize=14) \n",
    "        plt.rc('ytick', labelsize=14)\n",
    "\n",
    "        ax.set_ylabel('y', fontsize=14)\n",
    "        ax.set_xlabel('x', fontsize=14)\n",
    "        ax.set_zlabel(r'Kernel Weights $w_i$', labelpad=10, fontsize=14)\n",
    "\n",
    "        for xi in self.x:\n",
    "            x, y  = np.mgrid[0:points:complex(0.0, points), 0:points:complex(0.0, points)]\n",
    "            xy    = np.column_stack([x.flat, y.flat])\n",
    "            z     = multivariate_normal.pdf(xy, mean=xi, cov=self.b)\n",
    "            z     = z.reshape(x.shape)\n",
    "            zsum += z\n",
    "\n",
    "        ax.plot_surface(x, y, zsum, cmap=plt.cm.inferno)\n",
    "\n",
    "        m = plt.cm.ScalarMappable(cmap=plt.cm.inferno)\n",
    "        m.set_array(zsum)\n",
    "        \n",
    "        plt.colorbar(m, ax=ax, shrink=0.5)\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a reference funcion to use for machine learning\n",
    "def reference_function_3d(x=None, y=None, mesh=False):\n",
    "    \n",
    "    if mesh: x, y = np.meshgrid(x, y)\n",
    "\n",
    "    z = ( x * np.exp(-x**2 - y**2) )\n",
    "\n",
    "    if mesh: return x, y, z\n",
    "    \n",
    "    else: return z\n",
    "\n",
    "# Generate a grid of points for a 3D function\n",
    "points  = np.linspace(-2, 2, 51)\n",
    "\n",
    "x, y, z = reference_function_3d(x=points, y=points, mesh=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a seed for reproducibility\n",
    "np.random.seed(23971)\n",
    "number_of_points = 100\n",
    "\n",
    "b = 0.25\n",
    "\n",
    "x_random = np.random.uniform(-2, 2, number_of_points)\n",
    "y_random = np.random.uniform(-2, 2, number_of_points)\n",
    "\n",
    "# Plot function using a dense regular mesh\n",
    "fig, (ax1, ax2, ax3) = plt.subplots( nrows=3, figsize=(10, 24) )\n",
    "\n",
    "plt.subplots_adjust(hspace=0.2)\n",
    "\n",
    "plt.rc('xtick', labelsize=18) \n",
    "plt.rc('ytick', labelsize=18)\n",
    "\n",
    "ax1.set_title('Plot for dense mesh of points', fontsize=20)\n",
    "ax2.set_title(f'Triangulation plot for '\n",
    "              f'{number_of_points} random points', fontsize=20)\n",
    "ax3.set_title('Gaussian Kernel Regression on '\n",
    "              f'{number_of_points} random points', fontsize=20)\n",
    "\n",
    "ax1.set(xlim=(-2, 2), ylim=(-2, 2))\n",
    "ax2.set(xlim=(-2, 2), ylim=(-2, 2))\n",
    "ax3.set(xlim=(-2, 2), ylim=(-2, 2))\n",
    "\n",
    "# Dense mesh of points\n",
    "ax1.contour(x, y, z, levels=14, linewidths=0.5, colors='k')\n",
    "\n",
    "contour_ax1 = ax1.contourf(x, y, z, levels=14, cmap=\"RdBu_r\")\n",
    "\n",
    "fig.colorbar(contour_ax1, ax=ax1)\n",
    "\n",
    "ax1.plot(x, y, 'ko', ms=1)\n",
    "\n",
    "# Triangulation plot for random points\n",
    "X_train = np.vstack((x_random, y_random)).T\n",
    "Y_train = reference_function_3d(x_random, y_random)\n",
    "\n",
    "ax2.tricontour(x_random, y_random, Y_train, levels=14, linewidths=0.5, colors='k')\n",
    "contour_ax2 = ax2.tricontourf(x_random, y_random, Y_train, levels=14, cmap=\"RdBu_r\")\n",
    "\n",
    "fig.colorbar(contour_ax2, ax=ax2)\n",
    "\n",
    "ax2.plot(x_random, y_random, 'ko', ms=5)\n",
    "\n",
    "# Train Gaussian Kernel Regression on the random points\n",
    "gaussian_kernel_regression = GaussianKernelRegressionND(X_train, Y_train, b)\n",
    "\n",
    "x_flat = x.flatten()\n",
    "y_flat = y.flatten()\n",
    "\n",
    "z_flat = [gaussian_kernel_regression.predict( [x_val, y_val] )\n",
    "          for  x_val, y_val in zip(x_flat, y_flat)]\n",
    "\n",
    "ax3.tricontour(x_flat, y_flat, z_flat, levels=14, linewidths=0.5, colors='k')\n",
    "contour_ax3 = ax3.tricontourf(x_flat, y_flat, z_flat, levels=14, cmap=\"RdBu_r\")\n",
    "\n",
    "fig.colorbar(contour_ax3, ax=ax3)\n",
    "ax3.plot(x_random, y_random, 'ko', ms=5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Assignment\n",
    ">\n",
    "> How is the training affected by different choices of $b$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a different set of points\n",
    "X = np.array([[ 11, 15],\n",
    "              [ 22, 30],\n",
    "              [ 33, 45],\n",
    "              [ 44, 60],\n",
    "              [ 50, 52],\n",
    "              [ 67, 92],\n",
    "              [ 78,107],\n",
    "              [ 89,123],\n",
    "              [100,137]])\n",
    "\n",
    "Y = np.array([2337,2750,2301,2500,1700,1100,1000,1642,1932])\n",
    "\n",
    "# Plot this new data set\n",
    "fig, ax = plt.subplots( figsize = (8,8) )\n",
    "\n",
    "scatter = ax.scatter(X[:,0], X[:,1], c=Y, s=64, edgecolors='k', cmap=plt.cm.viridis)\n",
    "\n",
    "cbar = fig.colorbar(scatter, ax=ax)\n",
    "\n",
    "ax.set_xlabel(r'$x_0$', fontsize=18)\n",
    "ax.set_ylabel(r'$x_1$', fontsize=18)\n",
    "\n",
    "cbar.ax.set_title('y', fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the width of the Gaussian kernel\n",
    "b = 10\n",
    "\n",
    "# Create an instance of the object\n",
    "gaussian_kernel_regression = GaussianKernelRegressionND(X, Y, b)\n",
    "\n",
    "plt.rc('xtick', labelsize=12)\n",
    "plt.rc('ytick', labelsize=12)\n",
    "\n",
    "gaussian_kernel_regression.plot_kernels(points=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Assignment\n",
    ">\n",
    "> How are the weights modified for different values of the bandwidth parameter $b$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ridge kernel regression\n",
    "\n",
    "This method uses the kernel to transform our dataset to the kernel space and then performs a linear regression in kernel-space. Therefore, one should always **choose the appropriate kernel for a problem**. Here, we will be using using a polynomial kernel for two vectors (two points in our one-dimensional example) ${\\bf x}_1$ and ${\\bf x}_2$ that is given by\n",
    "$$\n",
    "K({\\bf x}_1, {\\bf x}_2) = \\left[ \\gamma ({\\bf x}_1^\\mathrm{T}\\, {\\bf x}_2) + c \\right]^d,\n",
    "$$\n",
    "where $\\gamma$ is the kernel coefficient, $c$ is the independent term and $d$ is the degree of the polynomial. In this case, $\\gamma$ and $c$ play a minor role, and their default value of 1.0 is adequate, so we will only focus on optimizing the polynomial degree $d$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a seed for reproducibility\n",
    "np.random.seed(seed=5)\n",
    "\n",
    "data_points = 300\n",
    "test_points = 1001\n",
    "\n",
    "# Generate a data set for machine learning\n",
    "x = np.linspace(-2, 2, num=data_points) + \\\n",
    "    np.random.normal(0.0, 0.3, data_points)\n",
    "y = np.cos(x) - 2*np.sin(x) + 3*np.cos(x*2) + \\\n",
    "    np.random.normal(0.0, 1.0, data_points)\n",
    "\n",
    "# Create list with points within the range of the dataset\n",
    "x_pred = np.linspace(np.amin(x), np.amax(x), num=test_points, endpoint=True)\n",
    "x_pred = np.array(x_pred).reshape(-1, 1)\n",
    "\n",
    "# Split the dataset into 80% for training and 20% for testing\n",
    "x = x.reshape( (x.size,1) )\n",
    "x_train,x_test,y_train,y_test = train_test_split(x, y, train_size=0.8, shuffle=True)\n",
    "\n",
    "# Plot the training and testing dataset\n",
    "fig,ax=plt.subplots( figsize=(8,8) )\n",
    "\n",
    "ax.set_title('Training and testing data',fontsize=20)\n",
    "\n",
    "ax.set_xlabel('X values',fontsize=28)\n",
    "ax.set_ylabel('cos(x)+2sin(x)+3cos(2x)',fontsize=18)\n",
    "\n",
    "ax.scatter(x_train, y_train, color=blue, marker=\"o\", label='Training')\n",
    "ax.scatter(x_test,  y_test,  color=orange, marker=\"o\", label='Testing')\n",
    "\n",
    "plt.legend(loc='best', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to plot and compare results\n",
    "def plot_comparison(degrees=None, regularization=None, reg_degree=4, rows=2, cols=2):\n",
    "\n",
    "    # Create lists to store results\n",
    "    y_pred = []\n",
    "\n",
    "    training_rmse = []\n",
    "    testing_rmse  = []\n",
    "\n",
    "    training_predictions = []\n",
    "    testing_predictions  = []\n",
    "\n",
    "    # Shape of the plot\n",
    "    #rows, cols = 2, 2\n",
    "\n",
    "    loop_vector = degrees if degrees is not None else regularization\n",
    "\n",
    "    for choice in loop_vector:\n",
    "        \n",
    "        if degrees is not None:\n",
    "            kernel_ridge_regression = KernelRidge(alpha=1.0,\n",
    "                kernel='polynomial',\n",
    "                degree=choice)\n",
    "            \n",
    "        if regularization is not None:\n",
    "            kernel_ridge_regression = KernelRidge(alpha=choice,\n",
    "                kernel='polynomial',\n",
    "                degree=reg_degree)\n",
    "            \n",
    "        kernel_ridge_regression.fit(x_train, y_train)\n",
    "\n",
    "        y_pred.append( kernel_ridge_regression.predict(x_pred) )\n",
    "\n",
    "        pred_y_train = kernel_ridge_regression.predict(x_train)\n",
    "        pred_y_test  = kernel_ridge_regression.predict(x_test)\n",
    "        \n",
    "        # Calculate training and testing errors\n",
    "        training_predictions.append(pred_y_train)\n",
    "\n",
    "        mean_error = np.sqrt(mean_squared_error(y_train, pred_y_train))\n",
    "        training_rmse.append(mean_error)\n",
    "\n",
    "        testing_predictions.append(pred_y_test)\n",
    "        mean_error = np.sqrt(mean_squared_error(y_test, pred_y_test))\n",
    "        testing_rmse.append(mean_error)\n",
    "\n",
    "    # Plot the results for each polynomial degree\n",
    "    fig, axs = plt.subplots( rows, cols, figsize=(12,12) )\n",
    "\n",
    "    for ax in axs.flat:\n",
    "        ax.set_xlabel('x', fontsize=18)\n",
    "        ax.set_ylabel('y', fontsize=18)\n",
    "\n",
    "        ax.label_outer()\n",
    "\n",
    "        ax.scatter(x_train, y_train, color=blue, marker='o', label='Training')\n",
    "        ax.scatter(x_test,  y_test,  color=orange, marker='o', label='Testing')\n",
    "\n",
    "    idx = 0\n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            \n",
    "            if degrees is not None:\n",
    "                title_string = r'$d$ = {:2d}'.format( degrees[idx] )\n",
    "                \n",
    "            if regularization is not None:\n",
    "                title_string = r'$\\alpha$ = {:.0e}'.format( regularization[idx] )\n",
    "\n",
    "            axs[row,col].set_title(title_string, fontsize=20)\n",
    "\n",
    "            axs[row,col].plot(x_pred, y_pred[idx], color='black', lw=4)\n",
    "\n",
    "            axs[row,col].annotate(f'Training RMSE = {training_rmse[idx]:.2f}',\n",
    "                    xy=(0.2,0.2), xycoords='axes fraction', fontsize=14)\n",
    "            \n",
    "            axs[row,col].annotate(f'Testing RMSE = {testing_rmse[idx]:.2f}',\n",
    "                    xy=(0.2,0.1), xycoords='axes fraction', fontsize=14)\n",
    "\n",
    "            idx += 1\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparison( degrees=[1, 2, 3, 4] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Assignment\n",
    ">\n",
    "> What happens to the RMSE in both training and testing as the degree of the polynomial increases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Regularization parameter\n",
    "\n",
    "The regularization paremeter, $\\alpha$, should also be optimized. It controls the conditioning of the problem, and larger $\\alpha$ values result into results that are more âgeneralâ and ignore the peculiarities of the problem. Larger values of $\\alpha$ allow to ignore noise in the system, but this might result into the model being blind to actual trends of the data.\n",
    "\n",
    "If we perform our kernel ridge regression for different $\\alpha$ values, we can see its effect, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparison( regularization=[1e-4, 1e0, 1e2, 1e4] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Assignment\n",
    ">\n",
    "> How do the testing RMSEs change as the regularization parameter increases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Assignment\n",
    ">\n",
    "> Optimize the degree for the polynomial kernel and the regularization parameter for the Gaussian kernel. Show your results using a heatmap and plot your best prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
