{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deriving the Normal Equation for Linear Regression\n",
    "\n",
    "## Objective:\n",
    "We aim to derive the **normal equation** for linear regression by minimizing the **mean squared error (MSE)**. The goal is to find the parameter vector $ \\boldsymbol{\\beta} $ that minimizes the error between predicted and actual values.\n",
    "\n",
    "---\n",
    "\n",
    "## Linear Regression Model in Matrix Form:\n",
    "The hypothesis for linear regression is:\n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\n",
    "$$\n",
    "Where:\n",
    "- $ \\mathbf{y} $: $ n \\times 1 $ vector of target values.\n",
    "- $ \\mathbf{X} $: $ n \\times (p + 1) $ matrix of features (including a column of ones for the intercept).\n",
    "- $ \\boldsymbol{\\beta} $: $ (p + 1) \\times 1 $ vector of parameters (coefficients to be estimated).\n",
    "- $ \\boldsymbol{\\epsilon} $: $ n \\times 1 $ vector of residuals (errors).\n",
    "\n",
    "---\n",
    "\n",
    "## Mean Squared Error:\n",
    "The mean squared error (MSE) is given by:\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "In matrix form:\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\|\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta}\\|^2\n",
    "$$\n",
    "\n",
    "The term $\\|\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta}\\|^2$ is the squared norm of the residual vector:\n",
    "$$\n",
    "\\|\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta}\\|^2 = (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta})^\\top (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta})\n",
    "$$\n",
    "\n",
    "Thus:\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta})^\\top (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta})\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Objective Function:\n",
    "To minimize MSE, we ignore the constant $ \\frac{1}{n} $ (it does not affect optimization) and define the objective function:\n",
    "$$\n",
    "J(\\boldsymbol{\\beta}) = (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta})^\\top (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta})\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Expanding the Objective Function:\n",
    "Expand $ J(\\boldsymbol{\\beta}) $:\n",
    "$$\n",
    "J(\\boldsymbol{\\beta}) = \\mathbf{y}^\\top \\mathbf{y} - 2\\mathbf{y}^\\top \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\beta}^\\top \\mathbf{X}^\\top \\mathbf{X} \\boldsymbol{\\beta}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\mathbf{y}^\\top \\mathbf{y} $: A constant term (independent of $ \\boldsymbol{\\beta} $).\n",
    "- $ -2\\mathbf{y}^\\top \\mathbf{X} \\boldsymbol{\\beta} $: Linear term in $ \\boldsymbol{\\beta} $.\n",
    "- $ \\boldsymbol{\\beta}^\\top \\mathbf{X}^\\top \\mathbf{X} \\boldsymbol{\\beta} $: Quadratic term in $ \\boldsymbol{\\beta} $.\n",
    "\n",
    "---\n",
    "\n",
    "## Minimizing the Objective Function:\n",
    "To find the optimal $ \\boldsymbol{\\beta} $, take the derivative of $ J(\\boldsymbol{\\beta}) $ with respect to $ \\boldsymbol{\\beta} $ and set it to zero:\n",
    "$$\n",
    "\\frac{\\partial J(\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta}} = -2 \\mathbf{X}^\\top \\mathbf{y} + 2 \\mathbf{X}^\\top \\mathbf{X} \\boldsymbol{\\beta}\n",
    "$$\n",
    "\n",
    "Simplify:\n",
    "$$\n",
    "-2 \\mathbf{X}^\\top \\mathbf{y} + 2 \\mathbf{X}^\\top \\mathbf{X} \\boldsymbol{\\beta} = 0\n",
    "$$\n",
    "\n",
    "Divide through by 2:\n",
    "$$\n",
    "\\mathbf{X}^\\top \\mathbf{X} \\boldsymbol{\\beta} = \\mathbf{X}^\\top \\mathbf{y}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## The Normal Equation:\n",
    "Solve for $ \\boldsymbol{\\beta} $:\n",
    "$$\n",
    "\\boldsymbol{\\beta} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Key Points:\n",
    "1. **Interpretation**:\n",
    "   - $ (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top $: Known as the **pseudoinverse** of $ \\mathbf{X} $.\n",
    "   - $ \\boldsymbol{\\beta} $: The vector of coefficients that minimizes the sum of squared errors.\n",
    "\n",
    "2. **Assumptions**:\n",
    "   - $ \\mathbf{X}^\\top \\mathbf{X} $ must be invertible (no multicollinearity).\n",
    "   - The solution assumes a linear relationship between $ \\mathbf{X} $ and $ \\mathbf{y} $.\n",
    "\n",
    "3. **Application**:\n",
    "   The normal equation provides a closed-form solution for linear regression, though iterative methods like gradient descent are often used for large datasets.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
