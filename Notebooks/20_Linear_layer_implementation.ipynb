{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "511e86ab-f7c3-4439-a88f-1fb9c425d818",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "<img src=\"https://jackhaviland.com/posts/connecting-tensors-with-convolutions/img/fully-connected-nn.svg\" alt=\"Illustration of different architectures using neural networks\" align=\"right\" style=\"width: 500px; float: right;\"/>\n",
    "\n",
    "In this notebook we will implement our own neural network step by step. To achieve this goal, we will need to write the code for a **fully connected layer**, and our choices for the **activation function** and **loss function**. Each with the partial derivatives needed for the back propagation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7f5a0f-31bd-432b-b659-397aa482c44e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Libraries\n",
    "\n",
    "We will use the [NumPy](https://numpy.org/) library for the implementation of our neural network. But notice that we can also use [PyTorch](https://pytorch.org/) if we wish to take advantage of the AutoGrad library included therein to compute the derivatives for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08bc814-eb5f-4fd9-aa86-21bd21b978a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec69abc5-bdeb-4709-a23e-70c581fa2b3f",
   "metadata": {},
   "source": [
    "# 2. Fully Connected Layer\n",
    "\n",
    "Recall that a fully connected layer is an architecture that connects every input neuron to every output neuron. During this operation, the input ${\\bf X}$ is multiplied by a weight matrix ${\\bf W}$ and then a bias vector ${\\bf B}$ is finally added.\n",
    "\n",
    "For a fully connected layer $y_i, \\forall\\, i \\in \\{1,\\, 2,\\, 3,\\,\\cdots,\\,n\\}$ with the form\n",
    "$$\n",
    "y_1 = \\sum_i x_iw_{i,1} + b_1\\\\\n",
    "y_2 = \\sum_i x_iw_{i,2} + b_2\\\\\n",
    "y_3 = \\sum_i x_iw_{i,3} + b_3\\\\\n",
    "\\vdots \\\\\n",
    "y_n = \\sum_i x_iw_{i,n} + b_n\\\\\n",
    "$$\n",
    "\n",
    "the forward pass can be expressed easily by defining the following notation\n",
    "\n",
    "$$\n",
    "{\\bf X} = \\begin{bmatrix} x_1 & \\cdots & x_i \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "{\\bf B} = \\begin{bmatrix} b_1 & \\cdots & b_n\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$ {\\bf W} =\n",
    "\\begin{bmatrix}\n",
    "w_{11} & \\cdots & w_{1n}\\\\\n",
    "\\vdots & \\ddots & \\vdots\\\\\n",
    "w_{i1} & \\cdots & w_{in}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Hence, the forward propagation becomes ${\\bf Y} = {\\bf X W} + {\\bf B}$.\n",
    "\n",
    "## 2.1 Backward propagation\n",
    "\n",
    "We start by first identifying that ${\\bf W}$ and ${\\bf B}$ are the parameters in our layer, therefore we need to compute the derivatives with respect to these parameters, as well as the input ${\\bf X}$. For such purpose, we will define ${\\bf L}$ as the loss (error) associated to the input. For a single layer $L$ is a scalar, but for the whole network, it is more appropriate to express it as a vector ${\\bf L}$. The derivatives we need are\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial L}{\\partial \\bf X} &=& \\frac{\\partial L}{\\partial \\bf Y}{\\bf W}^T \\\\\n",
    "\\frac{\\partial L}{\\partial \\bf W} &=& {\\bf X}^T \\frac{\\partial L}{\\partial \\bf Y} \\\\\n",
    "\\frac{\\partial L}{\\partial \\bf B} &=& \\frac{\\partial L}{\\partial \\bf Y} \\\\\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "With these equations at hand, let's implement the code for a fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1347d358-ca78-4882-a665-15bfde64263e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "affa42d5-db78-44b8-a94e-93331c0f9600",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Activation Function\n",
    "\n",
    "The forward propagation for an activation function consists of applying an activation function ${\\bf Y} = f({\\bf X})$ to all the elements of a given input ${\\bf X}$. Note that this operation preserves the dimensions of the matrix ${\\bf X}$.\n",
    "\n",
    "## 3.1 Backward propagation\n",
    "\n",
    "The contribution to the loss value $L$ is an element-wise multiplication between $\\partial L/\\partial {\\bf X}$ and the derivative of the activation function, $f'({\\bf X})$. We can express this operation through the following expression\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\bf X} = \\frac{\\partial L}{\\partial \\bf Y} \\odot f'({\\bf X})\n",
    "$$\n",
    "\n",
    "Let's now implement the code for the forward and backward propagation for an activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b251dde0-cc20-4eab-a8f2-b78166fc64db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fc6ad7d-1d15-41dd-881e-a1a409b187db",
   "metadata": {},
   "source": [
    "# 4. Loss Function\n",
    "The loss function measures the error of the predictions with respect to the reference values. We have seen already different loss functions that commonly are used in artificial intelligence. Here, for the sake of simplicity, we will focus on the Mean Squared Error, MSE, that is expressed as\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{n} \\sum{(\\hat{\\bf Y} - {\\bf Y})^2}\n",
    "$$\n",
    "\n",
    "where $\\hat{\\bf Y}$ and ${\\bf Y}$ are the reference and predicted values, respectively.\n",
    "\n",
    "## 4.1 Backward propagation\n",
    "\n",
    "Same as we have done previously, we will need to compute the derivative for the MSE. Since it depends on ${\\bf Y}$, we have that\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\bf Y} = \\frac{2}{n}({\\bf Y} - \\hat{\\bf Y})\n",
    "$$\n",
    "\n",
    "This is the last piece of information we need. Let's proceed to implement the code for the MSE loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff63a1d6-c2b1-4fe1-8549-9ed75790b8cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c06a239a-7d49-4d2c-b754-e2d37985e0f1",
   "metadata": {},
   "source": [
    "# 5. Build Neural Network\n",
    "\n",
    "Now that we have everything we need, we can define our neural network with as many fully connected layers as we wish. Since we used a notation analogous to that in [PyTorch](https://pytorch.org/), the implementation should look familiar to you. The only difference is that we have to define ourselves the backward propagation. For that, remember that you must **start from the last layer and continue until the first one**. Backwards. And now the term *backward propagation* might make more sense to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64cee9d-a30a-45d3-ab78-e17dab6a8530",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ef2f073-84a7-4bb5-9229-585ba4801c2d",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can run some quick test to evaluate the performance of our neural network. A simple task could be learning to map one-hot encoding features into reals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737979c5-31e8-47da-8ce1-48d09f786a85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = np.eye(8, dtype=float)\n",
    "labels = np.arange(start=1.0, stop=9.0, step=1.0, dtype=float).reshape(-1,1)\n",
    "\n",
    "network   = # define the network\n",
    "criterion = # define the loss function\n",
    "\n",
    "print(f\"inputs are\\n{inputs}\\n\\nlabels are\\n{labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c64c90c-bdbf-40e8-a60b-106c2381ff69",
   "metadata": {},
   "source": [
    "Now the customary training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8db3c7-b146-4fcb-a411-15ecb3b7afd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    \n",
    "    # define training loop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
