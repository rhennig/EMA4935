{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b14b62c6-9c90-4449-bd43-975ec14666cb",
   "metadata": {},
   "source": [
    "# Deploy tasks on SLURM schedulers\n",
    "\n",
    "High-perforamnce supercomputers, academic institutions and other small clusters typically are managed with a job scheduler like PBS, SLURM, MOAB, SGE, LSF, HTCondor, or others. Here, Dask and [Dask-Jobqueue](https://jobqueue.dask.org/en/latest/) offer capabilities to work on big data using large-scale computing facilities for submitting cluster jobs with Python scripts.\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, we will look briefly at two generic examples that ilustrate the steps to instantiate workers on SLURM-based clusters. The first example enables computations that rely solely on python instructions, whereas the second example depicts an alternative method that allows running code from a different file, normally a bash script, that can be used to call some external executable code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e3646a-66e9-4463-88e8-c4b6a00dba9c",
   "metadata": {},
   "source": [
    "# 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b8cff2-b32e-41e9-9120-dd758cc2c29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import textwrap\n",
    "import subprocess\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from dask.distributed import Client\n",
    "from dask_jobqueue    import SLURMCluster\n",
    "\n",
    "from pathlib import Path as path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948e789c-17c6-4205-ab73-cee58eaa2bc7",
   "metadata": {},
   "source": [
    "# 2. Run a python function using the SLURM scheduler\n",
    "\n",
    "The steps for this example are based on the following module `run_task`. This is the driver module each worker will be using to run its task. Since the code inside this module can become arbitrarily complex, here we will assume that it only recieves one argument named `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04fbc01-606d-4cb9-93d4-8bad67c1cf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_task(data):\n",
    "    \n",
    "    # Include here the code of your choice\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5b99e4-e699-4a2a-8105-2dacb8751aea",
   "metadata": {},
   "source": [
    "As usual, we now have to set the Cluster and Client objects, then iterate over the elements obtained, e.g, from a DataFrame. For the sake of generalization, please keep in mind that strings alike '\\<some_string\\>' are generic tags specific to the cluster you are using. Hence you will need to modify them accordingly.\n",
    "\n",
    "Note that we use `client.submit` to indicate that we want to include one task as a *future* run. Here we indicate our callable and its arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebfbfa1-d41e-492d-8a1b-c868c8aa70ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "#\n",
    "### If needed, prepare empty array to collect futures\n",
    "#\n",
    "    futures = []\n",
    "#\n",
    "### Define Cluster and Client\n",
    "#\n",
    "    cluster = SLURMCluster(cores=1,\n",
    "                           memory='2GB',\n",
    "                           account='<your_account>',\n",
    "                           queue='<the_queue>',\n",
    "                           walltime='12:00:00',\n",
    "                           job_extra_directives=['--ntasks=1', '--nodes=1', '--qos=<your_qos>']\n",
    "                          )\n",
    "    \n",
    "    cluster.scale(jobs=16)\n",
    "    \n",
    "    client = Client(cluster)\n",
    "#\n",
    "### Define the setting you need for 'run_task'\n",
    "#\n",
    "\n",
    "#\n",
    "### Define your dataframe\n",
    "#\n",
    "    pandascsv = pd.read_csv('<your_file.csv>')\n",
    "    dataframe = dask.dataframe.from_pandas(pandascsv, npartitions=1)\n",
    "#\n",
    "### Execute 'run_task' for a generic field named 'Species'\n",
    "#\n",
    "    for row in dataframe.itertuples():\n",
    "        future = client.submit(run_task, row.Species)\n",
    "        \n",
    "        futures.append(future)\n",
    "#\n",
    "### Collect your results if needed\n",
    "#\n",
    "    for future in futures: result = future.result()\n",
    "#\n",
    "### Close Cluster and Client\n",
    "#\n",
    "    client.shutdown()\n",
    "    \n",
    "    client.close()\n",
    "    \n",
    "    cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef8dc83-e65a-476c-a27e-51477f64ba7a",
   "metadata": {},
   "source": [
    "# 3. Run a script file using the SLURM Scheduler\n",
    "\n",
    "Instead of running purely python code, sometimes we might be interested in running an external code we compiled previously, or simply calling some files outside python. For such cases, we can use an analogous approach. The difference is that we call the `subprocess` function insted to execute that external command, e.g., `bash filename` and collect the commands necessary to run the external code in `filename`. Again, please notice that you will need to modify the code depending on your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28211896-d033-45be-98d5-27e8009d172c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_bash_file(filename):\n",
    "    \n",
    "    open(filename, 'w').write(textwrap.dedent('''\\\n",
    "        #!/bin/bash\n",
    "        \n",
    "        <your bash code>\n",
    "    '''))\n",
    "    \n",
    "def run_bash_file(filename):\n",
    "    \n",
    "    write_bash_file(filename)\n",
    "    \n",
    "    stdout = open(filename.parent/f'{filename.name}.out', 'w')\n",
    "    stderr = open(filename.parent/f'{filename.name}.err', 'w')\n",
    "    \n",
    "    subprocess.call(['bash', filename], stdout=stdout, stderr=stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d81318-820f-41df-9bae-4b9eb1680f49",
   "metadata": {
    "tags": []
   },
   "source": [
    "Just like in the previous example, we now need to set the Cluster and Client objects. The following code also iterates over the rows of a generic DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddb46b8-30c8-40cc-8ccb-3fcca32e9292",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "#\n",
    "### If needed, prepare empty array to collect futures\n",
    "#\n",
    "    futures = []\n",
    "#\n",
    "### Define Cluster and Client\n",
    "#\n",
    "    cluster = SLURMCluster(cores=1,\n",
    "                           memory='2GB',\n",
    "                           account='<your_account>',\n",
    "                           queue='<the_queue>',\n",
    "                           walltime='12:00:00',\n",
    "                           job_extra_directives=['--ntasks=1', '--nodes=1', '--qos=<your_qos>']\n",
    "                          )\n",
    "    \n",
    "    cluster.scale(jobs=16)\n",
    "    \n",
    "    client = Client(cluster)\n",
    "#\n",
    "### Define the setting you need for 'run_bash_file'\n",
    "#\n",
    "    \n",
    "#\n",
    "### Define your dataframe\n",
    "#\n",
    "    pandasscv = pd.read_csv('<your_file.csv>')\n",
    "    dataframe = dask.dataframe.from_pandas(pandascsv, npartitions=1)\n",
    "#\n",
    "### Execute 'run_bash_file' for a generic field named 'Species'\n",
    "#\n",
    "    for row in dataframe.itertuples():\n",
    "        future = client.submit(run_bash_file, f'{row.Species}.sh')\n",
    "        \n",
    "        futures.append(future)\n",
    "#\n",
    "### Collect your results if needed\n",
    "#\n",
    "    for future in futures: result = future.result()\n",
    "#\n",
    "### Close Cluster and Client\n",
    "#\n",
    "    client.shutdown()\n",
    "    \n",
    "    client.close()\n",
    "    \n",
    "    cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
