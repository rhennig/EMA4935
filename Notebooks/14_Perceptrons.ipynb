{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a07352ce-4146-4fe3-b4cb-02197906282d",
   "metadata": {},
   "source": [
    "# Perceptrons\n",
    "\n",
    "Based on https://towardsdatascience.com/perceptron-algorithm-in-python-f3ac89d2e537\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, we will implement the Perceptron algorithm. It is the simplest single-layer neural network algorithm and illustrates some of the fundamental aspects of artifical neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edbcb5e-2956-4c74-a3dc-64453a9f382c",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2626ed75-09d3-4734-be0d-144d519aefc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy  as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics   as metric\n",
    "\n",
    "from sklearn                 import datasets\n",
    "from sklearn.neural_network  import MLPClassifier\n",
    "from sklearn.linear_model    import Perceptron\n",
    "\n",
    "plt.rc('xtick', labelsize=14) \n",
    "plt.rc('ytick', labelsize=14)\n",
    "\n",
    "blue   = '#0021A5'\n",
    "orange = '#FA4616'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447f2164-1efa-46a4-9a35-e9b538de21a2",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Perceptrons are part of the important components in machine learning. A reason for developing a perceptron was that of classifying data into two categories. The idea behind it actually is based on real-life biological neurons. A perceptron may be considered an artificial neuron, or node, with the purpose of collecting the input features and then calculating a weighted sum of all these features. As a result of such process, it makes a binary decision based on the result. Hence, making it useful in binary classification problems.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Neuron.svg/640px-Neuron.svg.png\" alt=\"Neuron\" align=\"right\" style=\"width:500px; float:center\"/>\n",
    "\n",
    "There are several concepts that we need to keep in mind,\n",
    "\n",
    "- **Weights and Bias**: A perceptron weights every input parameter present in the data and then adds a bias unit. These parameters are optimized during training.\n",
    "\n",
    "- **Activation Function**: Determines whether a neuron should be activated or not based on certain conditions, e.g., the perceptron chooses the first class if the weighted sum of inputs and the bias is larger than or equal to zero, otherwise chooses the second class.\n",
    "\n",
    "- **Learning Rate**: Controls how quickly the neural network fits and updates the attributes learned previously.\n",
    "\n",
    "Some of the advantages offered by preceptrons are their simplicity, computational efficiency, and good performance on linearly separable data. However, there also are disadvantages, such as the sensitivity to the initial choice for the weigths and bias, or its restriction to only provide binary ouputs, 0 or 1, that might result problematic if we are interested in computing probabilities instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe30858-f99c-46f7-ba2f-aff134aa6cb2",
   "metadata": {},
   "source": [
    "## 2. Data\n",
    "\n",
    "To illustrate the Perceptron algorithm for clustering, we create a dataset with two classes, orange and blue. The goal for the Perceptron algorithm is that of learning an optimal straight line that separates both classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad500ae7-639e-4691-adb2-23ae9ad02f30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_data(x,y, x1=None, y1=None):\n",
    "    \n",
    "    fig = plt.figure( figsize=(6,6) )\n",
    "    \n",
    "    plt.title('Classification Data', fontsize=16)\n",
    "    \n",
    "    plt.scatter(x[:, 0][y == 0], x[:, 1][y == 0], c=orange, marker='^', s=64)\n",
    "    plt.scatter(x[:, 0][y == 1], x[:, 1][y == 1], c=blue,   marker='s', s=64)\n",
    "    \n",
    "    if (x1 is not None) and (y1 is not None):\n",
    "        plt.plot(x1, y1, 'k-', lw=4)\n",
    "    \n",
    "    plt.xlabel('Feature 1', fontsize=18)\n",
    "    plt.ylabel('Feature 2', fontsize=18)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "points = 150\n",
    "X, y   = datasets.make_blobs(n_samples=points, n_features=2,\n",
    "                             centers=2, cluster_std=3, random_state=1)\n",
    "\n",
    "plot_data(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7363574f-2e04-4b92-a3d5-8b98a713b215",
   "metadata": {},
   "source": [
    "## 3. Perceptron\n",
    "\n",
    "The Perceptron algorithm is illustrated by the following flowchart. For every training example, we first take the dot product of the input features and the parameters, $\\theta$. Then, we apply the Unit Step Function to make the prediction, $\\hat y$.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f0/Computer.Science.AI.Neuron.svg/640px-Computer.Science.AI.Neuron.svg.png\" alt=\"Perceptron\" align=\"center\" style=\"width:500px; float:center\"/>\n",
    "\n",
    "- If the prediction is wrong and the model has misclassified that data point, we update for the parameters, $\\theta$.\n",
    "- We donâ€™t update when the prediction is correct.\n",
    "\n",
    "The Perceptron algorithm sums the input features using weights and applies a Unit Step Function, or Heaviside function, to that sum:\n",
    "$$\n",
    "\\sigma(z) = \\begin{cases}\n",
    "0 \\,\\, \\text{if} \\,\\, z < 0 \\\\\n",
    "1 \\,\\, \\text{if} \\,\\, z \\ge 0\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f509290a-43d1-499a-98c5-b160306ba629",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "z     = np.linspace(-1, 1, 100)\n",
    "sigma = np.heaviside(z, 1)\n",
    "\n",
    "plt.plot(z, sigma, c=blue, lw=4)\n",
    "\n",
    "plt.xlabel('z', fontsize=14)\n",
    "plt.ylabel('Step function', fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746c8f78-a969-4310-a419-1a162519fc0b",
   "metadata": {},
   "source": [
    "### 3.1 Update rule\n",
    "\n",
    "The Perceptron update rule is similar to the Gradient Descent update rule:\n",
    "$$\n",
    "\\theta_{n+1} = \\theta_n + \\eta \\left ( {\\bf y} -\\sigma(\\theta \\cdot {\\bf X}) \\right ) {\\bf X}\\, ,\n",
    "$$\n",
    "where $\\eta$ is the learning rate that directly control the weights and represents how quickly the perceptron understands and updates its learning.\n",
    "\n",
    "Let's implement our own perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c42a80-f05f-41df-a6e2-8fd7b2b9c598",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SingleLayerPerceptron:\n",
    "#\n",
    "### Initialize\n",
    "#\n",
    "    def __init__(self, X, y):\n",
    "        \n",
    "        samples, features = X.shape\n",
    "        \n",
    "        self.X = np.asarray(X)\n",
    "        self.y = np.asarray(y)\n",
    "                \n",
    "        self.parameters = np.zeros( (features+1,1) )\n",
    "        self.X_interval = [ np.min(X[:,0]), np.max(X[:,0]) ]\n",
    "#\n",
    "### Training loop\n",
    "#\n",
    "    def fit(self, learning_rate=1e-3, epochs=100):\n",
    "               \n",
    "        for epoch in range(epochs):            \n",
    "            for idx, X_i in enumerate(self.X):\n",
    "                \n",
    "                # Insert 1 for bias, X_0 = 1\n",
    "                X_i = np.insert(X_i, 0, 1).reshape(-1,1)\n",
    "                \n",
    "                # Calculate prediction\n",
    "                z = np.dot(X_i.T, self.parameters)\n",
    "                \n",
    "                y = np.heaviside(z, 1)\n",
    "                \n",
    "                # Update if the sample is missclassified\n",
    "                if not y.squeeze() - self.y[idx] == 0.0:\n",
    "                    self.parameters += learning_rate*(self.y[idx] - y)*X_i    \n",
    "#\n",
    "### Boundary\n",
    "#\n",
    "    def plot(self, c1=orange, c2=blue):\n",
    "        \n",
    "        # The equation for a line is y = mx + b\n",
    "        slope = -self.parameters[1]/self.parameters[2]\n",
    "        bias  = -self.parameters[0]/self.parameters[2]\n",
    "        \n",
    "        y = slope*self.X_interval + bias\n",
    "        \n",
    "        fig = plt.figure( figsize=(6,6) )\n",
    "    \n",
    "        plt.title('Perceptron Algorithm', fontsize=18)\n",
    "\n",
    "        plt.scatter(self.X[:,0][self.y == 0], self.X[:,1][self.y == 0], c=orange, marker='^', s=64)\n",
    "        plt.scatter(self.X[:,0][self.y == 1], self.X[:,1][self.y == 1], c=blue,   marker='s', s=64)\n",
    "\n",
    "        plt.plot(self.X_interval, y, 'k-', lw=4)\n",
    "\n",
    "        plt.xlabel('Feature 1', fontsize=14)\n",
    "        plt.ylabel('Feature 2', fontsize=14)\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6851543a-2b92-4091-8aa1-93599e321f50",
   "metadata": {},
   "source": [
    "We can use our implementation on the data we generated previously for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f462ecf-5db7-41e4-a37b-8a5a3200551f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "perceptron = SingleLayerPerceptron(X, y)\n",
    "\n",
    "perceptron.fit(learning_rate=0.01, epochs=20)\n",
    "\n",
    "perceptron.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c303ea4d-4047-4720-9301-131afd521778",
   "metadata": {},
   "source": [
    "We observe that we are able to categorize both data classes.\n",
    "\n",
    "> ### Assignment\n",
    ">\n",
    "> Try different choices for the random seed and the standard deviation for generating the data to explore how the Perceptron works for other distributions. Comment your findings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba71458-8bdb-4230-9699-3b2a986aa45b",
   "metadata": {},
   "source": [
    "## 4. Simple perceptron with scikit-learn\n",
    "\n",
    "Let's create a simple dataset for a square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728c3b8d-0e4c-4d4f-a84a-6d2bb95824fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([0, 0, 0, 1])\n",
    "\n",
    "plot_data(X,y)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9009c47-a747-4da5-85d2-79bbb84f8548",
   "metadata": {},
   "source": [
    "Instead of our implementation, we will use the `Perceptron` object provided by the scikit-learn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3e841d-432d-4484-8e22-b30f697103d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "classifier = Perceptron()\n",
    "\n",
    "classifier.fit(X,y)\n",
    "\n",
    "print(f'accuracy   : {classifier.score(X, y)}\\n'\n",
    "      f'predictions: {classifier.predict(X)}\\n'\n",
    "      f'expected   : {y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b09e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision boundary\n",
    "fig = plt.figure( figsize=(6,6) )\n",
    "\n",
    "plt.title('Perceptron Algorithm', fontsize=20)\n",
    "\n",
    "plt.scatter(X[:,0][y == 0], X[:,1][y == 0], c=orange, marker='^', s=64)\n",
    "plt.scatter(X[:,0][y == 1], X[:,1][y == 1], c=blue,   marker='s', s=64)\n",
    "\n",
    "x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "\n",
    "xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, 0.01),\n",
    "                       np.arange(x2_min, x2_max, 0.01))\n",
    "\n",
    "Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "Z = Z.reshape(xx1.shape)\n",
    "\n",
    "plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=plt.cm.coolwarm)\n",
    "\n",
    "plt.xlabel('Feature 1', fontsize=18)\n",
    "plt.ylabel('Feature 2', fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43a0679-f0e6-419e-8e75-8dc8133e77fd",
   "metadata": {},
   "source": [
    "Pay closer attention to the elements in our data set. You will realize that the dataset actually is an example of a [logic gate](https://www.techtarget.com/whatis/definition/logic-gate-AND-OR-XOR-NOT-NAND-NOR-and-XNOR) AND.\n",
    "\n",
    "> ### Assignment\n",
    ">\n",
    "> Modify the data for the other logic gates NOT, OR, NOR, XOR, NAND, and XNOR. What do you observe? Can the single neuron perceptron algorithm model these operations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23afaab-95d0-44dc-9b7f-c381f8d47616",
   "metadata": {},
   "source": [
    "## 5. Multilayer perceptron\n",
    "\n",
    "Generalizing the perceptron algorithm into one that includes multiple neurons and several layers of neurons results in a **neural network**. We can use the multilayer perceptron algorithm in scikit-learn `MLPClassifier` to train a classifier using a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ff82fe-e322-47e7-9986-6e8decd4f8ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Routine to optimize a classifier on data X,y and plot the decision boundaries of the classifier\n",
    "def plot_decision_boundaries(X, y, ax=None, mesh_step=0.1, threshold=1, model=None, **kwargs):\n",
    "    \n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "    \n",
    "    # Create the object for the multilayer perceptron classifier\n",
    "    classifier = model(**kwargs)\n",
    "    \n",
    "    # Fit the model with our data\n",
    "    classifier.fit(X, y)\n",
    "    \n",
    "    # Compute predicted values with the MLP\n",
    "    y_predicted = classifier.predict(X)\n",
    "    accuracy   = metric.accuracy_score(y, y_predicted, normalize=True)\n",
    "    \n",
    "    # Decision boundary\n",
    "    x0_min, x0_max = X[:,0].min() - threshold, X[:,0].max() + threshold\n",
    "    x1_min, x1_max = X[:,1].min() - threshold, X[:,1].max() + threshold\n",
    "    \n",
    "    # Generate grid\n",
    "    x0_mesh, x1_mesh = np.meshgrid(np.arange(x0_min, x0_max, mesh_step), np.arange(x1_min, x1_max, mesh_step))\n",
    "    \n",
    "    # Compute labels\n",
    "    labels = Z = classifier.predict(np.c_[x0_mesh.ravel(), x1_mesh.ravel()])\n",
    "    \n",
    "    labels = labels.reshape(x0_mesh.shape)\n",
    "    \n",
    "    # Plot the classification boundaries\n",
    "    \n",
    "    ax.contourf(x0_mesh, x1_mesh, labels, alpha=0.4)\n",
    "    \n",
    "    ax.scatter(X[:,0], X[:,1], c=y, s=100, ec='k', lw=2)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c671370-c144-45dd-b6ce-8b0028140f2f",
   "metadata": {},
   "source": [
    "> ### Assignment\n",
    ">\n",
    "> Generate the data for the XOR logic gate and save it to the variable `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7c7336-fb45-49d5-981a-7ae6576c75b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "\n",
    "# your code here\n",
    "y = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8ab314-193d-4a09-aae0-fbc01de38c26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots( figsize=(6,6) )\n",
    "\n",
    "accuracy = plot_decision_boundaries(X, y, ax=ax, model=MLPClassifier,\n",
    "                    solver='lbfgs', hidden_layer_sizes=(2),\n",
    "                    activation='logistic', random_state=4)\n",
    "\n",
    "ax.set_xlabel(\"Feature 1\",fontsize=14)\n",
    "ax.set_ylabel(\"Feature 2\",fontsize=14)\n",
    "\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2227a64c-a266-489b-8b34-5d72a403c231",
   "metadata": {},
   "source": [
    "Let's see quickly how our model goes from underfitting to overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9b666a-8d5a-4083-8e44-ac43f83391ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=3, ncols=4, figsize=(24,16),\n",
    "                       sharex=True, sharey=True, layout='tight')\n",
    "\n",
    "layer = 1\n",
    "\n",
    "for row in range(ax.shape[0]):\n",
    "    for col in range(ax.shape[1]):\n",
    "        _ = plot_decision_boundaries(X, y,\n",
    "                ax=ax[row,col], model=MLPClassifier,\n",
    "                solver='lbfgs', hidden_layer_sizes=(layer),\n",
    "                activation='logistic', random_state=4)\n",
    "    \n",
    "        ax[row,col].set_xlabel('')\n",
    "        ax[row,col].set_ylabel('')\n",
    "\n",
    "        ax[row,col].set_xticks([])\n",
    "        ax[row,col].set_yticks([])\n",
    "        \n",
    "        ax[row,col].set_title(f'{layer} Layer{\"\" if layer==1 else \"s\"}', fontsize=20)\n",
    "        \n",
    "        layer += 1\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dee988-e03a-4184-aef2-796347bb2954",
   "metadata": {},
   "source": [
    "> ### Assignment\n",
    ">\n",
    ">  Modify the parameters for the `MLPClassifier` and discuss the sensitivity of the classifier to the activation function `activation`, the optimization method `solver`, and the initial random guess `random_state`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2047c832",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
