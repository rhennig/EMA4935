{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Attention Network\n",
    "\n",
    "- The molecules for this notebook were taken from the [QM09 dataset](https://www.nature.com/articles/sdata201422) by R. Ramakrishnan1, P. O. Dral, M. Rupp, and O. A. von Lilienfeld *Sci. Data*, **1**, 140022 (2014).\n",
    "\n",
    "- Description of the Graph Attention Network taken from https://openreview.net/forum?id=rJXMpikCZ.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*3D844_twutCaunYMPuo-Sw.png\" alt=\"Illustration of the attention mechanism\" aling=\"right\" style=\"width: 500px;float: right;\"/>\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook we will learn how to implement a [Graph Attention Network](https://arxiv.org/abs/1710.10903), as reported by Veličković, Cucurull, Casanova, Romero, Liò, and Bengio. This algorithm uses the graph structure that represents an isolated molecule or the unit cell for a given material.\n",
    "\n",
    "To understand the architecture, consider a graph of $n$ nodes, specified as a set of node features, $(\\vec{h}_1, \\vec{h}_2, \\dots, \\vec{h}_n)$, and an adjacency matrix $\\bf A$, such that ${\\bf A}_{ij} = 1$ if $i$ and $j$ are connected, and 0 otherwise. A graph convolutional layer then computes a set of new node features, $(\\vec{h}'_1, \\vec{h}'_2, \\dots, \\vec{h}'_n)$, based on the input features as well as the graph structure. In order to achieve a higher-level representation, every graph convolutional layer starts from a shared node-wise feature transformation, specified by a weight matrix ${\\bf W}$. This transforms the feature vectors into $\\vec{g}_i = {\\bf W}\\vec{h}_i$. After this, the vectors $\\vec{g}_i$ typically are recombined in some way at each node.\n",
    "\n",
    "In general, we can define a graph convolutional operator as an aggregation of features across neighborhoods, defining $\\mathcal{N}_i$ as the neighborhood of node $i$ that mostly consists of all first-order neighbors of $i$, including $i$ itself. We can define the output features of node $i$ as\n",
    "$$\n",
    "\\vec{h}'_i = \\sigma\\left(\\sum_{j\\in\\mathcal{N}_i}\\alpha_{ij}\\vec{g}_j\\right)\\, ,\n",
    "$$\n",
    "where $\\sigma$ is an activation function, and $\\alpha_{ij}$ specifies the weighting factor (importance) of node $j$’s features to node $i$.\n",
    "\n",
    "We can instead let $\\alpha_{ij}$ be implicitly defined, employing self-attention over the node features. Notice that self-attention has previously been shown to be self-sufficient for state-of-the-art-level results on machine translation, as demonstrated by the [Transformer architecture](https://arxiv.org/abs/1706.03762).\n",
    "\n",
    "Generally, we let $\\alpha_{ij}$ be computed as a byproduct of an attention mechanism, $a \\in \\mathbb{R}^N \\times \\mathbb{R}^N \\rightarrow \\mathbb{R}$, which computes unnormalised coefficients $e_{ij}$ across pairs of nodes $i, j$, based on their features\n",
    "$$\n",
    "e_{ij} = a(\\vec{h}_i, \\vec{h}_j)\\, .\n",
    "$$\n",
    "We inject the graph structure by only allowing node $i$ to attend over nodes in its neighborhood, $j \\in \\mathcal{N}_i$. These coefficients then are normalised using the [Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html) function, in order to be comparable across different neighborhoods\n",
    "$$\n",
    "\\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{k\\in\\mathcal{N}_i}\\exp(e_{ik})}\\, .\n",
    "$$\n",
    "Overall, the framework is agnostic to the choice of attention mechanism $a$. The parameters of the mechanism are trained jointly with the rest of the network in an end-to-end fashion.\n",
    "\n",
    "To stabilise the learning process of self-attention, multi-head attention might be beneficial. Namely, the operations of the layer are independently replicated $K$ times, each replica with different parameters, and outputs are aggregated feature-wise by concatenation or addition.\n",
    "$$\n",
    "\\vec{h}'_i = {\\LARGE \\vert}_{k=1}^K \\sigma\\left(\\sum_{j\\in\\mathcal{N}_i}\\alpha_{ij}^k{\\bf W}^k\\vec{h}_j\\right)\\, ,\n",
    "$$\n",
    "where $\\alpha_{ij}^k$ are the attention coefficients derived by the $k$-th replica, and ${\\bf W}^k$ is the weight matrix specifying the linear transformation of the $k$-th replica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import numpy   as np\n",
    "import pandas  as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib             import cm\n",
    "from scipy                  import stats\n",
    "from pathlib                import Path\n",
    "from torch_geometric.loader import DataLoader\n",
    "#\n",
    "### Import local libraries\n",
    "#\n",
    "from model import GraphAttentionNetwork\n",
    "from model import batches, passdata\n",
    "\n",
    "plt.rc('xtick', labelsize=18) \n",
    "plt.rc('ytick', labelsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare data\n",
    "\n",
    "Fisrt we need to load our dataset. We will use the `data.pth` that includes the adjacency matrix and node features for all molecules. Our training data will be these two descriptors and our target data will be the HOMO-LUMO gap for each molecule. The data is shuffled, hence we can directly divide our set into 60 % for training, 20 % testing, and the remaining 20 % for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "### Define data path\n",
    "#\n",
    "imhere  = Path.cwd()\n",
    "#\n",
    "### Load dataset\n",
    "#\n",
    "datapth = torch.load(imhere/'data.pth')\n",
    "#\n",
    "#datapth = datapth[:20_000] # Reduce as needed from size = 132_364\n",
    "#\n",
    "### Divide into 60% training, 20% testing, and 20% validating\n",
    "#\n",
    "limit   = 40*len(datapth)//100\n",
    "\n",
    "print(f'train = { len(datapth[:-limit]) }, '\n",
    "      f'test = { len(datapth[-limit:-limit//2]) }, '\n",
    "      f'validate = { len(datapth[-limit//2:]) }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Settings and hyperparameters\n",
    "\n",
    "Our optimization algorithm is the ADAptive Moment estimation, [Adam](https://arxiv.org/pdf/1412.6980.pdf), that is based on stochastic gradient descent. We will need to define the **learning rate** and the **weight decay**. The learning rate is a hyperparameter that controls how much we are adjusting the weights of our network with respect to the loss gradient, whereas the weight decay is a regularization term that penalizes large weights\n",
    "\n",
    "The number of **epochs** is the number of times the learning algorithm will work through the entire training dataset. One epoch means that each sample in the training dataset has had an opportunity to update the internal model parameters.\n",
    "\n",
    "Because we are interested in learning the regression for a continuous variable, we will use the Mean Squared Error **loss function**.\n",
    "$$\n",
    "\\mathrm{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "where $N$ is the number of samples in the training set, $y_i$ is the reference value, and $\\hat{y}_i$ is predicted value.\n",
    "\n",
    "The training, testing, and validation data may be used in a loop function,\n",
    "\n",
    "~~~\n",
    "for batch in training: print(batch.shape)\n",
    "~~~\n",
    "\n",
    "each loop will automatically pass a sample to the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "### Training parameters\n",
    "#\n",
    "learnig_rate = 1e-5\n",
    "weight_decay = 1e-5\n",
    "\n",
    "epochs       = 1\n",
    "test_epoch   = 2\n",
    "#\n",
    "### Define neural network\n",
    "#\n",
    "network = GraphAttentionNetwork(hidden_nf=4, output_nf=1, attention_nf=1, reduce='cat', drop=0.0)\n",
    "#\n",
    "### Optimizer and Loss\n",
    "#\n",
    "optimizer = torch.optim.Adam(params=network.parameters(), lr=learnig_rate, weight_decay=weight_decay)\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "#\n",
    "### Training and testing data\n",
    "#\n",
    "training   = DataLoader(datapth[:-limit], shuffle=True)\n",
    "testing    = DataLoader(datapth[-limit:-limit//2], shuffle=False)\n",
    "validating = DataLoader(datapth[-limit//2:], shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training\n",
    "\n",
    "We can now train our neural network for the total `epochs` we selected and testing it every `test_epoch` epochs.\n",
    "\n",
    "Passing training data to the networks consists in five steps:\n",
    "\n",
    "1. Set the gradients to zero, `optimizer.zero_grad()`.\n",
    "\n",
    "2. Pass batch to the network, `output = network(batch)`.\n",
    "\n",
    "3. Compute the loss, `loss = criterion(output, y)`.\n",
    "\n",
    "4. Perform backward pass, `loss.backward()`.\n",
    "\n",
    "5. Perform the optimization step, `optimizer.step()`.\n",
    "\n",
    "Keep in mind that during testing you **DO NOT** want to update the gradients in your neural network. Otherwise you will leak testing information and your model will also learn from the testing set. To prevent this from happening, you need to use the `torch.no_grad()` context manager.  This will prevent the gradient from being updated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "\n",
    "    # your code here for the training set\n",
    "\n",
    "    print(f'{epoch+1},train,{loss:.4f}')\n",
    "\n",
    "    if (epoch+1)%test_epoch == 0:\n",
    "        # your code here for the testing set\n",
    "\n",
    "        print(f'{epoch+1},test,{loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test the model\n",
    "\n",
    "Use the validation set to compare the reference and predicted HOMO-LUMO gap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
