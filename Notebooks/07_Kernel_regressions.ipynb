{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel regressions\n",
    "\n",
    "Kernel regression is a technique for non-parametric estimation of regression models, this means that the method does not make assumptions about the distribution of the data. Unlike linear regression, which estimates a single constant coefficient for each predictor variable, kernel regression estimates a smooth function of the predictor variables. It does it through the kernel, which is a function that determines how similar two points are and estimates new values based on the values from nearby points. This makes it well suited for data sets where the underlying relationship between the dependent and independent variables is non-linear. This method can be applied to curve fitting, prediction, classification, and density estimation. It is also well-suited for data with discontinuities or nonlinear structure. In general, any situation where you need to estimate an unknown function from data can benefit from using kernel regression. There exist several methods, some of them are listed below.\n",
    "\n",
    "The **Watson estimator** is used for data with high-dimensional features or with highly correlated variables. The kernel $K_h$ for a bandwidth $h$ is used in the expression\n",
    "$$\n",
    "\\hat{m}_h(x) = \\frac{\\sum_{i=1}^n K_h (x - x_i)\\, y_i}{\\sum_{i=1}^n K_h (x - x_i)}.\n",
    "$$\n",
    "\n",
    "The **Priestley-Chao** kernel, on the other hand, is known for being translation invariant. This means that the estimates produced by the regression will be unchanged if the data points are shifted by any constant amount. This is especially important in applications where data may be subject to measurement error or other sources of imprecision, as it ensures that these errors will not affect the results of the analysis. It is calculated with\n",
    "$$\n",
    "\\hat{m}_\\mathrm{PC}(x) = h^{-1} \\sum_{1=2}^n (x_i - x_{i-1}) K \\left( \\frac{x - x_i}{h} \\right)\\, y_i,\n",
    "$$\n",
    "where $h$ is the bandwidth or smoothing parameter.\n",
    "\n",
    "Finally, the **Gasser-Müller** kernel is considered to be more efficient than other types of kernel functions. It has been shown to be more accurate when predicting outcomes for data sets with a high degree of variance. Additionally, the Gasser kernel can handle non-linear relationships between input data points better than other kernel functions. This makes it a popular choice for use in machine learning applications.\n",
    "$$\n",
    "\\hat{m}_\\mathrm{GM}(x) = h^{-1} \\sum_{1=1}^n \\left[ \\int_{s_{i-1}}^{s_i} K \\left( \\frac{x - u}{h} \\right)\\, du \\right]\\, y_i,\n",
    "$$\n",
    "where $s_i = (x_{i-1} + x_i)/2$.\n",
    "\n",
    "## Overview\n",
    "\n",
    "We will explore representative examples of the gaussian kernel for regression applications using the 1D and its multi-dimensional generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats             import norm\n",
    "from scipy.stats             import multivariate_normal\n",
    "\n",
    "from sklearn.kernel_ridge    import KernelRidge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics         import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gaussian kernel regression\n",
    "\n",
    "In Gaussian kernel regression the shape of the kernel is the Gaussian curve:\n",
    "$$\n",
    "\\frac{1}{\\sqrt{2\\,\\pi}} \\exp \\left[ - \\frac{z^2}{2} \\right].\n",
    "$$\n",
    "\n",
    "Each constructed kernel describes a normal distribution with mean value ${\\bf x}_i$ and standard deviation $b$, where $b$ is a hyperparameter that controls the width of the Gaussian:\n",
    "$$\n",
    "k(x, x_i) = \\frac{1}{\\sqrt{2\\,\\pi}} \\exp \\left[ - \\frac{(x-x_i)^2}{2\\,b^2} \\right].\n",
    "$$\n",
    "\n",
    "Note that the normalization of the Gaussian does not matter as the weights are being normalized themselves.\n",
    "\n",
    "The weights for a given new input $\\tilde x$ are calculated from the normalized kernel values:\n",
    "$$\n",
    "w_i = \\frac{k(\\tilde x, x_i)}{\\sum_{l=1}^N k(x_l, x_i)}.\n",
    "$$\n",
    "\n",
    "The prediction $\\tilde y$ is obtained by multiplying the weight vector ${\\bf w} = [w_1, w_2, \\dots, w_N]$ with the label vector ${\\bf y} = [y_1, y_2, \\dots, y_N]$:\n",
    "$$\n",
    "\\tilde y = \\sum_{i=1}^N w_i\\, y_i.\n",
    "$$\n",
    "\n",
    "### 1.1 One-dimensional regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 1D dataset\n",
    "X = np.array([10,20,30,40,50,60,70,80,90,100,110,120])\n",
    "Y = np.array([2337,2750,2301,2500,1700,2100,1100,1750,1000,1642,2000,1932])\n",
    "\n",
    "# Plot the dataset\n",
    "fig,ax=plt.subplots( figsize=(8,8) )\n",
    "\n",
    "plt.rc('xtick', labelsize=18) \n",
    "plt.rc('ytick', labelsize=18)\n",
    "\n",
    "ax.set_xlabel('x', fontsize=18)\n",
    "ax.set_ylabel('y', fontsize=18)\n",
    "\n",
    "ax.set_title('Data',fontsize=20)\n",
    "\n",
    "ax.scatter(X, Y, color='blue', label='Training')\n",
    "\n",
    "plt.legend(fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of simplicity, we will create an object for the implementation of the linear regression model. The object will have the following methods:\n",
    "    \n",
    "- **kernel**          : This method will compute the Gaussian kernel for a given set of points.\n",
    "- **predict**         : This method will compute the prediction for a given set of points.\n",
    "- **plot_kerneles**   : This method will plot the Gaussian kernels.\n",
    "- **plot_predictions**: This method will plot the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a class for Gaussian Kernel Regression\n",
    "class GaussianKernelRegression1D:\n",
    "    #\n",
    "    # Initialization\n",
    "    #\n",
    "    def __init__(self, x, y, b):\n",
    "\n",
    "        self.x = np.asarray(x)\n",
    "        self.y = np.asarray(y)\n",
    "        self.b = b\n",
    "    #\n",
    "    # Implement the Gaussian Kernel\n",
    "    #\n",
    "    def kernel(self, z):\n",
    "\n",
    "        return ( 1.0/np.sqrt(2.0*np.pi) )*np.exp(-0.5*z*z)\n",
    "    #\n",
    "    # Calculate weights and return prediction\n",
    "    #\n",
    "    def predict(self, X):\n",
    "\n",
    "        kernels = [self.kernel( (xi-X)/self.b ) for xi in self.x]\n",
    "        weights = [kernel/np.sum(kernels) for kernel in kernels]\n",
    "\n",
    "        return np.dot(weights, self.y)\n",
    "    #\n",
    "    # Visualize the kernels\n",
    "    #\n",
    "    def plot_kernels(self, points=100):\n",
    "\n",
    "        plt.figure( figsize = (12,6) )\n",
    "\n",
    "        plt.title('Kernel', fontsize=20)\n",
    "\n",
    "        plt.ylabel(r'Kernel Weights $w_i$', fontsize=18)\n",
    "        plt.xlabel('x', fontsize=18)\n",
    "\n",
    "        for xi in self.x:\n",
    "            x_normal = np.linspace(xi - 3.0*self.b, xi + 3.0*self.b, num=points)\n",
    "            y_normal = norm.pdf(x_normal, xi, self.b)\n",
    "\n",
    "            plt.plot(x_normal, y_normal, label=r'Kernel at $x_i$={}'.format(xi))\n",
    "            \n",
    "        plt.legend(fontsize=14)\n",
    "    #\n",
    "    # Visualize the predictions\n",
    "    #\n",
    "    def plot_predictions(self, X, points=100):\n",
    "\n",
    "        max_y = 0\n",
    "\n",
    "        plt.figure( figsize = (12,6) )\n",
    "\n",
    "        plt.title('Prediction', fontsize=20)\n",
    "\n",
    "        plt.ylabel(r'Kernel Weights $w_i$', fontsize=18)\n",
    "        plt.xlabel('x', fontsize=18)\n",
    "        \n",
    "        for xi in self.x:\n",
    "            x_normal = np.linspace(xi - 3.0*self.b, xi + 3.0*self.b, num=points)\n",
    "            y_normal = norm.pdf(x_normal, xi, self.b)\n",
    "\n",
    "            max_y = max(max(y_normal), max_y)\n",
    "\n",
    "            plt.plot(x_normal, y_normal, label=r'Kernel at $x_i$={}'.format(xi))\n",
    "            \n",
    "        plt.plot([X,X], [0, max_y], 'k-', lw=2, dashes=[2, 2])\n",
    "        \n",
    "        plt.legend(fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the width of the Gaussian kernel\n",
    "b = 20\n",
    "\n",
    "# Create an instance of the GaussianKernelRegression class\n",
    "gaussian_kernel_regression = GaussianKernelRegression1D(X, Y, b)\n",
    "gaussian_kernel_regression.plot_kernels(points=100)\n",
    "\n",
    "# Prediction for test x\n",
    "x = 26.0\n",
    "gaussian_kernel_regression.plot_predictions(x, points=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the 1-dimensional prediction\n",
    "xlist = np.linspace(0, 120, 240)\n",
    "ylist = np.array([gaussian_kernel_regression.predict(x) for x in xlist])\n",
    "\n",
    "fig,ax = plt.subplots( figsize=(8,8) )\n",
    "\n",
    "ax.set_title('1D Gaussian Kernel',fontsize=20)\n",
    "\n",
    "ax.set_xlabel('x',fontsize=18)\n",
    "ax.set_ylabel('y',fontsize=18)\n",
    "\n",
    "ax.scatter(X, Y, color='b', label='Training')\n",
    "ax.plot(xlist, ylist, color='k', label='Prediction')\n",
    "\n",
    "plt.legend(fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 N-dimensional regression\n",
    "\n",
    "For $N$-dimenisonal inputs, we need to calculate the kernels with the Euclidean metric instead.\n",
    "$$\n",
    "k(x, x_i) = \\frac{1}{\\sqrt{2\\,\\pi}} \\exp \\left ( - \\frac{\\|{\\bf x}-{\\bf x}_i\\|^2}{2\\,b^2} \\right ).\n",
    "$$\n",
    "\n",
    "Once again, we will create an object for the implementation of the linear regression model. The object will have the following methods:\n",
    "    \n",
    "- **kernel**          : This method will compute the Gaussian kernel for a given set of points.\n",
    "- **predict**         : This method will compute the prediction for a given set of points.\n",
    "- **plot_kerneles**   : This method will plot the Gaussian kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianKernelRegressionND:\n",
    "    #\n",
    "    # Initialization\n",
    "    #\n",
    "    def __init__(self, x, y, b):\n",
    "\n",
    "        self.x = np.asarray(x)\n",
    "        self.y = np.asarray(y)\n",
    "        self.b = b\n",
    "    #\n",
    "    # Implement the Gaussian Kernel\n",
    "    #\n",
    "    def gaussian_kernel(self, z):\n",
    "\n",
    "        return (1.0/np.sqrt(2.0*np.pi))*np.exp(-0.5*z*z)\n",
    "    #\n",
    "    # Calculate weights and return prediction\n",
    "    #\n",
    "    def predict(self, X):\n",
    "\n",
    "        kernels = [self.gaussian_kernel( ( np.linalg.norm(xi-X) )/self.b ) for xi in self.x]\n",
    "        weights = [(kernel/np.sum(kernels)) for kernel in kernels]\n",
    "\n",
    "        weights = np.asarray(weights)\n",
    "\n",
    "        return np.dot(weights.T, self.y)\n",
    "    #\n",
    "    # Visualize the kernels\n",
    "    #\n",
    "    def plot_kernels(self, points=100):\n",
    "        zsum = np.zeros( (points,points) )\n",
    "\n",
    "        plt.figure(figsize = (14,8))\n",
    "\n",
    "        ax = plt.axes(projection = '3d')\n",
    "\n",
    "        plt.rc('xtick', labelsize=14) \n",
    "        plt.rc('ytick', labelsize=14)\n",
    "\n",
    "        ax.set_ylabel('y', fontsize=14)\n",
    "        ax.set_xlabel('x', fontsize=14)\n",
    "        ax.set_zlabel(r'Kernel Weights $w_i$', labelpad=5, fontsize=14)\n",
    "\n",
    "        for xi in self.x:\n",
    "            x, y  = np.mgrid[0:points:complex(0.0, points), 0:points:complex(0.0, points)]\n",
    "            xy    = np.column_stack([x.flat, y.flat])\n",
    "            z     = multivariate_normal.pdf(xy, mean=xi, cov=self.b)\n",
    "            z     = z.reshape(x.shape)\n",
    "            zsum += z\n",
    "            \n",
    "        ax.plot_surface(x, y, zsum)\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a reference funcion to use for machine learning\n",
    "def reference_function_3d(x=None, y=None, mesh=False):\n",
    "    \n",
    "    if mesh: x, y = np.meshgrid(x, y)\n",
    "\n",
    "    z = ( x * np.exp(-x**2 - y**2) )\n",
    "\n",
    "    if mesh: return x, y, z\n",
    "    \n",
    "    else: return z\n",
    "\n",
    "# Generate a grid of points for a 3D function\n",
    "points  = np.linspace(-2, 2, 51)\n",
    "\n",
    "x, y, z = reference_function_3d(x=points, y=points, mesh=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a seed for reproducibility\n",
    "np.random.seed(23971)\n",
    "number_of_points = 100\n",
    "\n",
    "b = 0.25\n",
    "\n",
    "x_random = np.random.uniform(-2, 2, number_of_points)\n",
    "y_random = np.random.uniform(-2, 2, number_of_points)\n",
    "\n",
    "# Plot function using a dense regular mesh\n",
    "fig, (ax1, ax2, ax3) = plt.subplots( nrows=3, figsize=(10, 24) )\n",
    "\n",
    "plt.subplots_adjust(hspace=0.2)\n",
    "\n",
    "plt.rc('xtick', labelsize=18) \n",
    "plt.rc('ytick', labelsize=18)\n",
    "\n",
    "ax1.set_title('Plot for dense mesh of points', fontsize=20)\n",
    "ax2.set_title(f'Triangulation plot for '\n",
    "              f'{number_of_points} random points', fontsize=20)\n",
    "ax3.set_title('Gaussian Kernel Regression on '\n",
    "              f'{number_of_points} random points', fontsize=20)\n",
    "\n",
    "ax1.set(xlim=(-2, 2), ylim=(-2, 2))\n",
    "ax2.set(xlim=(-2, 2), ylim=(-2, 2))\n",
    "ax3.set(xlim=(-2, 2), ylim=(-2, 2))\n",
    "\n",
    "# Dense mesh of points\n",
    "ax1.contour(x, y, z, levels=14, linewidths=0.5, colors='k')\n",
    "\n",
    "contour_ax1 = ax1.contourf(x, y, z, levels=14, cmap=\"RdBu_r\")\n",
    "\n",
    "fig.colorbar(contour_ax1, ax=ax1)\n",
    "\n",
    "ax1.plot(x, y, 'ko', ms=1)\n",
    "\n",
    "# Triangulation plot for random points\n",
    "X_train = np.vstack((x_random, y_random)).T\n",
    "Y_train = reference_function_3d(x_random, y_random)\n",
    "\n",
    "ax2.tricontour(x_random, y_random, Y_train, levels=14, linewidths=0.5, colors='k')\n",
    "contour_ax2 = ax2.tricontourf(x_random, y_random, Y_train, levels=14, cmap=\"RdBu_r\")\n",
    "\n",
    "fig.colorbar(contour_ax2, ax=ax2)\n",
    "\n",
    "ax2.plot(x_random, y_random, 'ko', ms=5)\n",
    "\n",
    "# Train Gaussian Kernel Regression on the random points\n",
    "gaussian_kernel_regression = GaussianKernelRegressionND(X_train, Y_train, b)\n",
    "\n",
    "x_flat = x.flatten()\n",
    "y_flat = y.flatten()\n",
    "\n",
    "z_flat = [gaussian_kernel_regression.predict( [x_val, y_val] )\n",
    "          for  x_val, y_val in zip(x_flat, y_flat)]\n",
    "\n",
    "ax3.tricontour(x_flat, y_flat, z_flat, levels=14, linewidths=0.5, colors='k')\n",
    "contour_ax3 = ax3.tricontourf(x_flat, y_flat, z_flat, levels=14, cmap=\"RdBu_r\")\n",
    "\n",
    "fig.colorbar(contour_ax3, ax=ax3)\n",
    "ax3.plot(x_random, y_random, 'ko', ms=5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Assignment\n",
    ">\n",
    "> How is the training affected by different choices of $b$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a different set of points\n",
    "X = np.array([[ 11, 15],\n",
    "              [ 22, 30],\n",
    "              [ 33, 45],\n",
    "              [ 44, 60],\n",
    "              [ 50, 52],\n",
    "              [ 67, 92],\n",
    "              [ 78,107],\n",
    "              [ 89,123],\n",
    "              [100,137]])\n",
    "\n",
    "Y = np.array([2337,2750,2301,2500,1700,2100,1100,1750,1000,1642,2000,1932])\n",
    "\n",
    "b = 10\n",
    "\n",
    "# Create an instance of the GaussianKernelRegression class\n",
    "gaussian_kernel_regression = GaussianKernelRegressionND(X, Y, b)\n",
    "\n",
    "plt.rc('xtick', labelsize=12)\n",
    "plt.rc('ytick', labelsize=12)\n",
    "\n",
    "gaussian_kernel_regression.plot_kernels(points=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Assignment\n",
    ">\n",
    "> How are the weights modified for different values of the bandwidth parameter $b$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ridge kernel regression\n",
    "\n",
    "This method uses the kernel to transform our dataset to the kernel space and then performs a linear regression in kernel-space. Therefore, one should always **choose the appropriate kernel for a problem**. Here, we will be using using a polynomial kernel for two vectors (two points in our one-dimensional example) ${\\bf x}_1$ and ${\\bf x}_2$ that is given by\n",
    "$$\n",
    "K({\\bf x}_1, {\\bf x}_2) = \\left[ \\gamma ({\\bf x}_1^\\mathrm{T}\\, {\\bf x}_2) + c \\right]^d,\n",
    "$$\n",
    "where $\\gamma$ is the kernel coefficient, $c$ is the independent term and $d$ is the degree of the polynomial. In this case, $\\gamma$ and $c$ play a minor role, and their default value of 1.0 is adequate, so we will only focus on optimizing the polynomial degree $d$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a seed for reproducibility\n",
    "np.random.seed(seed=5)\n",
    "\n",
    "data_points = 300\n",
    "test_points = 1001\n",
    "\n",
    "# Generate a data set for machine learning\n",
    "x = np.linspace(-2, 2, num=data_points) + \\\n",
    "    np.random.normal(0.0, 0.3, data_points)\n",
    "y = np.cos(x) - 2*np.sin(x) + 3*np.cos(x*2) + \\\n",
    "    np.random.normal(0.0, 1.0, data_points)\n",
    "\n",
    "# Create list with points within the range of the dataset\n",
    "x_pred = np.linspace(np.amin(x), np.amax(x), num=test_points, endpoint=True)\n",
    "x_pred = np.array(x_pred).reshape(-1, 1)\n",
    "\n",
    "# Split the dataset into 80% for training and 20% for testing\n",
    "x = x.reshape( (x.size,1) )\n",
    "x_train,x_test,y_train,y_test = train_test_split(x, y, train_size=0.8, shuffle=True)\n",
    "\n",
    "# Plot the training and testing dataset\n",
    "fig,ax=plt.subplots( figsize=(8,8) )\n",
    "\n",
    "ax.set_title('Training and testing data',fontsize=20)\n",
    "\n",
    "ax.set_xlabel('X values',fontsize=28)\n",
    "ax.set_ylabel('cos(x)+2sin(x)+3cos(2x)',fontsize=18)\n",
    "\n",
    "ax.scatter(x_train, y_train, color='#005AB5', marker=\"o\", label='Training')\n",
    "ax.scatter(x_test,  y_test,  color='#DC3220', marker=\"D\", label='Testing')\n",
    "\n",
    "plt.legend(loc='best', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to plot and compare results\n",
    "def plot_comparison(degrees=None, regularization=None, reg_degree=4):\n",
    "\n",
    "    # Create lists to store results\n",
    "    y_pred = []\n",
    "\n",
    "    training_rmse = []\n",
    "    testing_rmse  = []\n",
    "\n",
    "    training_predictions = []\n",
    "    testing_predictions  = []\n",
    "\n",
    "    # Shape of the plot\n",
    "    rows, cols = 2, 2\n",
    "\n",
    "    loop_vector = degrees if degrees is not None else regularization\n",
    "\n",
    "    for choice in loop_vector:\n",
    "        \n",
    "        if degrees is not None:\n",
    "            kernel_ridge_regression = KernelRidge(alpha=1.0,\n",
    "                kernel='polynomial',\n",
    "                degree=choice)\n",
    "            \n",
    "        if regularization is not None:\n",
    "            kernel_ridge_regression = KernelRidge(alpha=choice,\n",
    "                kernel='polynomial',\n",
    "                degree=reg_degree)\n",
    "            \n",
    "        kernel_ridge_regression.fit(x_train, y_train)\n",
    "\n",
    "        y_pred.append( kernel_ridge_regression.predict(x_pred) )\n",
    "\n",
    "        pred_y_train = kernel_ridge_regression.predict(x_train)\n",
    "        pred_y_test  = kernel_ridge_regression.predict(x_test)\n",
    "        \n",
    "        # Calculate training and testing errors\n",
    "        training_predictions.append(pred_y_train)\n",
    "\n",
    "        mean_error = np.sqrt(mean_squared_error(y_train, pred_y_train))\n",
    "        training_rmse.append(mean_error)\n",
    "\n",
    "        testing_predictions.append(pred_y_test)\n",
    "        mean_error = np.sqrt(mean_squared_error(y_test, pred_y_test))\n",
    "        testing_rmse.append(mean_error)\n",
    "\n",
    "    # Plot the results for each polynomial degree\n",
    "    fig, axs = plt.subplots( rows, cols, figsize=(12,12) )\n",
    "\n",
    "    for ax in axs.flat:\n",
    "        ax.set_xlabel('x', fontsize = 18)\n",
    "        ax.set_ylabel('y', fontsize = 18)\n",
    "\n",
    "        ax.label_outer()\n",
    "\n",
    "        ax.scatter(x_train, y_train, color='#005AB5', marker='o', label='Training')\n",
    "        ax.scatter(x_test,  y_test,  color='#DC3220', marker='D', label='Testing')\n",
    "\n",
    "    idx = 0\n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            \n",
    "            if degrees is not None:\n",
    "                title_string = r'$d$ = {:2d}'.format( degrees[idx] )\n",
    "                \n",
    "            if regularization is not None:\n",
    "                title_string = r'$\\alpha$ = {:.0e}'.format( regularization[idx] )\n",
    "\n",
    "            axs[row,col].set_title(title_string, fontsize=20)\n",
    "\n",
    "            axs[row,col].plot(x_pred, y_pred[idx], color='black', lw=4)\n",
    "\n",
    "            axs[row,col].annotate(f'Training RMSE = {training_rmse[idx]:.2f}',\n",
    "                    xy=(0.2,0.2), xycoords='axes fraction', fontsize=14)\n",
    "            \n",
    "            axs[row,col].annotate(f'Testing RMSE = {testing_rmse[idx]:.2f}',\n",
    "                    xy=(0.2,0.1), xycoords='axes fraction', fontsize=14)\n",
    "\n",
    "            idx += 1\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparison( degrees=[1, 2, 3, 4] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Assignnment\n",
    ">\n",
    "> What happens to the RMSE in both training and testing as the degree of the polynomial increases?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Regularization parameter\n",
    "\n",
    "The regularization paremeter, $\\alpha$, should also be optimized. It controls the conditioning of the problem, and larger $\\alpha$ values result into results that are more “general” and ignore the peculiarities of the problem. Larger values of $\\alpha$ allow to ignore noise in the system, but this might result into the model being blind to actual trends of the data.\n",
    "\n",
    "If we perform our kernel ridge regression for different $\\alpha$ values, we can see its effect, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparison( regularization=[1e-4, 1e0, 1e2, 1e4] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Assignment\n",
    ">\n",
    "> How do the testing RMSEs change as the polynomial degree increases?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
