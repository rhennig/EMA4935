{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis\n",
    "\n",
    "Based on https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, we will discuss the concept of principal component analysis (PCA), an unsupervised machine learning method for dimensionality reduction, and how to implement it in Python using the scikit-learn library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy   as np\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from mendeleev.fetch       import fetch_table\n",
    "\n",
    "plt.rc('xtick', labelsize=18) \n",
    "plt.rc('ytick', labelsize=18)\n",
    "\n",
    "blue   = '#0021A5'\n",
    "orange = '#FA4616'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Definition: PCA is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated numerical variables into a set of values of linearly uncorrelated variables called principal components.\n",
    "\n",
    "- Principal component analysis is a fast and flexible unsupervised method for dimensionality reduction in data. It is a linear transformation technique that is widely used across different fields, most prominently for feature extraction and dimensionality reduction in machine learning.\n",
    "\n",
    "- PCA is an **unsupervised linear dimensionality reduction technique** that can be utilized for extracting information from a high-dimensional space by projecting it into a lower-dimensional sub-space.\n",
    "\n",
    "- PCA aims to preserve the directions of greatest variance in the data, reducing the influence of low-variance directions that may represent noise or redundant information.\n",
    "\n",
    "- Dimensions are nothing but features that represent the data. For example, the atomic descriptors we used already included the atomic number, atomic and ionic radii, electronegetivity, etc. Each of these descriptor components is a dimension. Note: Features, Dimensions, and Variables are all referring to the same idea. You will find them being used interchangeably.\n",
    "\n",
    "- You can use PCA to cluster similar data points based on the feature correlation between them.\n",
    "\n",
    "### 1.1 Five steps of PCA\n",
    "\n",
    "Principal component analysis can be broken down into five steps. We will go through each step, explaining what PCA does and discuss the underlying mathematical concepts such as standardization, covariance, eigenvectors and eigenvalues without focusing on how to compute them.\n",
    "\n",
    "1. [**Standardize**](####-1.2.1-Step-1:-Standardization) the range of continuous initial variables, i.e., zero mean, unit variance.\n",
    "\n",
    "2. Compute the [**covariance matrix**](####-1.2.2-Step-2:-Covariance-matrix) to identify correlations.\n",
    "\n",
    "3. Compute the [**eigenvectors** and **eigenvalues**](####-1.2.3-Step-3:-Eigenvectors-and-eigenvalues) of the covariance matrix to identify the principal components.\n",
    "\n",
    "4. Create a [**feature vector**](####-1.2.4-Step-4:-Create-the-feature-vector) to decide which principal components to keep.\n",
    "\n",
    "5. [**Recast**](####-1.2.5-Step-5:-Recast-data-along-principal-components) the data along the principal components axes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Illustrative example of PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points       = 200\n",
    "random_state = np.random.RandomState(1)\n",
    "\n",
    "X = np.dot(random_state.rand(2, 2), random_state.randn(2, points)).T\n",
    "\n",
    "plt.figure( figsize=(8, 8) )\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], color=blue, s=64)\n",
    "plt.xlabel('x', fontsize=18)\n",
    "plt.ylabel('y', fontsize=18)\n",
    "\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is apparent that there is a nearly linear relationship between the $x$ and $y$ variables. This is reminiscent of the linear regression data we explored previously, but the problem setting here is slightly different. Rather than attempting to predict the $y$ values from the $x$ values, the unsupervised learning problem attempts to learn about the relationship between the $x$ and $y$ values.\n",
    "\n",
    "In principal component analysis, this relationship is quantified by finding a list of the principal axes in the data, and using those axes to describe the dataset.\n",
    "\n",
    "#### 1.2.1 Step 1: Standardization\n",
    "\n",
    "The goal is to standardize the range of the variables so that each one of them contributes equally to the analysis.\n",
    "\n",
    "PCA, is sensitive to the variances of the initial variables. If there are large differences between the ranges of the different features, those with larger ranges will dominate over those with small ranges.\n",
    "\n",
    "For example, the $x$ values have a much larger range than the $y$ values. Hence the $x$ values would dominate over the $y$ values, which will lead to biased results. So, transforming the data to comparable scales can prevent this problem.\n",
    "\n",
    "Mathematically, this can be done by subtracting the mean $\\mu$ and dividing by the standard deviation $\\sigma$ for each value of each variable. This is applied column-wise to each feature:\n",
    "$$\n",
    "\\overline{\\mathbf{X}} = \\frac{\\mathbf{X} - \\mu}{\\sigma}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean   = np.mean(X, axis=0)\n",
    "stddev = np.std(X,  axis=0, ddof=1)\n",
    "\n",
    "X_transformed = (X - mean)/stddev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can alternatively use sklearn's StandardScaler to achieve the same result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_scaling = StandardScaler(with_mean=True, with_std=True)\n",
    "\n",
    "standard_scaling.fit(X)\n",
    "\n",
    "X_transformed_using_sklearn = standard_scaling.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that the two methods produce the same result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(X_transformed, X_transformed_using_sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that scikit-learn uses a normalization of $N$ instead on $N-1$ to calculate the standard deviation. To change this behavior, you can set the parameter ddof to 0 in `np.std()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 1.2.2 Step 2: Covariance matrix\n",
    "\n",
    "The **covariance matrix** is a square matrix that captures the pairwise correlation between the components of a vector. For a dataset with $ n $ observations and $ d $ dimensions for the vector, represented as a matrix $ \\mathbf{X} \\in \\mathbb{R}^{n \\times d} $, the covariance matrix $ \\boldsymbol{\\Sigma} \\in \\mathbb{R}^{d \\times d} $ is defined as:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\Sigma} = \\mathrm{cov}(\\mathbf{X}) = \\frac{1}{n - 1} \\mathbf{X}^\\top \\mathbf{X}\n",
    "$$\n",
    "\n",
    "where $ \\mathbf{X} $ is assumed to be **mean-centered**, i.e., each column (feature) has zero mean.\n",
    "\n",
    "Each entry $ \\Sigma_{ij} $ of the covariance matrix represents the covariance between feature $ i $ and feature $ j $:\n",
    "\n",
    "$$\n",
    " \\Sigma_{ij} = \\mathbf{cov}(x_i, x_j) = \\frac{1}{n - 1} \\sum_{k=1}^{n} (x_{ki} - \\bar{x}_i)(x_{kj} - \\bar{x}_j)\n",
    "$$\n",
    "\n",
    "For example, for a 3-dimensional data set with 3 variables $x$, $y$, and $z$, $\\mathrm{\\Sigma}(\\mathbf{X}) \\in \\mathbb{R}^{3 \\times 3}$ with elements\n",
    "$$\n",
    "\\mathrm{\\Sigma}(\\mathbf{X}) = \n",
    "\\begin{bmatrix}\n",
    "   \\mathrm{cov}(x,x) & \\mathrm{cov}(x,y) & \\mathrm{cov}(x,z) \\\\\n",
    "   \\mathrm{cov}(y,x) & \\mathrm{cov}(y,y) & \\mathrm{cov}(y,z) \\\\\n",
    "   \\mathrm{cov}(z,x) & \\mathrm{cov}(z,y) & \\mathrm{cov}(z,z) \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the covariance matrix\n",
    "mean_vector = np.mean(X_transformed, axis=0)\n",
    "\n",
    "covariance  = (X_transformed - mean_vector).T.dot((X_transformed - mean_vector)) / (points - 1)\n",
    "\n",
    "print(f'Covariance matrix\\n{covariance}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can alternatively use the numpy function `cov`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariance = np.cov(X_transformed.T)\n",
    "\n",
    "print(f'\\nCovariance matrix\\n{covariance}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the covariance of a variable with itself is its variance, namely, $\\mathrm{cov}(a,a) = \\mathrm{var}(a)$. With that in mind, we can easily see that the diagonal elements of $\\mathrm{cov}(\\mathbf{X})$ diagonal are the variance for each initial variable. And since the covariance is commutative, i.e., $\\mathrm{cov}(a,b) = \\mathrm{cov}(b,a)$, the entries of the covariance matrix are symmetric with respect to the diagonal elements of $\\mathrm{cov}(\\mathbf{X})$. This means that the upper and the lower triangular portions are equal:\n",
    "$$\n",
    "\\mathrm{cov}(\\mathbf{X}) = \n",
    "\\begin{bmatrix}\n",
    "   \\mathrm{var}(x,x) & \\mathrm{cov}(x,y) & \\mathrm{cov}(x,z) \\\\\n",
    "   \\mathrm{cov}(x,y) & \\mathrm{var}(y,y) & \\mathrm{cov}(y,z) \\\\\n",
    "   \\mathrm{cov}(x,z) & \\mathrm{cov}(y,z) & \\mathrm{var}(z,z) \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Key properties:\n",
    "- $ \\boldsymbol{\\Sigma} $ is symmetric: $ \\Sigma_{ij} = \\Sigma_{ji} $\n",
    "- The diagonal elements $ \\Sigma_{ii} $ represent the variances of individual features\n",
    "- The matrix is positive semi-definite\n",
    "- The sign of the covariance entries $a$ and $b$ determines the correlation:\n",
    "  - $\\mathrm{cov}(a,b) > 0$: Both increase or decrease together (**correlated**)\n",
    "  - $\\mathrm{cov}(a,b) < 0$: One increases when the other decreases (**inversely correlated**)\n",
    "  - $\\mathrm{cov}(a,b) = 0$: The two variables are **uncorrelated**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 1.2.3 Step 3: Eigenvectors and eigenvalues\n",
    "\n",
    "Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data.\n",
    "\n",
    "What do we mean by principal components?\n",
    "\n",
    "- Principal components are new variables constructed as linear combinations or mixtures of the initial variables.\n",
    "- These combinations are such that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components.\n",
    "- So, the idea is that a $p$-dimensional data give $p$ principal components, but the PCA tries to put the maximum possible information in the first component, then the maximum of the remaining information in the second and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the covariance of a variable with itself is its variance, namely, $\\mathrm{cov}(a,a) = \\mathrm{var}(a)$. With that in mind, we can easily see that the diagonal elements of $\\mathrm{cov}(\\mathbf{X})$ diagonal are the variance for each initial variable. And since the covariance is commutative, i.e., $\\mathrm{cov}(a,b) = \\mathrm{cov}(b,a)$, the entries of the covariance matrix are symmetric with respect to the diagonal elements of $\\mathrm{cov}(\\mathbf{X})$. This means that the upper and the lower triangular portions are equal:\n",
    "$$\n",
    "\\mathrm{cov}(\\mathbf{X}) = \n",
    "\\begin{bmatrix}\n",
    "   \\mathrm{var}(x,x) & \\mathrm{cov}(x,y) & \\mathrm{cov}(x,z) \\\\\n",
    "   \\mathrm{cov}(x,y) & \\mathrm{var}(y,y) & \\mathrm{cov}(y,z) \\\\\n",
    "   \\mathrm{cov}(x,z) & \\mathrm{cov}(y,z) & \\mathrm{var}(z,z) \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Key properties:\n",
    "- $ \\boldsymbol{\\Sigma} $ is symmetric: $ \\Sigma_{ij} = \\Sigma_{ji} $\n",
    "- The diagonal elements $ \\Sigma_{ii} $ represent the variances of individual features\n",
    "- The matrix is positive semi-definite\n",
    "- The sign of the covariance entries $a$ and $b$ determines the correlation:\n",
    "  - $\\mathrm{cov}(a,b) > 0$: Both increase or decrease together (**correlated**)\n",
    "  - $\\mathrm{cov}(a,b) < 0$: One increases when the other decreases (**inversely correlated**)\n",
    "  - $\\mathrm{cov}(a,b) = 0$: The two variables are **uncorrelated**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues, eigenvectors = np.linalg.eig(covariance)\n",
    "\n",
    "print(f'eigenvalues  = {eigenvalues}')\n",
    "print(f'eigenvectors =\\n{eigenvectors}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the cumulative summation of the eigenvalues\n",
    "cummulative = np.cumsum(eigenvalues)/np.sum(eigenvalues)\n",
    "        \n",
    "print(f'cummulative summation = {cummulative}\\n')\n",
    "\n",
    "plt.figure( figsize=(8, 8) )\n",
    "\n",
    "plt.bar( np.arange(eigenvalues.size), 100*eigenvalues/np.sum(eigenvalues) , color=blue, label='Individual')\n",
    "plt.plot(100*cummulative, color=orange, label='Cumulative', lw=4)\n",
    "\n",
    "plt.xlabel('Principal components', fontsize=18)\n",
    "plt.ylabel('Percentage of explained variance', fontsize=18)\n",
    "\n",
    "plt.xticks(np.arange(eigenvalues.size), fontsize=18)\n",
    "plt.yticks(np.arange(0, 110, 10), fontsize=18)\n",
    "\n",
    "plt.legend(fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that there are as many principal components as variables in the data. Principal components are constructed in such a manner that the first principal component accounts for the **largest possible variance** in the data set.\n",
    "\n",
    "For our simple 2D dataset, we can guess the first principal component. It is a line going from the lower left to the upper right. It matches the blue marks because it goes through the origin and it’s the line in which the projection of the points is the most spread out. Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points to the origin).\n",
    "\n",
    "#### 1.2.4 Step 4: Create the feature vector\n",
    "\n",
    "We need to decide which principal components to keep.\n",
    "\n",
    "Computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call **feature vector**.\n",
    "\n",
    "So, the feature vector is simply a matrix whose columns are the eigenvectors of the components that we decide to keep. This makes it the first step towards dimensionality reduction, because if we choose to keep only $p$ eigenvectors (components) out of $n$, the final data set will only have $p$ dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vectors = (eigenvectors.T[:][:2])\n",
    "\n",
    "print(f'feature vectors =\\n{feature_vectors}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.5 Step 5: Recast data along principal components\n",
    "\n",
    "In the previous steps, apart from standardization, we did not make any changes to the data. We just selected the principal components and built the feature vector. The input data set remained always in terms of the original axes (i.e, in terms of the initial variables).\n",
    "\n",
    "In this last step, we use the feature vector formed from the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.\n",
    "$$\n",
    "\\mathrm{PCA}_\\mathrm{Dataset} = \\mathrm{Standardized\\, Original\\, Dataset} \\times \\mathrm{Feature\\, vectors}^\\mathrm{T}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca = np.dot(X_transformed, feature_vectors.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of simplicity, we can visualize the components we just calculated as vectors, where we will use the components as the direction of the vectors and the eigenvalues as the length of the vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_vector(v0, v1, ax=None):\n",
    "\n",
    "    arrowprops=dict(arrowstyle='->',\n",
    "                    lw=4,\n",
    "                    color=orange)\n",
    "    \n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 8), layout='tight')\n",
    "\n",
    "ax[0].scatter(X_transformed[:, 0], X_transformed[:, 1], color=blue, s=64, alpha=0.7)\n",
    "ax[1].scatter(X_pca[:, 0], X_pca[:, 1], color=blue, s=64, alpha=0.7)\n",
    "\n",
    "for length, vector, identity in zip(eigenvalues, eigenvectors.T, np.identity(2)):\n",
    "    draw_vector([0,0], 3.0*vector*np.sqrt(length),   ax=ax[0])\n",
    "    draw_vector([0,0], 3.0*identity*np.sqrt(length), ax=ax[1])\n",
    "\n",
    "ax[0].set_xlabel('x', fontsize=18)\n",
    "ax[0].set_ylabel('y', fontsize=18)\n",
    "ax[1].set_xlabel(f'PCA$_1$', fontsize=18)\n",
    "ax[1].set_ylabel(f'PCA$_2$', fontsize=18)\n",
    "\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These vectors represent the principal axes of the data, and the length of the vector illustrates the importance of that axis to describe its distribution. This procedures allows to measure the variance of the data projected onto that axis, where the projection of each data point onto the principal axes are the *principal components* of our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Principal Component Analysis using Chemical Data\n",
    "\n",
    "Let's start by creating a pandas dataframe containg the properties of the chemical elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "periodic_table = fetch_table('elements').select_dtypes([np.number])\n",
    "\n",
    "# Select all elements from hydrogen to lawrencium, the last of the actinides\n",
    "periodic_table = periodic_table.iloc[list(range(1,103)), :]\n",
    "\n",
    "# Drop the columns that include incomplete data\n",
    "periodic_table = periodic_table.dropna(axis=1)\n",
    "\n",
    "periodic_table.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Standardize the data\n",
    "\n",
    "Recall that the purpose of this procedure is that all variables will contribute equally to the analysis. For example, a the atomic number ranges from 1 to 118, while the atomic radius ranges from 0.25 Å for hydrogen to 2.65 Å for cesium. Hence the atomic number would dominate over the atomic radius, which will lead to biased results. So, transforming the data to comparable scales can prevent this problem.\n",
    "\n",
    "> ### Assignment\n",
    ">\n",
    "> Standardize the data and save it to the variable `periodic_table_transformed`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can alternatively use scikit-learn to standardize the data with the function `zscore` from the module `scipy.stats` in the form\n",
    "\n",
    "~~~\n",
    "periodic_table.apply(zscore).describe()\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Calculate the covariance matrix\n",
    "\n",
    "We can use the built-in function in the pandas library to calculate the covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariance = periodic_table_transformed.cov()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By means of a heatmap we can visualize and inspect the covariance matrix of the periodic table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots( figsize=(10, 8) )\n",
    "\n",
    "sns.heatmap(covariance, ax=ax, cmap='coolwarm', cbar=True,\n",
    "        xticklabels=covariance.columns,\n",
    "        yticklabels=covariance.columns)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Compute the eigenvectors and eigenvalues of the covariance matrix\n",
    "\n",
    "> ### Assignment\n",
    ">\n",
    "> Calculate the eigenvalues and eigenvectors. Save your results to the variables `eigenvalues` and `eigenvectors`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the cumulative summation of the eigenvalues\n",
    "cummulative = np.cumsum(eigenvalues)/np.sum(eigenvalues)\n",
    "\n",
    "plt.figure( figsize=(8, 8) )\n",
    "\n",
    "plt.bar( np.arange(eigenvalues.size), 100*eigenvalues/np.sum(eigenvalues) , color=blue, label='Individual')\n",
    "plt.plot(100*cummulative, color=orange, label='Cumulative', lw=4)\n",
    "\n",
    "plt.xlabel('Principal components', fontsize=18)\n",
    "plt.ylabel('Percentage of explained variance', fontsize=18)\n",
    "\n",
    "plt.legend(loc='best', fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Create the feature vectors\n",
    "\n",
    "> ### Assignment\n",
    ">\n",
    "> Generate these vectors using a total of two features. Save your result to the variable `feature_vectors`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Recast data along the principal component axes\n",
    "\n",
    "> ### Assignment\n",
    ">\n",
    "> Project the data to its principal component axes and save your results to the variable `periodic_table_pca`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_vector(v0, v1, ax=None):\n",
    "\n",
    "    arrowprops=dict(arrowstyle='->',\n",
    "                    linewidth=4,\n",
    "                    color=orange)\n",
    "    \n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 8), layout='tight')\n",
    "\n",
    "ax[0].scatter(periodic_table.vdw_radius,\n",
    "              periodic_table.dipole_polarizability,\n",
    "              s=periodic_table.index, color=blue, alpha=0.7)\n",
    "\n",
    "ax[1].scatter(periodic_table_pca.loc[:, 0],\n",
    "              periodic_table_pca.loc[:, 1],\n",
    "              s=periodic_table_pca.index, color=blue, alpha=0.7)\n",
    "\n",
    "for length, vector in zip(eigenvalues, np.identity(2)):\n",
    "    v = vector*np.sqrt(length)\n",
    "    draw_vector([0,0], v, ax=ax[1])\n",
    "\n",
    "ax[0].set_xlabel('vdW radius', fontsize=18)\n",
    "ax[0].set_ylabel('Dipole polarizability', fontsize=18)\n",
    "\n",
    "ax[1].set_xlabel('PCA1', fontsize=18)\n",
    "ax[1].set_ylabel('PCA2', fontsize=18)\n",
    "\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
