{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equivariant Graph Neural Networks\n",
    "\n",
    "- The molecules for this notebook were taken from the [refined QM09 dataset](https://doi.org/10.1038/s41597-019-0121-7) by H. Kim, J.Y. Park, and S. Choi, *Sci. Data*, **6**, 109 (2019).\n",
    "\n",
    "<img src=\"https://ehoogeboom.github.io/publication/egnn/featured_hua4419112e0b0f9c21e721be460820b18_120982_680x500_fill_q90_lanczos_center_2.png\" alt=\"Illustration of rotation equivariance\" aling=\"right\" style=\"width: 500px;float: right;\"/>\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook we will learn how to implement an [Equivariant Graph Neural Network](https://arxiv.org/abs/2102.09844), as reported by Satorras, Hoogeboom, and Welling. Their work explores three types of equivariance on a set of particles ${\\bf x}$, namely,\n",
    "\n",
    "* Translation equivariance, where translating the input results in an equivalent translation of the output.\n",
    "* Rotation/Reflection equivariance, where rotating/reflecting the input results in an equivalent rotation/reflection of the ouput.\n",
    "* Permutation equivariance, where permuting the input results in the same permutation of the output.\n",
    "\n",
    "As seen previously, we start by considering a graph of $n$ nodes $v_i \\in \\mathcal{V}$ and edges $e_{ij} \\in \\mathcal{E}$, as well as node features ${\\bf h} = (\\vec{h}_1, \\vec{h}_2, \\dots, \\vec{h}_n)$, and the set of coordinates ${\\bf x}_i$ associated with each of the graph nodes. An equivariant graph convolutional layer, EGCL, takes as input the set of node embeddings ${\\bf h}$, coordinate embeddings ${\\bf x}$ and edge information $\\mathcal{E} = (e_{ij})$, and outputs a transformation on ${\\bf h'}$ and ${\\bf x'}$. Concisely, ${\\bf h'}, {\\bf x'} = \\mathrm{EGCL}[{\\bf h}, {\\bf x}, \\mathcal{E}]$. The set of equations associated to an EGCL contains\n",
    "$$\n",
    "\\begin{aligned}\n",
    "{\\bf m}_{ij} &= \\phi_e \\left( {\\bf h}_{i}, {\\bf h}_{j}, \\left\\| {\\bf x}_{i} - {\\bf x}_{j} \\right\\|^{2}, e_{i j} \\right)\\, , \\\\\n",
    "{\\bf x'}_{i} &= {\\bf x}_{i} + C \\sum_{j \\neq i} \\left( {\\bf x}_{i} - {\\bf x}_{j} \\right) \\phi_x \\left( {\\bf m}_{ij} \\right)\\, , \\\\\n",
    "{\\bf m}_{i} &= \\sum_{j \\neq i} {\\bf m}_{ij}\\, , \\\\\n",
    "{\\bf h'}_{i} &= \\phi_h \\left( {\\bf h}_{i}, {\\bf m}_{i} \\right)\\, .\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Notice that the graph incorporates the relative squared distance between two coordinates $\\left\\| {\\bf x}_{i} - {\\bf x}_{j} \\right\\|^{2}$ into the **edge operation** $\\phi_e$. The node embeddings ${\\bf h}_i$ and ${\\bf h}_j$, and edge attributes $e_{ij}$ also are provided as input to that same edge operation. It is worth noting that the embeddings ${\\bf m}_{ij}$ can carry information from the whole graph and not only for a given edge $e_{ij}$.\n",
    "\n",
    "Next we update the position for each particle ${\\bf x}_i$ as a vector field in a radial direction. Here, the position ${\\bf x}_i$ is updated by the weighted sum of all the relative differences $({\\bf x}_i - {\\bf x}_j)_{\\forall j}$. The constant $C = 1/(n - 1)$. These weights then are multiplied to the output of the **coordinate operation** $\\phi_x$. This operation takes as input the edge embeddings ${\\bf m}_{ij}$. The overall operation results in the updated particle positions ${\\bf x'}_i$.\n",
    "\n",
    "These two operations are followed by an aggregation step that combines messages from all $j \\neq i$ nodes.\n",
    "\n",
    "Finally, the **node operation** $\\phi_h$ takes as input the node embeddings ${\\bf h}_i$ and the aggregated messages ${\\bf m}_{i}$ that result in the updated node embeddings ${\\bf h'}_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import numpy   as np\n",
    "import pandas  as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib             import cm\n",
    "from scipy                  import stats\n",
    "from pathlib                import Path\n",
    "from torch_geometric.loader import DataLoader\n",
    "#\n",
    "### Import local libraries\n",
    "#\n",
    "from model import EquivariantGraphNetwork\n",
    "from model import batches, passdata\n",
    "\n",
    "plt.rc('xtick', labelsize=18) \n",
    "plt.rc('ytick', labelsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare data\n",
    "\n",
    "Fisrt we need to load our dataset. We will use the `data.pth` that includes the nodes, edges, node features, edge features, and coordinates for all molecules. Our training data will be these descriptors and our target data will be the atomization energy for each molecule. The data is shuffled, hence we can directly divide our set into 60 % for training, 20 % testing, and the remaining 20 % for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "### Define data path\n",
    "#\n",
    "imhere  = Path.cwd()\n",
    "#\n",
    "### Load dataset\n",
    "#\n",
    "datapth = torch.load(imhere/'data.pth')\n",
    "\n",
    "#datapth = datapth[:20_000] # Reduce as needed from size = 132_723\n",
    "#\n",
    "### Divide into 60% training, 20% testing, and 20% validating\n",
    "#\n",
    "limit   = 40*len(datapth)//100\n",
    "\n",
    "print(f'train = { len(datapth[:-limit]) }, '\n",
    "      f'test = { len(datapth[-limit:-limit//2]) }, '\n",
    "      f'validate = { len(datapth[-limit//2:]) }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Settings and hyperparameters\n",
    "\n",
    "Our optimization algorithm is the ADAptive Moment estimation, [Adam](https://arxiv.org/pdf/1412.6980.pdf), that is based on stochastic gradient descent. We will need to define the **learning rate** and the **weight decay**. The learning rate is a hyperparameter that controls how much we are adjusting the weights of our network with respect to the loss gradient, whereas the weight decay is a regularization term that penalizes large weights\n",
    "\n",
    "The number of **epochs** is the number of times the learning algorithm will work through the entire training dataset. One epoch means that each sample in the training dataset has had an opportunity to update the internal model parameters.\n",
    "\n",
    "Because we are interested in learning the regression for a continuous variable, we will use the Mean Squared Error **loss function**.\n",
    "$$\n",
    "\\mathrm{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "where $N$ is the number of samples in the training set, $y_i$ is the reference value, and $\\hat{y}_i$ is predicted value.\n",
    "\n",
    "The training, testing, and validation data may be used in a loop function,\n",
    "\n",
    "~~~\n",
    "for batch in training: print(batch.shape)\n",
    "~~~\n",
    "\n",
    "each loop will automatically pass a sample to the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "### Training parameters\n",
    "#\n",
    "learnig_rate = 1e-3\n",
    "weight_decay = 1e-5\n",
    "\n",
    "epochs       = 1\n",
    "batch_size   = 10\n",
    "test_epoch   = 10\n",
    "#\n",
    "### Define neural network\n",
    "#\n",
    "network = EquivariantGraphNetwork(hidden_nf=4, activation=torch.nn.SiLU(), aggregation='sum')\n",
    "#\n",
    "### Optimizer and Loss\n",
    "#\n",
    "optimizer = torch.optim.Adam(params=network.parameters(), lr=learnig_rate, weight_decay=weight_decay)\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "#\n",
    "### Training and testing data\n",
    "#\n",
    "training   = DataLoader(datapth[:-limit],          shuffle=True,  batch_size=batch_size)\n",
    "testing    = DataLoader(datapth[-limit:-limit//2], shuffle=False, batch_size=batch_size)\n",
    "validating = DataLoader(datapth[-limit//2:],       shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training\n",
    "\n",
    "We can now train our neural network for the total `epochs` we selected and testing it every `test_epoch` epochs.\n",
    "\n",
    "Passing training data to the networks consists in five steps:\n",
    "\n",
    "1. Set the gradients to zero, `optimizer.zero_grad()`.\n",
    "\n",
    "2. Pass batch to the network, `output = network(batch)`.\n",
    "\n",
    "3. Compute the loss, `loss = criterion(output, y)`.\n",
    "\n",
    "4. Perform backward pass, `loss.backward()`.\n",
    "\n",
    "5. Perform the optimization step, `optimizer.step()`.\n",
    "\n",
    "Keep in mind that during testing you **DO NOT** want to update the gradients in your neural network. Otherwise you will leak testing information and your model will also learn from the testing set. To prevent this from happening, you need to use the `torch.no_grad()` context manager.  This will prevent the gradient from being updated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "\n",
    "    # your code here for the training set\n",
    "\n",
    "    print(f'{epoch+1},train,{loss:.4f}')\n",
    "\n",
    "    if (epoch+1)%test_epoch == 0:\n",
    "        # your code here for the testing set\n",
    "\n",
    "        print(f'{epoch+1},test,{loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test the model\n",
    "\n",
    "Use the validation set to compare the reference and predicted atomization energy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
