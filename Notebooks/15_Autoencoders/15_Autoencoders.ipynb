{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder\n",
    "\n",
    "- The molecules for this notebook are a subset of the [QM09 dataset](https://www.nature.com/articles/sdata201422) by R. Ramakrishnan1, P. O. Dral, M. Rupp, and O. A. von Lilienfeld *Sci. Data*, **1**, 140022 (2014).\n",
    "\n",
    "- A total of 17,834 small molecules with 18 atoms each were selected.\n",
    "\n",
    "<img src=\"https://www.mdpi.com/sensors/sensors-21-04294/article_deploy/html/images/sensors-21-04294-g003.png\" alt=\"Illustration of an Autoencoder\" align=\"center\" style=\"width: 500px;float: center;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## üß† What are autoencoders\n",
    "\n",
    "In this notebook we will learn how to implement an autoencoder. Autoencoders are a type of neural network designed to **learn efficient representations** of input data, typically for the purpose of **dimensionality reduction**, **feature learning**, or **denoising**. They consist of two main parts:\n",
    "\n",
    "- **Encoder:** Maps the input data to a lower-dimensional latent representation.\n",
    "- **Decoder:** Reconstructs the original data from the latent representation.\n",
    "\n",
    "The training objective is to minimize the difference between the input and its reconstruction (often using mean squared error)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Why Use Autoencoders?\n",
    "\n",
    "Autoencoders are useful in many scenarios, including:\n",
    "\n",
    "- **Dimensionality Reduction**  \n",
    "  Similar to PCA but capable of learning **nonlinear** transformations.\n",
    "\n",
    "- **Feature Learning**  \n",
    "  The latent space can capture abstract and task-relevant features that are useful for downstream models.\n",
    "\n",
    "- **Data Denoising**  \n",
    "  Denoising autoencoders can learn to reconstruct clean inputs from noisy versions, which is useful in preprocessing.\n",
    "\n",
    "- **Anomaly Detection**  \n",
    "  Autoencoders trained on normal data will struggle to reconstruct anomalies, leading to high reconstruction error ‚Äî a useful signal for outlier detection.\n",
    "\n",
    "- **Pretraining / Initialization**  \n",
    "  Autoencoders can initialize deep networks (unsupervised pretraining), though this is now less common with modern optimizers and architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Comparison to PCA\n",
    "\n",
    "| Feature              | PCA                     | Autoencoder                  |\n",
    "|---------------------|-------------------------|------------------------------|\n",
    "| Linear/Nonlinear    | Linear only             | Can be nonlinear             |\n",
    "| Interpretability    | High                    | Lower                        |\n",
    "| Dimensionality      | Fixed transformation    | Learnable transformation     |\n",
    "| Supervision         | Unsupervised            | Unsupervised                 |\n",
    "| Flexibility         | Rigid (matrix-based)    | Flexible (deep learning)     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib          import Path\n",
    "from torch.utils.data import DataLoader\n",
    "#\n",
    "### Import local libraries\n",
    "#\n",
    "from model import AutoEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare data\n",
    "\n",
    "Fisrt we need to load our dataset. We will use the `data_2d.pth` that includes the adjacency matrix for 17,834 small molecules. Our training data will be the adjacency matrix and the target will be the same matrix. For that, we will shuffle and dive our set into 80 % for training and the remaining 20 % for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define data files\n",
    "imhere  = Path.cwd()\n",
    "\n",
    "# Set file name for the network\n",
    "nnfile  = imhere/'network.pth'\n",
    "\n",
    "# Load dataset\n",
    "datapth = torch.load(imhere/'data_2d.pth')\n",
    "\n",
    "# Shuffle data\n",
    "idx     = torch.randperm(datapth.shape[0])\n",
    "datapth = datapth[idx]\n",
    "\n",
    "# Divide into 80% training and 20% testing\n",
    "limit   = 20*len(idx)//100\n",
    "\n",
    "print(f'train = {len(datapth[:-limit])}, '\n",
    "      f'test = {len(datapth[-limit:])}, '\n",
    "      f'test ratio = {len(datapth[-limit:])/len(datapth):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Settings and hyperparameters\n",
    "\n",
    "Our optimization algorithm is the ADAptive Moment estimation, [Adam](https://arxiv.org/pdf/1412.6980.pdf), that is based on stochastic gradient descent. We will need to define the **learning rate** and the **weight decay**. The learning rate is a hyperparameter that controls how much we are adjusting the weights of our network with respect to the loss gradient, whereas the weight decay is a regularization term that penalizes large weights\n",
    "\n",
    "The number of **epochs** is the number of times the learning algorithm will work through the entire training dataset. One epoch means that each sample in the training dataset has had an opportunity to update the internal model parameters.\n",
    "\n",
    "The **batch size** is a hyperparameter that defines the number of samples to work through before updating the internal model parameters. The batch size is a compromise between the speed and the quality of the learning. The larger the batch size, the faster the learning, but the less accurate the updates. The smaller the batch size, the slower the learning, but the more accurate the updates.\n",
    "\n",
    "Because the adjancency matrix contains only 0 and 1, we will use the Binary Cross Entropy **loss function**.\n",
    "$$\n",
    "\\text{BCE} = -\\frac{1}{N} \\sum_{i=1}^N \\left[ y_i \\log(\\hat{y}_i) + (1-y_i) \\log(1-\\hat{y}_i) \\right]\n",
    "$$\n",
    "where $N$ is the number of elements in the input tensor, $y_i$ is the target tensor, and $\\hat{y}_i$ is the input tensor. The loss function takes the input tensor and the target tensor as arguments and returns the loss. The loss is the average of the binary cross entropy loss of all elements in the input tensor.\n",
    "\n",
    "The training and testing data may be used in a loop function,\n",
    "\n",
    "~~~\n",
    "for batch in training: print(batch.shape)\n",
    "~~~\n",
    "\n",
    "each loop will automatically pass a total of `batch_size` samples to the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Training parameters\n",
    "\n",
    "learning_rate = 5e-3\n",
    "weight_decay = 0.0\n",
    "\n",
    "epochs       = 100\n",
    "batch_size   = 492 # 1, 2, 3, 4, 6, 12, 29, 41, 58, 82, 87, 116, 123, 164, 174, 246, 348, 492, 1189, 2378, 3567, 4756, 7134, 14268\n",
    "test_epoch   = 1\n",
    "\n",
    "### Define neural network\n",
    "network = AutoEncoder(input_nf = 18)\n",
    "\n",
    "### Optimizer and Loss function\n",
    "optimizer = torch.optim.Adam(params=network.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "criterion = torch.nn.BCELoss(reduction='mean')\n",
    "\n",
    "### Training and testing data\n",
    "\n",
    "training = DataLoader(datapth[:-limit], shuffle=True,  batch_size=batch_size)\n",
    "testing  = DataLoader(datapth[-limit:], shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training\n",
    "\n",
    "We can now train our neural network for the total `epochs` we selected and testing it every `test_epoch` epochs.\n",
    "\n",
    "Passing training data to the networks consists in five steps:\n",
    "\n",
    "1. Set the gradients to zero, `optimizer.zero_grad()`.\n",
    "\n",
    "2. Pass batch to the network, `output = network(batch)`.\n",
    "\n",
    "3. Compute the loss, `loss = criterion(output, y)`.\n",
    "\n",
    "4. Perform backward pass, `loss.backward()`.\n",
    "\n",
    "5. Perform the optimization step, `optimizer.step()`.\n",
    "\n",
    "Keep in mind that during testing you **DO NOT** want to update the gradients in your neural network. Otherwise you will leak testing information and your model will also learn from the testing set. To prevent this from happening, you need to use the `torch.no_grad()` context manager.  This will prevent the gradient from being updated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create arrays to store the training and testing loss for plotting\n",
    "training_loss_print = []\n",
    "testing_loss_print  = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    training_loss = []\n",
    "    testing_loss  = []\n",
    "    \n",
    "    # your code here for the training set\n",
    "    for batch in training:\n",
    "    \n",
    "    \n",
    "\n",
    "    # Average loss for the epoch, store in loss\n",
    "    loss = torch.mean(torch.stack(training_loss))\n",
    "\n",
    "    print(f'{epoch+1},train,{loss:.4f}')\n",
    "    \n",
    "    if (epoch+1)%test_epoch == 0:\n",
    "        # Save the training loss for plotting in a numpy array\n",
    "        training_loss_print.append(loss.item())\n",
    "\n",
    "        # your code here for the testing set\n",
    "        for batch in testing:\n",
    "            with torch.no_grad():\n",
    "                # Forward pass\n",
    "                output = network(batch)\n",
    "            # Compute loss\n",
    "            loss = criterion(output, batch)\n",
    "            # Store loss for this batch\n",
    "            testing_loss.append(loss)\n",
    "        \n",
    "        # Average loss for the epoch, store in loss\n",
    "        loss = torch.mean(torch.stack(testing_loss))\n",
    "        # Print loss\n",
    "        print(f'{epoch+1},test,{loss:.4f}')\n",
    "        # Save the testing loss for plotting using only the numeric value\n",
    "        # (not the tensor)\n",
    "        # Store loss for this batch\n",
    "        testing_loss_print.append(loss.item())\n",
    "\n",
    "# Save model\n",
    "torch.save(network.state_dict(), nnfile)\n",
    "\n",
    "# Plot the training and testing loss vs. epoch using a logscale for y\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.yscale('log')\n",
    "plt.plot(training_loss_print, label='Training Loss')\n",
    "plt.plot(testing_loss_print, label='Testing Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Testing Loss vs. Epoch')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test the model\n",
    "\n",
    "Choose a random sample from the test set to compare the original and reconstructed adjacency matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a random sample from the testing set to compare the original and reconstructed adjacency matrices.\n",
    "\n",
    "idx = torch.randint(0, len(datapth[-limit:]), (1,)).item()\n",
    "print(f'idx = {idx}')\n",
    "sample = datapth[-limit:][idx]\n",
    "sample = sample.unsqueeze(0)\n",
    "print(f'sample.shape = {sample.shape}')\n",
    "output = network(sample)\n",
    "\n",
    "# print the predicted and actual adjacency matrix\n",
    "print(f'Predicted adjacency matrix:\\n{output.squeeze().detach().numpy()}')\n",
    "print(f'Actual adjacency matrix:\\n{sample.squeeze().numpy()}')\n",
    "\n",
    "# Plot the predicted and actual adjacency matrix\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(output.squeeze().detach().numpy(), cmap='hot', interpolation='nearest')\n",
    "plt.title('Predicted')\n",
    "plt.colorbar()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(sample.squeeze().numpy(), cmap='hot', interpolation='nearest')\n",
    "plt.title('Actual')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the testing error and compare with the training error from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the testing error\n",
    "testing_loss = []\n",
    "for batch in testing:\n",
    "    with torch.no_grad():\n",
    "        # Forward pass\n",
    "        output = network(batch)\n",
    "    # Compute loss\n",
    "    loss = criterion(output, batch)\n",
    "    # Store loss for this batch\n",
    "    testing_loss.append(loss)\n",
    "\n",
    "# Average loss for the epoch, store in loss\n",
    "loss = torch.mean(torch.stack(testing_loss))\n",
    "print(f'Testing loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
