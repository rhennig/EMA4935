{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16. Convolutional Neural Networks (CNNs)\n",
    "\n",
    "## What are CNNs?\n",
    "\n",
    "**Convolutional Neural Networks (CNNs)** are a class of deep neural networks specifically designed for **image and spatial data**. Unlike fully connected networks, CNNs are capable of capturing spatial hierarchies in images using local receptive fields and shared weights. Instead of treating each pixel independently, CNNs leverage spatial structure and local context ‚Äî essential for recognizing patterns in images like edges, textures, and shapes.\n",
    "\n",
    "They are particularly effective in image-related tasks such as:\n",
    "\n",
    "- Object detection and recognition\n",
    "- Image classification\n",
    "- Semantic segmentation\n",
    "- Super-resolution and denoising\n",
    "\n",
    "In this notebook, we'll build a CNN to **identify nanoparticles in TEM images**.\n",
    "\n",
    "First, let‚Äôs understand what makes CNNs work.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Fully Connected Layers: Not Enough for Images\n",
    "\n",
    "Recall from our previous lecture:\n",
    "- Fully connected (dense) networks treat each input value independently.\n",
    "- A fully connected layer requires a weight for every pixel-to-neuron connection.\n",
    "- For a 256√ó256 grayscale image, that's **65,536 input** features!\n",
    "‚Äî This would lead to millions of parameters in even shallow networks.\n",
    "- For high-resolution TEM images, this quickly becomes computationally infeasible and ignores spatial patterns.\n",
    "\n",
    "CNNs solve this by:\n",
    "\n",
    "Using convolutional filters (kernels) that slide over the image.\n",
    "Extracting local patterns such as edges, corners, or textures.\n",
    "Sharing weights across spatial locations, which drastically reduces the number of parameters.\n",
    "\n",
    "üí° **Key idea**: Local features (edges, blobs, textures) matter more than global pixel-by-pixel connections.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts in CNNs\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*uk4KJEtyDuPOipfG4yd-WA.gif\" alt=\"Transposed convolution operation\" align=\"right\" style=\"width:300px; float:center\"/>\n",
    "\n",
    "#### 1. Convolutional Layers\n",
    "- Apply filters (e.g., 3√ó3, 5√ó5) to extract local features.\n",
    "- Output is called a feature map.\n",
    "- Learn filters that activate on visual patterns (edges, blobs, textures).\n",
    "\n",
    "#### 2. Activation Function\n",
    "- Usually ReLU: f(x)=max(0,x)\n",
    "- Introduces non-linearity into the model.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*1VJDP6qDY9-ExTuQVEOlVg.gif\" alt=\"Convolution operation\" align=\"right\" style=\"width:300px; float:center\"/>\n",
    "\n",
    "#### 3. Pooling Layers\n",
    "- Downsample feature maps to reduce spatial size.\n",
    "- Common types: Max Pooling, Average Pooling\n",
    "- Adds translation invariance and reduces computation.\n",
    "\n",
    "#### 4. Fully Connected (FC) Layers\n",
    "- Flatten the spatial features and connect to a standard dense layer.\n",
    "- Often appear near the output to make final predictions (e.g., classification scores).\n",
    "\n",
    "#### 5. Dropout, BatchNorm\n",
    "- Dropout: Prevents overfitting by randomly ‚Äúdropping‚Äù neurons during training.\n",
    "- Batch Normalization: Stabilizes and accelerates training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Architecture Example\n",
    "\n",
    "A basic CNN for image classification might look like this:\n",
    "\n",
    "![Conv Layer](https://upload.wikimedia.org/wikipedia/commons/6/63/Typical_cnn.png)\n",
    "(Source wikipedia)\n",
    "\n",
    "`Input (e.g., 1√ó64√ó64 TEM image)`\n",
    "\n",
    "‚Üì\n",
    "\n",
    "`Conv2D (32 filters, 3x3) + ReLU`\n",
    "\n",
    "‚Üì\n",
    "\n",
    "`MaxPool2D (2x2)`\n",
    "\n",
    "‚Üì\n",
    "\n",
    "`Conv2D (64 filters, 3x3) + ReLU`\n",
    "\n",
    "‚Üì\n",
    "\n",
    "`MaxPool2D (2x2)`\n",
    "\n",
    "‚Üì\n",
    "\n",
    "`Flatten`\n",
    "\n",
    "‚Üì\n",
    "\n",
    "`Fully Connected Layer + ReLU`\n",
    "\n",
    "‚Üì\n",
    "\n",
    "`Output Layer (e.g., Binary Classification: Nanoparticle / No Nanoparticle)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Core Components of a CNN\n",
    "\n",
    "### üîπ 1. Convolutional Layer\n",
    "\n",
    "Applies learnable filters to the image:\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "\n",
    "conv = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `in_channels=1` for grayscale TEM images.\n",
    "- `out_channels=32`: the number of filters we‚Äôll learn.\n",
    "- `kernel_size=3`: 3√ó3 filters are typical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîπ 2. Activation Function (ReLU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```relu = nn.ReLU()```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This helps the network learn complex functions and feature combinations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîπ 3. Pooling Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = nn.MaxPool2d(kernel_size=2, stride=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This:\n",
    "- Makes the model **invariant to small translations**\n",
    "- Reduces **memory/computation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîπ 4. Flattening & Fully Connected Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = nn.Flatten()\n",
    "fc = nn.Linear(in_features=..., out_features=2)  # For binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Architecture Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 16 * 16, 128),  # assuming 64x64 input\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 2)  # 2 classes: nanoparticle / no nanoparticle\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNNs for Image Segmentation\n",
    "\n",
    "Image segmentation is the process of assigning a **class label to each pixel** in an image. In binary segmentation, each pixel is labeled as either:\n",
    "\n",
    "- **Foreground** (e.g., nanoparticle)\n",
    "- **Background** (e.g., everything else)\n",
    "\n",
    "This task is different from classification (which outputs a single label) or object detection (which predicts bounding boxes). Unlike image classification (where the network outputs a single label), **image segmentation** requires the model to make a decision for **every pixel** ‚Äî is this pixel part of a nanoparticle or not?\n",
    "\n",
    "This is a type of **semantic segmentation**, where we produce a **binary mask** of the same size as the input image:\n",
    "- Input: Grayscale TEM image `[1, 128, 128]`\n",
    "- Output: Predicted mask `[1, 128, 128]` (after sigmoid + threshold)\n",
    "\n",
    "---\n",
    "\n",
    "## Why CNNs for Segmentation?\n",
    "\n",
    "Convolutional Neural Networks (CNNs) excel at **capturing spatial hierarchies** in images through their stacked layers of convolutions and pooling. However, traditional CNNs lose spatial resolution due to downsampling (e.g., MaxPooling), which is problematic for segmentation.\n",
    "\n",
    "To fix this, segmentation models use **encoder-decoder architectures**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Encoder-Decoder CNN Architecture\n",
    "\n",
    "This architecture consists of two parts:\n",
    "\n",
    "### üîπ Encoder\n",
    "- Learns feature representations at different scales.\n",
    "- Series of `Conv2d + ReLU + MaxPool` layers.\n",
    "- Gradually reduces spatial dimensions while increasing depth.\n",
    "\n",
    "### üîπ Decoder\n",
    "- Reconstructs the pixel-wise predictions.\n",
    "- Uses `ConvTranspose2d` (a.k.a. deconvolution or upsampling) layers.\n",
    "- Gradually increases spatial resolution back to the input size.\n",
    "\n",
    "### üîπ Final Output\n",
    "- A `1√óH√óW` mask where each value is a **logit**.\n",
    "- A **sigmoid** is applied during inference to get probabilities for each pixel.\n",
    "\n",
    "---\n",
    "\n",
    "## Visual Overview\n",
    "\n",
    "### Encoder-Decoder CNN\n",
    "\n",
    "![Encoder-Decoder Architecture](https://www.researchgate.net/publication/358442721/figure/fig2/AS:1121407341731840@1644375753000/U-Net-convolutional-neural-network-architecture.png)\n",
    "\n",
    "*Image Source: [Researchgate Article](https://www.researchgate.net/figure/U-Net-convolutional-neural-network-architecture_fig2_358442721)*\n",
    "\n",
    "> This shows how the input image is downsampled by the encoder, then upsampled by the decoder to produce a segmentation mask.\n",
    "\n",
    "---\n",
    "\n",
    "### Example Output\n",
    "\n",
    "| Input TEM Image | Ground Truth Mask | Model Prediction |\n",
    "|-----------------|-------------------|------------------|\n",
    "| ![input](https://i.imgur.com/xOZ5K4N.png) | ![gt](https://i.imgur.com/Wy3SZzi.png) | ![pred](https://i.imgur.com/OhjwTLT.png) |\n",
    "\n",
    "_(Replace with your own figures!)_\n",
    "\n",
    "---\n",
    "\n",
    "## Loss Function for Segmentation\n",
    "\n",
    "- **Binary Cross Entropy (BCEWithLogitsLoss)** is commonly used.\n",
    "- If class imbalance exists (many background pixels), use:\n",
    "  - `pos_weight` in BCE\n",
    "  - Dice Loss (focuses on overlap)\n",
    "  - Focal Loss (focuses on hard-to-classify pixels)\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "CNN segmentation with an encoder-decoder is:\n",
    "- Efficient for pixel-wise classification\n",
    "- Robust for sparse features like nanoparticles\n",
    "- Extendable: U-Net, DeepLab, SegNet are all built on this foundation\n",
    "\n",
    "---\n",
    "\n",
    "Ready to train your own segmentation model? Scroll down for code! üß™\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEM Images of Nanoparticles\n",
    "\n",
    "* The data for this notebook was obtained from J. P. Horwath, D. N. Zakharov, R. M√©gret, and E. A. Stach [npj Comput. Mater.](https://doi.org/10.1038/s41524-020-00363-x) **6**, 108 (2020).\n",
    "\n",
    "* A total of 15 transmission electron microscopy images were released as public data. Each with a resolution of $1920 \\times 1792$ pixels. The atuhors state that \"an ~1‚Äânm Au film was deposited by electron beam assisted deposition in Kurt J. Lesker PVD 75 vacuum deposition system to form nanoparticles with an approximate diameter of 5‚Äânm. The film was directly deposited onto DENSsolutions Wildfire series chips with SiN support suitable for in situ TEM heating experiments.\"\n",
    "\n",
    "## Today‚Äôs Objective\n",
    "\n",
    "Build a CNN that can:\n",
    "- Take grayscale TEM images as input\n",
    "- Learn to distinguish whether a pixel corresponds to a nanoparticle or not\n",
    "- Train on labeled data\n",
    "- Evaluate performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Import local libraries\n",
    "from model import CNN_Segmenter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare data\n",
    "\n",
    "First we need to prepare our data based on the images the authors made available. The set has a shape $15 \\times 1920 \\times 1792$. Since these are *large* images, we will need to divide each into multiple sub-images. We have several choices, such as\n",
    "\n",
    "* $201600$ images of size $16 \\times 16$,\n",
    "\n",
    "* $50400$ images of size $32\\times 32$,\n",
    "\n",
    "* $12600$ images of size $64\\times 64$,\n",
    "\n",
    "* $3150$ images of size $128 \\times 128$\n",
    "\n",
    "Since we are interested in distinguishing the deposited nano particles from the background, an image size of $128 \\times 128$ is a reasonable choice to prevent loosing the features of interest. With this in mind, we need to divide all our 15 images into 3150 sub-units, each with $128 \\times 128$ pixels. For the sake of simplicity, we will give the sets **X** and **y** the generic names `subimages` and `sublabels`, respectively.\n",
    "\n",
    "Write your choice of code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define local directory\n",
    "imhere = Path.cwd()\n",
    "\n",
    "# Load the data and labels\n",
    "images = np.load(imhere/\"images.npy\")\n",
    "labels = np.load(imhere/\"labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the shape of the data\n",
    "print(f\"images.shape: {images.shape}\")\n",
    "print(f\"labels.shape: {labels.shape}\")\n",
    "\n",
    "# Normalize image data\n",
    "\n",
    "\n",
    "# Visualize the first of the images and labels\n",
    "# Plot two images side by side\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the images into subimages of 128x128 pixels\n",
    "\n",
    "# Ensure the dimensions are divisible by 128\n",
    "assert images.shape[1] % 128 == 0 and images.shape[2] % 128 == 0, \"Image dimensions must be divisible by 128.\"\n",
    "\n",
    "# Reshape the images into subimages\n",
    "subimages = images.reshape(\n",
    "\timages.shape[0], \n",
    "\timages.shape[1] // 128, \n",
    "\t128, \n",
    "\timages.shape[2] // 128, \n",
    "\t128\n",
    ").transpose(0, 1, 3, 2, 4).reshape(-1, 128, 128)\n",
    "\n",
    "# Reshape the labels into sublabels\n",
    "sublabels = labels.reshape(\n",
    "\tlabels.shape[0], \n",
    "\tlabels.shape[1] // 128, \n",
    "\t128, \n",
    "\tlabels.shape[2] // 128, \n",
    "\t128\n",
    ").transpose(0, 1, 3, 2, 4).reshape(-1, 128, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding any further, we must verify that our data set actually has the shape `[3150, 128, 128]`. Both `images` and `labels` must share the same shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Verify the shape of the data\n",
    "\n",
    "\n",
    "# Visualize the first of the images and labels\n",
    "# Plot two images side by side\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a training set and a test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Split the data into training and test sets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Settings and hyperparameters\n",
    "\n",
    "Our optimization algorithm is the ADAptive Moment estimation, [Adam](https://arxiv.org/pdf/1412.6980.pdf), that is based on stochastic gradient descent.\n",
    "\n",
    "The number of **epochs** is the number of times the learning algorithm will work through the entire training dataset.\n",
    "\n",
    "The **batch size** is a hyperparameter that defines the number of samples to work through before updating the internal model parameters.\n",
    "\n",
    "We will use the Cross Entropy **loss function**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab0006b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import CNN_Segmenter\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Training parameters\n",
    "learnig_rate = 2e-3\n",
    "weight_decay = 0 # 1e-6\n",
    "\n",
    "epochs       = 20\n",
    "batch_size   = 150\n",
    "\n",
    "# Define neural network\n",
    "model = CNN_Segmenter()\n",
    "\n",
    "# Prepare training data\n",
    "X_train_tensor = torch.tensor(X_train).float().unsqueeze(1)\n",
    "y_train_tensor = torch.tensor(y_train).float().unsqueeze(1)\n",
    "train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Instantiate model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learnig_rate, weight_decay=weight_decay)\n",
    "criterion = torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor(10.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37eedb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2fcb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test data\n",
    "X_test_tensor = torch.tensor(X_test).float().unsqueeze(1)\n",
    "y_test_tensor = torch.tensor(y_test).float().unsqueeze(1)\n",
    "test_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=1)\n",
    "\n",
    "# Evaluate and visualize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Select five random samples from the test set\n",
    "    for i in range(5):\n",
    "        idx = np.random.randint(0, len(X_test_tensor))\n",
    "        xb = X_test_tensor[idx].unsqueeze(0)\n",
    "        yb = y_test_tensor[idx].unsqueeze(0)\n",
    "        pred_logits = model(xb)\n",
    "        preds = torch.sigmoid(pred_logits)\n",
    "        #preds = (preds > 0.5).float()\n",
    "    \n",
    "        fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "        axs[0].imshow(xb.squeeze(), cmap='gray')\n",
    "        axs[0].set_title(\"Input TEM\")\n",
    "        axs[1].imshow(yb.squeeze(), cmap='gray')\n",
    "        axs[1].set_title(\"Ground Truth\")\n",
    "        axs[2].imshow(preds.squeeze(), cmap='gray')\n",
    "        axs[2].set_title(\"Prediction\")\n",
    "        for ax in axs:\n",
    "            ax.axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        #break  # show just one example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot trained convolutional filters of the CNN_Segmenter\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i, filter in enumerate(model.encoder[0].weight.data):\n",
    "    plt.subplot(4, 8, i + 1)\n",
    "    plt.imshow(filter[0].detach().numpy(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Filter {i+1}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
