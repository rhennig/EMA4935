{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision trees\n",
    "\n",
    "Based on https://www.nbshare.io/notebook/312837011/Decision-Tree-Regression-With-Hyper-Parameter-Tuning-In-Python/.\n",
    "\n",
    "<img src=\"https://av-eks-blogoptimized.s3.amazonaws.com/416511.png\" alt=\"Illustration of a Decision tree\" aling=\"right\" style=\"width: 500px;float: right;\"/>\n",
    "\n",
    "A decision tree is a hierarchical model used in decision support based on conditional control statements. The tree structure is comprised of a root node, branches, internal nodes, and leaf nodes, forming a tree-like structure. These are consituted by\n",
    "\n",
    "* **Root node**: The initial node at the beginning of a decision tree, where the entire population or dataset starts dividing based on various features or conditions.\n",
    "\n",
    "* **Decision nodes**: Nodes resulting from the splitting of root nodes are known as decision nodes. These nodes represent intermediate decisions or conditions within the tree.\n",
    "\n",
    "* **Leaf Nodes/Terminal nodes**: Nodes where further splitting is not possible, often indicating the final classification or outcome.\n",
    "\n",
    "* **Branch/Sub-tree**: It represents a specific portion of the decision tree and outcomes within the tree.\n",
    "\n",
    "* **Pruning**: The process of removing or cutting down specific nodes in a decision tree to prevent overfitting and simplify the model.\n",
    "\n",
    "* **Parent** and **Child node**: In a decision tree, a node that is divided into sub-nodes is known as a parent node, and the sub-nodes emerging from it are called child nodes. The parent node represents a decision or condition, while the child nodes represent the potential outcomes or further decisions based on that condition.\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, we will use a decision tree model for regression of $({\\bf X}, {\\bf y})$ data to obtain a function $f({\\bf x})$ that best models the labels ${\\bf y}$. Then, we will use the analogous methodology but for classification, where we will divide the $({\\bf X}, {\\bf y})$ data into different categories.\n",
    "\n",
    "We had previously studied linear regression methods that provide a continuous model $f({\\bf x})$. In contrast, a decision trees learns by splitting the training samples in a way such that the sum of squared residuals is minimized. It then predicts the output value by taking the average of all of the examples that fall into a certain leaf on the decision tree and using that as the output prediction.\n",
    "\n",
    "   \n",
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn                 import datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree            import DecisionTreeRegressor\n",
    "from sklearn.tree            import DecisionTreeClassifier\n",
    "from sklearn.inspection      import DecisionBoundaryDisplay\n",
    "from sklearn.tree            import plot_tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics         import mean_squared_error\n",
    "\n",
    "plt.rc('xtick', labelsize=18)\n",
    "plt.rc('ytick', labelsize=18)\n",
    "\n",
    "blue   = '#0021A5'\n",
    "orange = '#FA4616'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Information theory\n",
    "\n",
    "Before training decision tree algorithms it is important that we understand how the method makes choices. We start by realizing that the algorithm must evaluate all variables using some statistical criteria and then choose the variable that performs best on the criteria. Such criterion is applied at each decision node and we typically use two methods, namely, the entropy and information gain, or the Gini index.\n",
    "\n",
    "Let's beging by discussing entropy and quantifying how much information there exists in a variable. For example, calculating the information for a random variable $X$ with probability distribution $P(X)$ is equivalent to calculating the information for the probability distribution of the events for the random variable. Its value is called **information entropy**, **Shannon entropy**, or simply **entropy**. It is related to the idea of entropy from physics by analogy, in that both are concerned with uncertainty.\n",
    "\n",
    "The intuition for entropy is that it is the average number of bits required to represent or transmit an event drawn from the probability distribution for the random variable. In general, the entropy for a random variable $X$ with $k \\in K$ discrete states is as follows:\n",
    "$$\n",
    "H(X) = - \\sum_{k \\in K} P(k) \\, \\log_2 \\left[ P(k) \\right].\n",
    "$$\n",
    "This equation should remind you of the **entropy of mixing for an ideal alloy**. The entropy is the negative of the sum of the probability of each event multiplied by the log of the probability of each event.\n",
    "\n",
    "Like information, the `log()` function uses base-2 and the units are bits. Here, the lowest entropy is calculated for a random variable that has a single event with a probability of 1.0, a certainty. The largest entropy for a random variable will be if all events are equally likely.\n",
    "\n",
    "We can consider a roll of a fair die and calculate the entropy for the variable. Each outcome has the same probability of 1/6, therefore it is a uniform probability distribution. We therefore would expect the average information to be the same information for a single event. We can further develop the intuition that low probability events have more information.\n",
    "\n",
    "To make this clear, we can calculate the information for probabilities between 0 and 1 and plot the corresponding information for each. We can then create a plot of probability vs information. We would expect the plot to curve downward from low probabilities with high information to high probabilities with low information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare probability vs information entropy\n",
    "plt.rc('xtick', labelsize=12) \n",
    "plt.rc('ytick', labelsize=12)\n",
    "\n",
    "# List of probabilities\n",
    "probabilities = np.linspace(0.1, 1.0, num=10)\n",
    "\n",
    "# Calculate information\n",
    "information = -np.log2(probabilities)\n",
    "\n",
    "# Plot probability vs information\n",
    "plt.figure( figsize=(8, 8) )\n",
    "\n",
    "plt.plot(probabilities, information, marker='o', c=blue, ms=8, ls='-', lw=2)\n",
    "\n",
    "plt.title('Information vs. Probability', fontsize=20)\n",
    "\n",
    "plt.xlabel('Probability', fontsize=18)\n",
    "plt.ylabel('Information', fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Low probability events are more surprising and carry more information, and the complement of high probability events carry less information. This relationship is not linear, it is in-fact slightly sub-linear. This makes sense given the use of the log function. \n",
    "\n",
    "We can further develop the intuition for entropy of probability distributions. Recall that entropy is the number of bits required to represent a randomly drawn even from the distribution, e.g., an average event. We can explore this for a simple distribution with two events, like a coin flip, but explore different probabilities for these two events and calculate the entropy for each. In the case where one event dominates, such as a skewed probability distribution, then there is less surprise and the distribution will have a lower entropy. In the case where no event dominates another, such as equal or approximately equal probability distribution, then we would expect larger or maximum entropy.\n",
    "\n",
    "- **Skewed Probability Distribution** (unsurprising): Low entropy.\n",
    "- **Balanced Probability Distribution** (surprising): High entropy.\n",
    "\n",
    "If we transition from skewed to equal probability of events in the distribution we would expect entropy to start low and increase, specifically from the lowest entropy of 0.0 for events with impossibility/certainty (probability of 0 and 1 respectively) to the largest entropy of 1.0 for events with equal probability. The example below implements this, creating each probability distribution in this transition, calculating the entropy for each and plotting the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(events, ets=1e-15):\n",
    "\treturn -np.sum( [p * np.log2(p + ets) for p in events] )\n",
    "\n",
    "# Define probabilities\n",
    "probabilities = np.linspace(0.0, 0.5, num=6)\n",
    "\n",
    "# Create probability distribution\n",
    "distributions = [ [p, 1.0 - p] for p in probabilities ]\n",
    "\n",
    "# Calculate entropy for each distribution\n",
    "entropies = [entropy(d) for d in distributions]\n",
    "\n",
    "plt.figure( figsize=(8, 8) )\n",
    "\n",
    "plt.plot(probabilities, entropies, marker='o', c=blue, ms=12, ls='-', lw=2)\n",
    "\n",
    "plt.title('Entropy vs. Probability Distribution', fontsize=20)\n",
    "\n",
    "plt.xticks(probabilities, [f'{d[0]:.1f} - {d[1]:.1f}' for d in distributions])\n",
    "\n",
    "plt.xlabel('Probability Distribution', fontsize=18)\n",
    "plt.ylabel('Entropy (bits)', fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example creates the 6 probability distributions from [0,1] to [0.5,0.5] probabilities. As expected, we see that as the distribution of events changes from skewed to balanced, the entropy increases from minimal to maximum values. That is, if the average event drawn from a probability distribution is not surprising we get a lower entropy, whereas if it is surprising, we get a larger entropy.\n",
    "\n",
    "We can see that the transition is not linear, that it is super linear. We can also see that this curve is symmetrical if we continued the transition to [0.6, 0.4] and onward to [1.0, 0.0] for the two events, forming an inverted parabola-shape. Note we had to add a tiny value to the probability when calculating the entropy to avoid calculating the log of a zero value, which would result in an infinity on not a number.\n",
    "\n",
    "Calculating the entropy for a random variable provides the basis for other measures such as **mutual information** (information gain). Entropy also provides the basis for calculating the difference between two probability distributions with **cross-entropy** and the **KL-divergence**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Decision trees for regression\n",
    "\n",
    "We can think of the decision tree learning as a divide and conquer strategy, using a search to identify the optimal split points within a tree. This process of splitting is then repeated in a top-down, recursive manner until all, or the majority of records have been classified. This classification is largely dependent on the complexity of the decision tree. Smaller trees are more easily able to attain pure leaf nodes, i.e., data points in a single class. However, as a tree grows in size, it becomes increasingly difficult to maintain this purity, and it usually results in too little data falling within a given subtree. When this occurs, it is known as data fragmentation, and it can often lead to overfitting. To better illustrate the behavior of decision trees, we will fit a simple one-dimensional function to the same data set that we previously used for linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the reference function that generates our data\n",
    "\n",
    "def reference_function(x):\n",
    "    return np.cos(x) + 2.0*np.sin(x) + 3.0*np.cos(2.0*x)\n",
    "\n",
    "np.random.seed(seed=5)\n",
    "\n",
    "# Generate a data set for machine learning\n",
    "x = np.linspace(0, 2, 300)\n",
    "x = x + np.random.normal(0.0, 0.3, x.shape)\n",
    "\n",
    "y = reference_function(x) + np.random.normal(0.0,1.0, x.shape)\n",
    "\n",
    "# Split the dataset into 80% for training and 20% for testing\n",
    "x = x.reshape( (-1,1) )\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, train_size=0.8, random_state=21)\n",
    "\n",
    "# Plot the training and testing dataset\n",
    "fig,ax=plt.subplots( figsize=(8,8) )\n",
    "\n",
    "ax.scatter(X_train, y_train, c=blue, label='Training')\n",
    "ax.scatter(X_test, y_test,   c=orange, label='Testing')\n",
    "\n",
    "ax.set_title('Training and testing data',fontsize=20)\n",
    "\n",
    "ax.set_xlabel('X Values',fontsize=18)\n",
    "ax.set_ylabel(r'$ \\cos(x)+2\\sin(x)+3\\cos(2x)$',fontsize=18)\n",
    "\n",
    "plt.legend(loc='best', fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Decision tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the decision tree regression to the dataset\n",
    "decision_tree_regression = DecisionTreeRegressor()\n",
    "decision_tree_regression.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred_from_training = decision_tree_regression.predict(X_train)\n",
    "y_pred_from_testing  = decision_tree_regression.predict(X_test)\n",
    "\n",
    "training_rmse = np.sqrt( mean_squared_error(y_train, y_pred_from_training) )\n",
    "testing_rmse  = np.sqrt( mean_squared_error(y_test, y_pred_from_testing) )\n",
    "\n",
    "# Report results\n",
    "print(f'Training score = '\n",
    "      f'{decision_tree_regression.score(X_train,y_train):6.3f} '\n",
    "      f'with RMSE = {training_rmse:6.3f}')\n",
    "\n",
    "print(f'Testing  score = '\n",
    "      f'{decision_tree_regression.score(X_test,y_test):6.3f} '\n",
    "      f'with RMSE = {testing_rmse:6.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a perfect high score on the training data and a zero RMSE but only a moderately high score on the testing data and a sizable RMSE. This demonstrate extreme overfitting. Let us visualize the model and data to see the results.\n",
    "\n",
    "Let's have a closer look at our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a series of sampling points to plot the model\n",
    "points  = 1000\n",
    "\n",
    "X_model = np.linspace(np.min(x), np.max(x), num=points)\n",
    "X_model = X_model.reshape( (-1,1) )\n",
    "\n",
    "y_model_predictions = decision_tree_regression.predict(X_model)\n",
    "y_model_reference   = reference_function(X_model)\n",
    "\n",
    "# Plot the dataset\n",
    "fig,ax=plt.subplots( figsize=(16,8) )\n",
    "\n",
    "ax.scatter(X_train, y_train, c=blue, label='Training')\n",
    "ax.scatter(X_test, y_test, c=orange, label='Testing')\n",
    "\n",
    "ax.plot(X_model, y_model_predictions, c=blue, lw=2, label='Model')\n",
    "ax.plot(X_model, y_model_reference,   c='k', lw=4, label='Reference')\n",
    "\n",
    "ax.set_title('Performance', fontsize=20)\n",
    "\n",
    "ax.set_xlabel('x values', fontsize=18)\n",
    "ax.set_ylabel('y values', fontsize=18)\n",
    "\n",
    "ax.legend(loc='best', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows an important issue commonly asssciated with decision tree regression. The resulting model is **piecewise constant**.\n",
    "\n",
    "We also observe that there is a strong tendency to overfit. In fact,the model has **memorized all the training data**.\n",
    "\n",
    "Let's check the set of predicted and reference ${\\bf y}$ values using a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots( figsize=(8,8) )\n",
    "\n",
    "ax.scatter(y_test, y_pred_from_testing, c=orange, label='Testing')\n",
    "ax.scatter(y_train, y_pred_from_training, c=blue, label='Training')\n",
    "\n",
    "ax.set_xlabel('Reference', fontsize=18)\n",
    "ax.set_ylabel('Prediction', fontsize=18)\n",
    "\n",
    "ax.legend(loc='best', fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Hyperparameter Optimization with Cross-Validation\n",
    "\n",
    "To address this extreme overfitting, we must optimize the hyperparameters in the decision tree regression.\n",
    "\n",
    "Here are some of the hyperparameters for decision trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the hyperparameters that can be tuned\n",
    "for idx, key in enumerate( decision_tree_regression.get_params().keys() ):\n",
    "    print(f'({idx+1:2d}): {key}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- `criterion` is used to measure the quality of a split. Its value is `'squared_error'`.\n",
    "\n",
    "- `max_depth` controls the depth of the tree. It does not make any calculations on our training set. The model stops splitting when max_depth is reached. Note that `max_depth` is less flexible compared to `min_impurity_decrease`.\n",
    "\n",
    "- `min_impurity_decrease`. When the algorithm performs a split, the main goal is to decrease impurity as much as possible. The more the impurity decreases, the more informative power that split gains. As the tree gets deeper, the amount of impurity decrease becomes lower. We can use this to prevent the tree from doing further splits. Its default is zero.\n",
    "\n",
    "- `min_samples_split` allows control of the tree based on impurity values by setting a threshold on the Gini index. If the algorithm keeps splitting nodes, the model will probably be overfit. \n",
    "\n",
    "- `min_samples_leaf` indicates the minimum number of samples required to be at a leaf node.\n",
    "\n",
    "- `max_leaf_nodes` limits the number of leaf nodes, which grows the tree in best-first fashion until max_leaf_nodes reached. The best split is decided based on impurity decrease.\n",
    "\n",
    "- `max_features` sets the number of features to consider when looking for the best split. If not specified, the model considers all of the features. There is only 1 feature in our dataset.\n",
    "\n",
    "In cell [2.1](###-2.1-Decision-tree-model) above, set the hyperparameter:\n",
    "\n",
    "~~~\n",
    "decision_tree_regression = DecisionTreeRegressor( hyperparameter = value )\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Grid Search for Optimal Hyperparameters\n",
    "\n",
    "Instead of optimizing hyperparameters one by one, we will use a grid search for the optimization of some of the hyperparameters of the decision tree model with cross-validation. The optimal values of hyperparameters depend on each other. The grid search varies all the parameters together, which ensures that we obtain a somewhat optimal model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters range to explore\n",
    "\n",
    "parameters = {\n",
    "    'min_impurity_decrease' : [0, 0.05, 0.1, 0.2],\n",
    "    'min_samples_split'     : [0.05, 0.1, 0.2],\n",
    "    'max_depth'             : [2, 3, 4, 5],\n",
    "    'min_samples_leaf'      : [1, 2, 3, 4, 5],\n",
    "    'max_leaf_nodes'        : [None, 2, 4, 8, 12]\n",
    "    }\n",
    "\n",
    "# Create the grid search object from sklearn\n",
    "grid_search = GridSearchCV(decision_tree_regression,\n",
    "                           param_grid=parameters,\n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           cv=5, verbose=0)\n",
    "\n",
    "# Fit the grid search object to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Optimized hyperparameters \n",
    "optimum_parameters = grid_search.best_params_\n",
    "\n",
    "# Report the optimized hyperparameters\n",
    "for key in optimum_parameters:\n",
    "    print(f'{key:21s} = {optimum_parameters[key]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know the set of hyperparameters that optimize the performance of the decision tree regression, we can use them to train a new model and compare its performance with the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the optimized decision tree regression to the dataset\n",
    "optimized_decision_tree_regression = DecisionTreeRegressor(**optimum_parameters)\n",
    "optimized_decision_tree_regression.fit(X_train, y_train)\n",
    "\n",
    "y_pred_from_training = optimized_decision_tree_regression.predict(X_train)\n",
    "y_pred_from_testing  = optimized_decision_tree_regression.predict(X_test)\n",
    "\n",
    "training_rmse = np.sqrt( mean_squared_error(y_train, y_pred_from_training) )\n",
    "testing_rmse  = np.sqrt( mean_squared_error(y_test, y_pred_from_testing) )\n",
    "    \n",
    "# Report results\n",
    "print(f'Training score = '\n",
    "      f'{optimized_decision_tree_regression.score(X_train,y_train):6.3f} '\n",
    "      f'with RMSE = {training_rmse:6.3f}')\n",
    "\n",
    "print(f'Testing  score = '\n",
    "      f'{optimized_decision_tree_regression.score(X_test,y_test):6.3f} '\n",
    "      f'with RMSE = {testing_rmse:6.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimized model does significantly better.\n",
    "- It predicts similar training and testing errors.\n",
    "- The testing error is significantly reduced compared to the unoptimized model.\n",
    "\n",
    "Let's plot our final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a series of sampling points to plot the model\n",
    "points  = 1000\n",
    "\n",
    "X_model = np.linspace(np.min(x), np.max(x), num=points)\n",
    "X_model = X_model.reshape( (-1,1) )\n",
    "\n",
    "y_model_predictions = optimized_decision_tree_regression.predict(X_model)\n",
    "y_model_reference   = reference_function(X_model)\n",
    "\n",
    "# Plot the dataset\n",
    "fig,ax=plt.subplots( figsize=(16,8) )\n",
    "\n",
    "ax.scatter(X_train, y_train, c=blue, label='Data')\n",
    "ax.scatter(X_test, y_test, c=orange, label='Testing')\n",
    "\n",
    "ax.plot(X_model, y_model_predictions, c=blue, lw=2, label='Model')\n",
    "ax.plot(X_model, y_model_reference,   c='k', lw=4, label='Reference')\n",
    "\n",
    "ax.set_title('Performance', fontsize=20)\n",
    "\n",
    "ax.set_xlabel('x values', fontsize=18)\n",
    "ax.set_ylabel('y values', fontsize=18)\n",
    "\n",
    "ax.legend(loc='best', fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots( figsize=(8,8) )\n",
    "\n",
    "ax.scatter(y_test, y_pred_from_testing, c=orange, label='Testing')\n",
    "ax.scatter(y_train, y_pred_from_training, c=blue, label='Training')\n",
    "\n",
    "ax.set_xlabel('Reference', fontsize=18)\n",
    "ax.set_ylabel('Prediction', fontsize=18)\n",
    "\n",
    "ax.legend(loc='best', fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24,8))\n",
    "\n",
    "plot_tree(optimized_decision_tree_regression, filled=True, fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Decision trees for classification\n",
    "\n",
    "As already stated, decision trees can also be used to classify our data into different categories. To illustrate this, we will use the same set of insulators, conductors, and semiconductors that we used in the previous chapter. The data is shown in the following figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.load_iris()\n",
    "\n",
    "x = data.data[:,:2]\n",
    "y = data.target\n",
    "\n",
    "# Plot data\n",
    "\n",
    "fig, ax = plt.subplots( figsize=(8,8) )\n",
    "\n",
    "scatter = plt.scatter(x[:,0], x[:,1], c=y, label=y, cmap=plt.cm.viridis, edgecolors='k')\n",
    "\n",
    "plt.title('Clustered data', fontsize=20)\n",
    "\n",
    "plt.xlabel(r'$x_0$', fontsize=18)\n",
    "plt.ylabel(r'$x_1$', fontsize=18)\n",
    "\n",
    "handles, _ = scatter.legend_elements()\n",
    "\n",
    "plt.legend(handles=handles, labels=['Insulator', 'Conductor', 'Semiconductor'], loc='best', fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Decision tree model\n",
    "\n",
    "We often use the so-called Gini index for classification models. It is computed through the set of probabilities $p_i$\n",
    "$$\n",
    "C_\\mathrm{Gini} = 1 - \\sum_{i=1}^n (p_i)^2.\n",
    "$$\n",
    "\n",
    "In simple words, this index meassures how often a randomly chosen element would be identified incorrectly. This results in giving preference to attributes with a lower Gini index.\n",
    "\n",
    "We can now fit a decision tree classifier to the data and visualize the decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_classification = DecisionTreeClassifier()\n",
    "decision_tree_classification.fit(x, y)\n",
    "\n",
    "accuracy = decision_tree_classification.score(x, y)\n",
    "\n",
    "fig, ax = plt.subplots( figsize=(8,8) )\n",
    "\n",
    "DecisionBoundaryDisplay.from_estimator(decision_tree_classification, x,\n",
    "                                       ax=ax, cmap=plt.cm.viridis, response_method='predict', alpha=0.3)\n",
    "\n",
    "scatter = plt.scatter(x[:,0], x[:,1], c=y, label=y, cmap=plt.cm.viridis, edgecolors='k')\n",
    "\n",
    "plt.title('Clustered data', fontsize=20)\n",
    "\n",
    "plt.xlabel(r'$x_0$', fontsize=18)\n",
    "plt.ylabel(r'$x_1$', fontsize=18)\n",
    "\n",
    "handles, _ = scatter.legend_elements()\n",
    "\n",
    "plt.text(0.05, 0.90, f'Accuracy = {100.0*accuracy:.2f} %', transform=ax.transAxes, fontsize=18)\n",
    "\n",
    "plt.legend(handles=handles, labels=['Insulator', 'Conductor', 'Semiconductor'], loc='lower right', fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear from the previous plot that the decision tree also overfits the data. We can use the same approach to optimize the hyperparameters of the decision tree classifier.\n",
    "\n",
    "### 3.2 Hyperparameter optimization with Cross-Validation\n",
    "\n",
    "First let's print all the options available for this class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the hyperparameters that can be tuned\n",
    "for idx, key in enumerate( decision_tree_classification.get_params().keys() ):\n",
    "    print(f'({idx+1:2d}): {key}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find the same options as for the Decision Tree Regression, but there is one additional hyperparameter that we can tune, namely, `class_weight`. This hyperparameter allows us to assign different weights to the classes. This is useful when we have an unbalanced dataset. For example, if we have 70 samples of insulators, 20 samples of conductors, and 10 samples of semiconductors, we can assign a weight of 7, 2, and 1 to insulators, conductors, and samples, respectively. This, however, will make the model pay more attention to class the insulators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Grid search for optimal hyperparemeters\n",
    "\n",
    "It is evident that we need to optimize the set of hyperparameters to generalize the applicability of our model. We can use the grid search method for such purpose.\n",
    "\n",
    "> ### Assignment\n",
    ">\n",
    "> Perform a grid search to optimize the hyperparameters, and print the resulting classification regions afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Summary\n",
    "\n",
    "Decision trees are white box machine learning algorithms that are useful for classification and regression. Some of the advantages of decision trees are:\n",
    "\n",
    "- Easy to understand and interpret\n",
    "- Can handle both numerical and categorical data\n",
    "- Requires little or no preprocessing such as normalization or dummy encoding\n",
    "\n",
    "On the downside, decision trees are prone to overfitting. They can easily become overly complex, which prevents them from generalizing well. It is therfore essential to optimize the hyperparameters for decision trees using cross-validation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
