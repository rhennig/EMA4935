{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Underfitting vs. Overfitting in Polynomial Regression\n",
    "\n",
    "Source: https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html\n",
    "\n",
    "## Overview\n",
    "\n",
    "This example demonstrates the problems of underfitting and overfitting and how we can use linear regression with polynomial features to approximate nonlinear functions. The plot shows the function that we want to approximate, which is a part of the cosine function. In addition, the samples from the real function and the approximations of different models are displayed. The models have polynomial features of different degrees. We can see that a linear function (polynomial with degree 1) is not sufficient to fit the training samples. This is called **underfitting**. A polynomial of degree 4 approximates the true function almost perfectly. However, for higher degrees the model will **overfit** the training data, i.e. it learns the noise of the training data.\n",
    "We evaluate quantitatively **overfitting** / **underfitting** by using cross-validation. We calculate the mean squared error (MSE) on the validation set, the higher, the less likely the model generalizes correctly from the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline        import Pipeline\n",
    "from sklearn.linear_model    import LinearRegression\n",
    "from sklearn.preprocessing   import PolynomialFeatures\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics         import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Polynomial regression\n",
    "\n",
    "Polynomial actually is a form of linear regression, because the resulting model is linear in the parameters, $\\beta_i$. The difference is that we change the features into polynomials of the input data $x_i$,\n",
    "$$\n",
    "\\hat y_i = \\beta_0 + \\beta_1\\, x_i + \\beta_2\\, x_i^2 + \\beta_3\\, x_i^3 + \\dots\n",
    "$$\n",
    "or in matrix form\n",
    "$$\n",
    "\\begin{bmatrix} \\hat y_1 \\\\ \\hat y_2 \\\\ \\vdots \\\\ \\hat y_n \\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "   1 & x_1 & x_1^2 & x_1^3 & \\dots \\\\\n",
    "   1 & x_2 & x_2^2 & x_2^3 & \\dots \\\\\n",
    "   \\vdots \\\\\n",
    "   1 & x_n & x_n^2 & x_n^3 & \\dots \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "   \\beta_0 \\\\\n",
    "   \\beta_1 \\\\\n",
    "   \\beta_2 \\\\\n",
    "   \\beta_3 \\\\\n",
    "   \\vdots\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "$$\n",
    "\\hat{\\bf y} = {\\bf X}\\, \\beta.\n",
    "$$\n",
    "\n",
    "The coefficients, $\\beta_i$, that minimize the cost function are given by the mean-squared error, MSE, and fullfill the normal equation:\n",
    "$$\n",
    "{\\bf X}^\\mathrm{T} {\\bf X}\\, \\beta = {\\bf X}^\\mathrm{T}\\, {\\bf y}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a module to define the reference function used to generate the data\n",
    "def reference_function(X):\n",
    "    return np.cos(1.5 * np.pi * X)\n",
    "\n",
    "# Define seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Define the number of samples and degrees for the polynomial\n",
    "number_of_samples = 30\n",
    "polynomial_degree = np.array([1, 4, 15])\n",
    "\n",
    "# Generate random data for fitting\n",
    "X = np.sort( np.random.rand(number_of_samples) )\n",
    "y = reference_function(X) + 0.1*np.random.randn(number_of_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure( figsize=(14,5) )\n",
    "\n",
    "for i in range(polynomial_degree.size):\n",
    "\n",
    "    ax = plt.subplot(1, polynomial_degree.size, i+1)\n",
    "\n",
    "    plt.setp( ax, xticks=(), yticks=() )\n",
    "\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "\n",
    "    plt.xlim( ( 0, 1) )\n",
    "    plt.ylim( (-2, 2) )\n",
    "\n",
    "    polynomial_features = PolynomialFeatures(degree=polynomial_degree[i], include_bias=False)\n",
    "    linear_regression   = LinearRegression()\n",
    "\n",
    "    pipeline = Pipeline(\n",
    "        [\n",
    "            (\"polynomial_features\", polynomial_features),\n",
    "            (\"linear_regression\", linear_regression),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    pipeline.fit(X[:, np.newaxis], y)\n",
    "\n",
    "    # Evaluate the models using cross-validation\n",
    "    scores = cross_val_score(\n",
    "        pipeline, X[:, np.newaxis], y, scoring=\"neg_mean_squared_error\", cv=10\n",
    "    )\n",
    "\n",
    "    X_test = np.linspace(0, 1, 100)\n",
    "\n",
    "    plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label=\"Model\")\n",
    "\n",
    "    plt.plot(X_test, reference_function(X_test), label=\"True function\")\n",
    "\n",
    "    plt.scatter(X, y, edgecolor=\"b\", s=20, label=\"Samples\")\n",
    "\n",
    "    plt.title(f\"Degree {polynomial_degree[i]}\\n\"\n",
    "              f\"MSE = {-scores.mean():.2e} (Â± {scores.std():.2e})\")\n",
    "    \n",
    "    plt.legend(loc=\"best\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. k-Fold cross validation\n",
    "\n",
    "### Training and Testing Data\n",
    "- Generally, we split data into training and testing sets.\n",
    "- The training set is used to optimize the parameters of the model.\n",
    "- The testing data is used to estimate the accuracy of the model on unseen data. \n",
    "\n",
    "### Validation Data\n",
    "- Some models do not have sufficient flexibility to approximate the training data. These models have a high bias. They often perform similarly in the training and testing data.\n",
    "\n",
    "- Sometimes models train well and have small loss on the training data, but perform poorly when presented with new data outside the training set. In this case the model has small bias but a large variance. This is known as overfitting to the training data.\n",
    "\n",
    "- Many machine learning models have parameters that control their flexibility. An example is polynomial regression, where the order of the polynomial degree controls the flexibility and, hence, bias of the model.\n",
    "\n",
    "- However, to decide what level of complexity is optimal for the machine learning model, we cannot use just the training data alone. We also cannot use the testing data as this would compromise its independence for estimating the accuracy of the model.\n",
    "\n",
    "- Instead, we split data into a training and validation sets, where the training set is used to optimize the parameters of the model and the validation set then is used to compare between different models.\n",
    "\n",
    "- In particular, k-fold cross validation is a technique for model selection where the training data set is divided into $k$ equal groups. The first group is considered as the validation set and the rest $k-1$ groups as training data, and the model is fit on it. This process is iteratively repeated $k-1$ times. Each time the $k^\\mathrm{th}$ group will be selected as validation and the remaining $k-1$ groups be used for optimizing the model parameters, $\\beta$. In each iteration, the validation MSE is calculated and the final MSE after $k$ iterations the Cross-Validation MSE is given as the average:\n",
    "$$\n",
    "\\text{CV}_k = \\frac{1}{N_k} \\sum_{i=1}^k \\text{MSE}_i\n",
    "$$\n",
    "This validation MSE is the estimate for our test data MSE.\n",
    "\n",
    "<img src=\"https://github.com/rhennig/EMA6938/blob/main/Notebooks/Figures/Cross-Validation.png?raw=1\" alt=\"Validation folds\" align=\"center\" style=\"width:400px; float:center\"/>\n",
    "\n",
    "Importantly, **cross-validation is for model selection or hyperparameter optimization** and utilizes the training data. To measure the **performance of our model**, we need to apply it to data that was not used in the optimization of the hyperparameters or model parameters. Usually, we keep a fraction of 20% of the data aside as a **holdout test set** and use 80% of the data for the training set used in cross-validations. We measure the model performance of the optimized model on the holdout test set.\n",
    "\n",
    "Let's use the `KFold` object from the scikit-learn library to illustrate the indexes for a dataset are split into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a generic dataset\n",
    "data_size = 100\n",
    "k_folds   = 10\n",
    "\n",
    "idx         = np.arange(data_size)\n",
    "data_series = np.ones( (k_folds, data_size), dtype=float)\n",
    "\n",
    "# Create the KFold object using 10 splits\n",
    "k_folding = KFold(n_splits=k_folds)\n",
    "\n",
    "for fold, (train, test) in enumerate( k_folding.split(idx) ):\n",
    "    data_series[fold, test] = 0.0\n",
    "\n",
    "# Plot the cross-validation indices for each fold\n",
    "fig, ax = plt.subplots( figsize=(8,8) )\n",
    "\n",
    "plt.rc('xtick', labelsize=18) \n",
    "plt.rc('ytick', labelsize=18)\n",
    "\n",
    "ax.set_xlabel('Elements',fontsize=18)\n",
    "ax.set_ylabel('Fold index',fontsize=18)\n",
    "\n",
    "plt.imshow(data_series, aspect='auto', cmap='Paired')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 k-fold cross-validation in machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define seed for reproducibility\n",
    "np.random.seed(seed=0)\n",
    "\n",
    "def reference_model(x, noise=False):\n",
    "\n",
    "    y = np.cos(x) + 2.0*np.sin(x) + 3.0*np.cos(x*2)\n",
    "\n",
    "    if noise:\n",
    "        y = y + np.random.normal(0.0, 1.0, x.shape)\n",
    "\n",
    "    return y\n",
    "\n",
    "# Generate a reference data set without noise\n",
    "x_ref = np.linspace(-0.3, 2.3, num=300)\n",
    "y_ref = reference_model(x_ref, noise=False)\n",
    "\n",
    "# Generate a data set with random noise\n",
    "x = np.linspace(0.0, 2.0, num=300)\n",
    "x = x + np.random.normal(0, 0.3, x.shape)\n",
    "\n",
    "y = reference_model(x, noise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the  data set and the reference model\n",
    "\n",
    "plt.figure( figsize=(8,6) )\n",
    "\n",
    "plt.plot(x_ref, y_ref, color=\"orange\", label=\"Reference model\")\n",
    "plt.scatter(x, y, edgecolor=\"b\", s=10, label=\"Samples\")\n",
    "\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into 80% for training and 20% for testing\n",
    "x = x.reshape( (x.size,1) )\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Select hyperparameters\n",
    "\n",
    "First we need to evaluate the appropriate degree for the polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the maximum degree of interest\n",
    "polynomial_degrees = np.arange(start=1, stop=12, step=1)\n",
    "\n",
    "# Define number of folds\n",
    "number_of_folds    = 5\n",
    "\n",
    "# Initialize empty lists to store the training and cross-validation errors\n",
    "training_error_per_degree         = []\n",
    "cross_validation_error_per_degree = []\n",
    "\n",
    "for degree in polynomial_degrees:\n",
    "\n",
    "    # Polynomial feature creation\n",
    "    X = PolynomialFeatures(degree=degree).fit_transform(x_train)\n",
    "\n",
    "    # Create regression model\n",
    "    linear_regression = LinearRegression(fit_intercept=False)\n",
    "\n",
    "    # Fit the model to the training data\n",
    "    polynomial_model  = linear_regression.fit(X, y_train)\n",
    "\n",
    "    y_train_pred      = polynomial_model.predict(X)\n",
    "\n",
    "    training_mse      = mean_squared_error(y_train,y_train_pred)\n",
    "\n",
    "    training_error_per_degree.append(training_mse)\n",
    "\n",
    "    # Calculate the cross-validation MSE\n",
    "    cross_validation_error = cross_validate(linear_regression, X, y_train,\n",
    "                                            scoring='neg_mean_squared_error',\n",
    "                                            cv=number_of_folds, return_train_score=True)\n",
    "\n",
    "    mean_cross_validation_error = np.mean( np.absolute(cross_validation_error['test_score']) )\n",
    "\n",
    "    cross_validation_error_per_degree.append(mean_cross_validation_error)\n",
    "\n",
    "fig,ax=plt.subplots( figsize=(8,8) )\n",
    "\n",
    "ax.set_xlabel('Degree', fontsize=18)\n",
    "ax.set_ylabel('MSE',    fontsize=18)\n",
    "\n",
    "ax.set_title('Hyperparameter optimization', fontsize=20)\n",
    "\n",
    "\n",
    "ax.plot(polynomial_degrees, cross_validation_error_per_degree, label=\"CV Error\")\n",
    "ax.plot(polynomial_degrees, training_error_per_degree, label=\"Training Error\")\n",
    "\n",
    "ax.legend(loc='best', fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Validation with the holdout method\n",
    "\n",
    "Now we use the best degree found in the previous section, then we train the model with the training set and validate it with the holdout set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the optimal hyperparameter\n",
    "degree = np.argmin(cross_validation_error_per_degree) + 1\n",
    "print(f\"Optimal degree of polynomial regression = {degree}\")\n",
    "\n",
    "# Polynomial feature creation\n",
    "X = PolynomialFeatures(degree=degree).fit_transform(x_train)\n",
    "\n",
    "# Create regression model\n",
    "linear_regression = LinearRegression(fit_intercept=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fit the model to the training data\n",
    "polynomial_model  = linear_regression.fit(X, y_train)\n",
    "\n",
    "# Measure performance of optimal model on holdout dataset\n",
    "X_test      = PolynomialFeatures(degree).fit_transform(x_test)\n",
    "\n",
    "predicted_y = polynomial_model.predict(X_test)\n",
    "\n",
    "mse_test    = mean_squared_error(y_test, predicted_y)\n",
    "\n",
    "print(f\"MSE on holdout set = {mse_test:.2f}\")\n",
    "\n",
    "# Calculate optimal polynomial regression\n",
    "x_regression   = np.linspace(np.min(x), np.max(x), num=100)\n",
    "x_regression_T = np.array([x_regression]).T\n",
    "X_regression   = PolynomialFeatures(degree).fit_transform(x_regression_T)\n",
    "y_model_pred   = linear_regression.predict(X_regression)\n",
    "\n",
    "# Reference model\n",
    "y_reference    = reference_model(x_regression, noise=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the whole dataset\n",
    "fig, ax = plt.subplots( figsize=(8,8) )\n",
    "\n",
    "ax.set_xlabel('x Values',   fontsize=18)\n",
    "ax.set_ylabel('y Values',   fontsize=18)\n",
    "\n",
    "ax.set_title('Performance', fontsize=20)\n",
    "\n",
    "ax.scatter(x, y, label='Data')\n",
    "\n",
    "ax.plot(x_regression, y_model_pred, color='b', label='Fit', lw=2)\n",
    "ax.plot(x_regression, y_reference,  color='k', label='Ref', lw=2)\n",
    "\n",
    "ax.legend(loc='best', fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Assignment\n",
    ">\n",
    "> Visualize what happens when you set the degree of the polynomial to a one or a large number?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Underfit model\n",
    "\n",
    "\n",
    "# Overfit model\n",
    "\n",
    "\n",
    "# Reference model\n",
    "\n",
    "\n",
    "# Plot the whole dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
