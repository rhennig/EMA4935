{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Set 1\n",
    "## Problem 1. Confusion Matrix + Bayes for a Materials Screening Model\n",
    "(30 points)\n",
    "\n",
    "### Background\n",
    "A machine-learning classifier is used to **screen candidate solid-state electrolyte materials**.  \n",
    "The model predicts whether a material is a **fast Li-ion conductor** (positive class) or **not** (negative class), based on structure-derived features.\n",
    "\n",
    "You are given the following information about a screening workflow:\n",
    "\n",
    "- **Total number of candidate materials screened:** 200,000  \n",
    "- **Prevalence (ground truth):**  \n",
    "  - 1% of candidates are truly **fast conductors** (positive class).  \n",
    "  - 99% are **not** fast conductors (negative class).  \n",
    "- **Model performance (measured on a representative validation set):**\n",
    "  - **True-positive rate** (Sensitivity): 80% (0.80)  \n",
    "  - **True-negative rate** (Specificity): 95% (0.95)\n",
    "\n",
    "Your goal is to estimate the **confusion matrix** for the full screening campaign, and then use **Bayes’ theorem** to answer practical questions about what a model prediction means.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "1. **Define key counts from prevalence**\n",
    "   - Compute how many materials in the 200,000-candidate pool are truly:\n",
    "     - **Fast conductors** (positive)\n",
    "     - **Not fast conductors** (negative)\n",
    "\n",
    "2. **Estimate the confusion matrix**\n",
    "   - Use the definitions below to estimate the confusion-matrix entries:\n",
    "     - **Sensitivity (TPR):**\n",
    "       $$\n",
    "       \\text{Sensitivity} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n",
    "       $$\n",
    "     - **Specificity (TNR):**\n",
    "       $$\n",
    "       \\text{Specificity} = \\frac{\\text{TN}}{\\text{TN} + \\text{FP}}\n",
    "       $$\n",
    "   - Solve for:\n",
    "     - **True Positives (TP)**\n",
    "     - **False Negatives (FN)**\n",
    "     - **True Negatives (TN)**\n",
    "     - **False Positives (FP)**\n",
    "   - Check that your totals satisfy:\n",
    "     $$\n",
    "     \\text{TP} + \\text{FN} + \\text{TN} + \\text{FP} = 200{,}000.\n",
    "     $$\n",
    "\n",
    "3. **Answer two questions about “What does a prediction mean?” using Bayes theorem.**\n",
    "\n",
    "   Use your confusion-matrix numbers (or the equivalent conditional probabilities) to answer:\n",
    "\n",
    "   a) If the model predicts **“fast conductor”**, what fraction of those predicted-fast materials are truly fast conductors?\n",
    "\n",
    "   b) If the model predicts **“not fast conductor”**, what fraction of those predicted-not-fast materials are *actually* fast conductors?\n",
    "\n",
    "   *Hint:* Both questions are conditional probabilities of the form $P(\\text{Truth} \\mid \\text{Prediction})$. You may compute them directly from the confusion matrix, or by writing Bayes’ theorem explicitly.\n",
    "\n",
    "4. **Plot the confusion matrix**\n",
    "   - Create a labeled confusion-matrix plot (counts, not just normalized fractions).\n",
    "   - You may use `seaborn.heatmap` or any other plotting method you prefer.\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2. One-Hot Encoding for Chemical Species in Small Molecules (70 points)\n",
    "\n",
    "### Objective:\n",
    "Learn how to implement one-hot encoding for representing the chemical species in small molecules. This assignment will teach you how categorical data (e.g., atom types) can be transformed into numerical representations suitable for machine learning applications.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is One-Hot Encoding?\n",
    "\n",
    "**Definition:**\n",
    "One-hot encoding is a method for converting categorical data (data that can take on a limited number of distinct values) into a numerical format that machine learning models can understand. Each unique category is represented as a binary vector with a length equal to the number of categories. In this vector, one position corresponding to the category is marked as `1`, and all other positions are marked as `0`.\n",
    "\n",
    "---\n",
    "\n",
    "### Example:\n",
    "For the categories `['H', 'C', 'O', 'N']`:\n",
    "- `H` → `[1, 0, 0, 0]`\n",
    "- `C` → `[0, 1, 0, 0]`\n",
    "- `O` → `[0, 0, 1, 0]`\n",
    "- `N` → `[0, 0, 0, 1]`\n",
    "\n",
    "---\n",
    "\n",
    "### Why not encode species as a single number?\n",
    "\n",
    "A tempting alternative is **label encoding**, e.g. `H=1, C=2, N=3, O=4`. The problem is that many models (and essentially all distance-based reasoning) will treat these numbers as if they carry *meaningful magnitudes*. For instance, with label encoding:\n",
    "- `|C − H| = |2 − 1| = 1` suggests **H is “closer” to C** than to O,\n",
    "- `|O − H| = |4 − 1| = 3` suggests **H is “far” from O**.\n",
    "\n",
    "But element identities are *categories*, not points on a number line. One-hot encoding avoids introducing this artificial “closeness” or ordering: all different species are equally distinct unless the data (or a learned embedding) provides a reason otherwise.\n",
    "\n",
    "---\n",
    "\n",
    "### Purpose in Machine Learning\n",
    "\n",
    "1. **Handling Categorical Data:**\n",
    "   Machine learning algorithms typically work with numerical data. One-hot encoding converts non-numeric categories into numbers **without implying any ranking** among categories.\n",
    "\n",
    "2. **Preventing Misinterpretation:**\n",
    "   Unlike label encoding, one-hot encoding does not smuggle in an unintended notion of distance or order between chemical species.\n",
    "\n",
    "3. **Enabling Compatibility:**\n",
    "   Many machine learning models (e.g., neural networks, decision trees) require consistent input shapes and cannot process raw categorical data directly.\n",
    "\n",
    "4. **Avoiding Bias:**\n",
    "   One-hot encoding treats all categories symmetrically, preventing the model from assuming that some species are “greater than” others.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Use One-Hot Encoding for Molecules?\n",
    "\n",
    "In cheminformatics and materials science, molecules often contain categorical data like atom types. Using one-hot encoding:\n",
    "- Ensures that all atom types (e.g., H, C, O, N) are treated as distinct entities.\n",
    "- Prepares molecular data for machine learning models that predict properties such as reactivity, stability, or other materials-relevant behavior.\n",
    "- Captures molecular composition in a structured and interpretable format.\n",
    "\n",
    "---\n",
    "\n",
    "By applying one-hot encoding to molecules, we can convert molecular structures into a numerical representation suitable for machine learning workflows while avoiding unintended numerical assumptions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Problem Description:\n",
    "You are provided with a small dataset of molecules represented by their chemical formulas. Each molecule is described by a list of atoms and their types (e.g., H, C, O, N). Your tasks are:\n",
    "\n",
    "You may represent the dataset as a Python dictionary mapping molecule names to a list of atom symbols (e.g., `{'Molecule 1': ['H','H','O'], ...}`), or an equivalent list-of-lists.\n",
    "\n",
    "1. **Identify Unique Chemical Species**:\n",
    "   Extract all unique atom types across the dataset.\n",
    "\n",
    "2. **Create One-Hot Encodings**:\n",
    "   Assign a binary vector to each unique atom type.\n",
    "\n",
    "3. **Encode Molecules Using One-Hot Representations**:\n",
    "   Convert the list of atoms for each molecule into their corresponding one-hot encoded matrix.\n",
    "\n",
    "4. **Composition of Molecule**:\n",
    "   - Summarize each molecule by the total count of each species (e.g., [2, 1, 0, 0] for 2 H, 1 C, 0 O, and 0 N).\n",
    "   - Visualize the one-hot encoded data using a heatmap.\n",
    "  - For the heatmap, use the **per-molecule composition vectors** (rows = molecules, columns = species in your chosen ordering).\n",
    "\n",
    "---\n",
    "\n",
    "## Dataset Example:\n",
    "\n",
    "| Molecule Name | Atoms        |\n",
    "|---------------|--------------|\n",
    "| Molecule 1    | H, H, O      |\n",
    "| Molecule 2    | C, H, H, O   |\n",
    "| Molecule 3    | N, H, H, C, O |\n",
    "\n",
    "---\n",
    "\n",
    "## Tasks:\n",
    "\n",
    "Use **alphabetical ordering** of species (e.g., C, H, N, O) when constructing one-hot vectors so that all answers are consistent.\n",
    "\n",
    "1. Extract the unique species from the dataset (e.g., $\\{H, C, O, N\\}$).\n",
    "2. Create one-hot encodings for these species:\n",
    "   - Example:\n",
    "     - $H: [1, 0, 0, 0]$\n",
    "     - $C: [0, 1, 0, 0]$\n",
    "     - $O: [0, 0, 1, 0]$\n",
    "     - $N: [0, 0, 0, 1]$\n",
    "3. Convert each molecule into a one-hot encoded matrix:\n",
    "   - Example for Molecule 1 $(H, H, O)$:\n",
    "     $$\n",
    "     \\begin{bmatrix}\n",
    "     1 & 0 & 0 & 0 \\\\\n",
    "     1 & 0 & 0 & 0 \\\\\n",
    "     0 & 0 & 1 & 0 \\\\\n",
    "     \\end{bmatrix}\n",
    "     $$\n",
    "4. (Optional) Summarize each molecule by counting the total occurrences of each species:\n",
    "   - Example:\n",
    "     - Molecule 1: $[2, 0, 1, 0]$\n",
    "     - Molecule 2: $[2, 1, 1, 0]$\n",
    "\n",
    "---\n",
    "\n",
    "## Deliverables:\n",
    "1. Python code that implements the above tasks.\n",
    "2. A report explaining your implementation and showing the results (encoded matrices for each molecule).\n",
    "3. A visualization of the one-hot encoded data.\n",
    "\n",
    "---\n",
    "\n",
    "## Hints:\n",
    "- Use Python’s `set()` to extract unique atom types.\n",
    "- Use libraries like `NumPy` or `pandas` for matrix manipulations.\n",
    "- Use the seaborn library to create a heatmap of the one-hot encoding.\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Generalizing One-Hot Encoding for Molecules\n",
    "For Graduate Students or Extra Credit for Undergraduate Students (30 points)\n",
    "\n",
    "### Objective:\n",
    "Write a Python program that generalizes the one-hot encoding process to work for a set of molecules given as XYZ files in a folder called `molecules`.\n",
    "\n",
    "### Instructions:\n",
    "\n",
    "1. **Folder Structure**:\n",
    "   - Use the provided folder named `molecules` containing XYZ files. Each XYZ file represents a molecule with atomic coordinates.\n",
    "\n",
    "2. **Reading XYZ Files**:\n",
    "   - Write a function `read_xyz(file_path)` that reads an XYZ file and returns a list of atoms in the molecule.\n",
    "   - Assume standard XYZ format: line 1 is the number of atoms, line 2 is a comment. Read the element symbol from each remaining line and ignore any extra columns beyond `Element x y z`.\n",
    "\n",
    "3. **One-Hot Encoding**:\n",
    "   - Build the **global species list** from *all* XYZ files in the folder first (or use a two-pass approach), then apply one-hot encoding using that shared ordering.\n",
    "   - Implement a function `one_hot_encode_atoms(atom_list)` that takes a list of atoms and returns a one-hot encoded representation.\n",
    "   - The one-hot encoding should create a binary vector for each atom type present in the dataset. For example, if the dataset contains Hydrogen (H), Carbon (C), and Oxygen (O), the one-hot encoding for H would be `[1, 0, 0]`, for C would be `[0, 1, 0]`, and for O would be `[0, 0, 1]`.\n",
    "\n",
    "4. **Processing All Molecules**:\n",
    "   - Write a function `process_molecules(folder_path)` that processes all XYZ files in the `molecules` folder, applies one-hot encoding to each molecule, and stores the results in a dictionary where the keys are the file names and the values are the one-hot encoded representations.\n",
    "\n",
    "5. **Output**:\n",
    "   - Print and visualize the one-hot encoded representations for each molecule.\n",
    "\n",
    "6. **Testing**:\n",
    "   - Make sure your program is general and reads all files in a given folder. We will test your program on a folder with a different set of molecules.\n",
    "\n",
    "**Submission note:** You may submit either a `.py` script or a notebook export, as long as the required functions are clearly defined and runnable.\n",
    "\n",
    "\n",
    "#### Example XYZ File Content:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5\n",
    "Comment line\n",
    "H 0.0 0.0 0.0  #Atom type  x  y  z\n",
    "C 0.0 0.0 1.0\n",
    "O 0.0 1.0 0.0\n",
    "H 1.0 0.0 0.0\n",
    "C 1.0 1.0 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Example Output:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "molecule1.xyz: [[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0]]\n",
    "molecule2.xyz: [[0, 1, 0], [1, 0, 0], [0, 0, 1], [0, 1, 0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Submission:\n",
    "Submit your Jupyter Notebook containing the functions and the main program. Ensure that your notebook and code are well-documented and follows best practices for readability and maintainability."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
